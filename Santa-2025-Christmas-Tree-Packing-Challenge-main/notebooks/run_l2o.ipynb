{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e05d1a",
   "metadata": {
    "title": "Kaggle/local bootstrap (deps + env)"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "IS_KAGGLE = Path(\"/kaggle\").exists() or os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") is not None\n",
    "if IS_KAGGLE:\n",
    "    # Evita o JAX pre-alocar 100% da memoria (especialmente em GPU)\n",
    "    os.environ.setdefault(\"XLA_PYTHON_CLIENT_PREALLOCATE\", \"false\")\n",
    "\n",
    "# JAX e necessario para o L2O. Se falhar no Kaggle, habilite Internet e rode novamente.\n",
    "try:\n",
    "    import jax  # noqa: F401\n",
    "except Exception:\n",
    "    print(\"[setup] JAX nao encontrado; instalando jax[cpu]...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"jax[cpu]\"])\n",
    "    import jax  # noqa: F401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53552e",
   "metadata": {
    "title": "Setup"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Callable, Dict, Iterable, List, Tuple\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import inspect\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib\n",
    "\n",
    "if not os.environ.get(\"DISPLAY\"):\n",
    "    matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Resolve repo root (local ou Kaggle)\n",
    "def resolve_repo_root() -> Path:\n",
    "    env_root = os.environ.get(\"SANTA_REPO_ROOT\") or os.environ.get(\"REPO_ROOT\") or os.environ.get(\"PROJECT_ROOT\")\n",
    "    if env_root:\n",
    "        p = Path(env_root).expanduser()\n",
    "        if (p / \"santa_packing\").exists():\n",
    "            return p.resolve()\n",
    "\n",
    "    cwd = Path.cwd()\n",
    "    if (cwd / \"santa_packing\").exists():\n",
    "        return cwd.resolve()\n",
    "    if (cwd.parent / \"santa_packing\").exists():\n",
    "        return cwd.parent.resolve()\n",
    "\n",
    "    # Kaggle: codigo normalmente esta em /kaggle/input/<dataset>/...\n",
    "    kaggle_input = Path(os.environ.get(\"KAGGLE_INPUT_DIR\", \"/kaggle/input\"))\n",
    "    if kaggle_input.exists():\n",
    "        candidates: list[Path] = []\n",
    "        for base in kaggle_input.iterdir():\n",
    "            if not base.is_dir():\n",
    "                continue\n",
    "            if (base / \"santa_packing\").exists():\n",
    "                candidates.append(base)\n",
    "                continue\n",
    "            # comum: dataset/<repo_root>/*\n",
    "            for child in base.iterdir():\n",
    "                if child.is_dir() and (child / \"santa_packing\").exists():\n",
    "                    candidates.append(child)\n",
    "\n",
    "        def _score(p: Path) -> tuple[int, str]:\n",
    "            s = 0\n",
    "            if (p / \"santa_packing\" / \"l2o.py\").exists():\n",
    "                s += 2\n",
    "            if (p / \"santa_packing\" / \"cli\" / \"generate_submission.py\").exists():\n",
    "                s += 1\n",
    "            if (p / \"notebooks\" / \"run_l2o.ipynb\").exists():\n",
    "                s += 1\n",
    "            return (-s, str(p))\n",
    "\n",
    "        uniq = {p.resolve() for p in candidates}\n",
    "        candidates = sorted(uniq, key=_score)\n",
    "        if candidates:\n",
    "            return candidates[0]\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Nao encontrei o repo root. Rode do root/notebooks ou defina SANTA_REPO_ROOT (ou REPO_ROOT).\"\n",
    "    )\n",
    "\n",
    "\n",
    "ROOT = resolve_repo_root()\n",
    "\n",
    "# Pasta gravavel (Kaggle: /kaggle/working)\n",
    "WORK_DIR = Path(os.environ.get(\"KAGGLE_WORKING_DIR\", \"/kaggle/working\")) if Path(\"/kaggle/working\").exists() else ROOT\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[paths] ROOT =\", ROOT)\n",
    "print(\"[paths] WORK_DIR =\", WORK_DIR)\n",
    "\n",
    "sys.path.insert(0, str(ROOT))\n",
    "# sys.path.insert(0, str(ROOT / \"src\"))\n",
    "\n",
    "from santa_packing.geom_np import (  # noqa: E402\n",
    "    packing_score,\n",
    "    polygon_bbox,\n",
    "    polygon_radius,\n",
    "    prefix_score,\n",
    "    shift_poses_to_origin,\n",
    "    transform_polygon,\n",
    ")\n",
    "import santa_packing.l2o as l2o_mod  # noqa: E402\n",
    "from santa_packing.optimizer import run_sa_batch  # noqa: E402\n",
    "import santa_packing.cli.train_l2o as train_l2o_mod  # noqa: E402\n",
    "from santa_packing.tree_data import TREE_POINTS  # noqa: E402\n",
    "\n",
    "train_l2o_mod = importlib.reload(train_l2o_mod)\n",
    "l2o_mod = importlib.reload(l2o_mod)\n",
    "L2OConfig = l2o_mod.L2OConfig\n",
    "load_params_npz = l2o_mod.load_params_npz\n",
    "save_params_npz = l2o_mod.save_params_npz\n",
    "optimize_with_l2o = l2o_mod.optimize_with_l2o\n",
    "\n",
    "\n",
    "def train_l2o_model_safe(**kwargs):\n",
    "    seed = kwargs.get(\"seed\")\n",
    "    if seed is not None:\n",
    "        try:\n",
    "            seed_int = int(seed)\n",
    "        except Exception:\n",
    "            seed_int = None\n",
    "        if seed_int is not None:\n",
    "            np.random.seed(seed_int)\n",
    "            random.seed(seed_int)\n",
    "\n",
    "    sig = inspect.signature(train_l2o_mod.train_l2o_model)\n",
    "    allowed = {k: v for k, v in kwargs.items() if k in sig.parameters}\n",
    "    missing = sorted(set(kwargs) - set(allowed))\n",
    "    if missing:\n",
    "        print(f\"[warn] train_l2o_model ignorou parametros nao suportados: {missing}\")\n",
    "    return train_l2o_mod.train_l2o_model(**allowed)\n",
    "\n",
    "\n",
    "def parse_int_list(text: str) -> list[int]:\n",
    "    raw = text.strip()\n",
    "    if not raw:\n",
    "        return []\n",
    "    if raw.lower() in {\"none\", \"off\", \"false\"}:\n",
    "        return []\n",
    "    out: list[int] = []\n",
    "    for part in raw.split(\",\"):\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        if \"..\" in part:\n",
    "            a, b = part.split(\"..\", 1)\n",
    "            start = int(a)\n",
    "            end = int(b)\n",
    "            step = 1 if end >= start else -1\n",
    "            out.extend(list(range(start, end + step, step)))\n",
    "            continue\n",
    "        if \"-\" in part and part.count(\"-\") == 1 and part[0] != \"-\":\n",
    "            a, b = part.split(\"-\", 1)\n",
    "            start = int(a)\n",
    "            end = int(b)\n",
    "            out.extend(list(range(start, end + 1)))\n",
    "            continue\n",
    "        out.append(int(part))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e548505",
   "metadata": {
    "title": "Initial layouts"
   },
   "outputs": [],
   "source": [
    "def grid_initial(n: int, spacing: float) -> np.ndarray:\n",
    "    cols = int(np.ceil(np.sqrt(n)))\n",
    "    poses = np.zeros((n, 3), dtype=float)\n",
    "    for i in range(n):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        poses[i] = (col * spacing, row * spacing, 0.0)\n",
    "    return poses\n",
    "\n",
    "\n",
    "def random_initial(n: int, spacing: float, rng: np.random.Generator, rand_scale: float) -> np.ndarray:\n",
    "    scale = spacing * math.sqrt(max(n, 1)) * rand_scale\n",
    "    xy = rng.uniform(-scale, scale, size=(n, 2))\n",
    "    theta = rng.uniform(0.0, 360.0, size=(n, 1))\n",
    "    return np.concatenate([xy, theta], axis=1)\n",
    "\n",
    "\n",
    "def make_initial(\n",
    "    points: np.ndarray,\n",
    "    n: int,\n",
    "    spacing: float,\n",
    "    seed: int,\n",
    "    init_mode: str,\n",
    "    rand_scale: float,\n",
    ") -> np.ndarray:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if init_mode == \"grid\":\n",
    "        poses = grid_initial(n, spacing)\n",
    "    elif init_mode == \"random\":\n",
    "        poses = random_initial(n, spacing, rng, rand_scale)\n",
    "    else:\n",
    "        poses = grid_initial(n, spacing) if seed % 2 == 0 else random_initial(n, spacing, rng, rand_scale)\n",
    "    return shift_poses_to_origin(points, poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f55f4",
   "metadata": {
    "title": "Scoring/plots/utilities"
   },
   "outputs": [],
   "source": [
    "def prefix_packing_score_np(points: np.ndarray, poses: np.ndarray) -> float:\n",
    "    if poses.shape[0] == 0:\n",
    "        return 0.0\n",
    "    min_x = min_y = float(\"inf\")\n",
    "    max_x = max_y = float(\"-inf\")\n",
    "    s_values: List[float] = []\n",
    "    for pose in poses:\n",
    "        bbox = polygon_bbox(transform_polygon(points, pose))\n",
    "        min_x = min(min_x, float(bbox[0]))\n",
    "        min_y = min(min_y, float(bbox[1]))\n",
    "        max_x = max(max_x, float(bbox[2]))\n",
    "        max_y = max(max_y, float(bbox[3]))\n",
    "        width = max_x - min_x\n",
    "        height = max_y - min_y\n",
    "        s_values.append(max(width, height))\n",
    "    return float(prefix_score(s_values))\n",
    "\n",
    "\n",
    "def plot_packing(poses: np.ndarray, title: str, out_path: Path | None = None) -> None:\n",
    "    points = np.array(TREE_POINTS, dtype=float)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for pose in poses:\n",
    "        poly = transform_polygon(points, pose)\n",
    "        p = np.vstack([poly, poly[0]])\n",
    "        plt.plot(p[:, 0], p[:, 1], \"g-\")\n",
    "    plt.axis(\"equal\")\n",
    "    plt.title(title)\n",
    "    if out_path is not None:\n",
    "        plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_cmd(cmd: List[str]) -> None:\n",
    "    print(\"$\", \" \".join(cmd))\n",
    "    result = subprocess.run(cmd, cwd=ROOT, text=True)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed with code {result.returncode}\")\n",
    "\n",
    "\n",
    "def l2o_config_from_meta(meta: Dict[str, object], *, reward: str, deterministic: bool) -> L2OConfig:\n",
    "    def _get_int(key: str, default: int) -> int:\n",
    "        val = meta.get(key, default)\n",
    "        return int(val) if hasattr(val, \"__int__\") else default\n",
    "\n",
    "    def _get_float(key: str, default: float) -> float:\n",
    "        val = meta.get(key, default)\n",
    "        if isinstance(val, (float, np.floating)):\n",
    "            return float(val)\n",
    "        if isinstance(val, (int, np.integer)):\n",
    "            return float(val)\n",
    "        if isinstance(val, np.ndarray) and val.shape == ():\n",
    "            return float(val.item())\n",
    "        return default\n",
    "\n",
    "    def _get_bool(key: str, default: bool) -> bool:\n",
    "        val = meta.get(key, default)\n",
    "        if isinstance(val, (bool, np.bool_)):\n",
    "            return bool(val)\n",
    "        if isinstance(val, (int, np.integer)):\n",
    "            return bool(int(val))\n",
    "        if isinstance(val, np.ndarray) and val.shape == ():\n",
    "            return bool(val.item())\n",
    "        return default\n",
    "\n",
    "    policy = str(meta.get(\"policy\", \"mlp\"))\n",
    "    knn_k = _get_int(\"knn_k\", 4)\n",
    "    hidden = _get_int(\"hidden\", 32)\n",
    "    mlp_depth = _get_int(\"mlp_depth\", 1)\n",
    "    gnn_steps = _get_int(\"gnn_steps\", 1)\n",
    "    gnn_attention = _get_bool(\"gnn_attention\", False)\n",
    "    action_scale = _get_float(\"action_scale\", 1.0)\n",
    "    feature_mode = str(meta.get(\"feature_mode\", \"raw\"))\n",
    "    overlap_penalty = _get_float(\"overlap_penalty\", 50.0)\n",
    "    overlap_lambda = _get_float(\"overlap_lambda\", 0.0)\n",
    "\n",
    "    return L2OConfig(\n",
    "        hidden_size=hidden,\n",
    "        policy=policy,\n",
    "        knn_k=knn_k,\n",
    "        reward=reward,\n",
    "        mlp_depth=mlp_depth,\n",
    "        gnn_steps=gnn_steps,\n",
    "        gnn_attention=gnn_attention,\n",
    "        feature_mode=feature_mode,\n",
    "        action_scale=action_scale,\n",
    "        overlap_penalty=overlap_penalty,\n",
    "        overlap_lambda=overlap_lambda,\n",
    "        action_noise=not deterministic,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f95d416",
   "metadata": {
    "title": "Evaluation helpers"
   },
   "outputs": [],
   "source": [
    "def evaluate_solver(\n",
    "    name: str,\n",
    "    solve_fn: Callable[[int, int], np.ndarray | Tuple[np.ndarray, Dict[str, str]]],\n",
    "    n_list: Iterable[int],\n",
    "    seeds: Iterable[int],\n",
    "    points: np.ndarray,\n",
    "    split: str,\n",
    ") -> List[Dict[str, float]]:\n",
    "    rows: List[Dict[str, float]] = []\n",
    "    for n in n_list:\n",
    "        for seed in seeds:\n",
    "            result = solve_fn(n, seed)\n",
    "            info: Dict[str, object] = {}\n",
    "            if isinstance(result, tuple):\n",
    "                poses, info = result\n",
    "            else:\n",
    "                poses = result\n",
    "            prefix = prefix_packing_score_np(points, poses)\n",
    "            pack = packing_score(points, poses)\n",
    "            row = {\n",
    "                \"split\": split,\n",
    "                \"model\": name,\n",
    "                \"n\": int(n),\n",
    "                \"seed\": int(seed),\n",
    "                \"prefix_score\": float(prefix),\n",
    "                \"packing_score\": float(pack),\n",
    "            }\n",
    "            if isinstance(info, dict) and \"selected\" in info:\n",
    "                row[\"selected\"] = str(info[\"selected\"])\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def summarize_results(rows: List[Dict[str, float]]) -> List[Dict[str, float]]:\n",
    "    grouped: Dict[Tuple[str, str, int], List[Dict[str, float]]] = {}\n",
    "    for row in rows:\n",
    "        key = (row[\"split\"], row[\"model\"], int(row[\"n\"]))\n",
    "        grouped.setdefault(key, []).append(row)\n",
    "\n",
    "    summary: List[Dict[str, float]] = []\n",
    "    for (split, model, n), items in sorted(grouped.items(), key=lambda x: (x[0][0], x[0][1], x[0][2])):\n",
    "        prefix_vals = np.array([r[\"prefix_score\"] for r in items], dtype=float)\n",
    "        pack_vals = np.array([r[\"packing_score\"] for r in items], dtype=float)\n",
    "        summary.append(\n",
    "            {\n",
    "                \"split\": split,\n",
    "                \"model\": model,\n",
    "                \"n\": int(n),\n",
    "                \"samples\": int(prefix_vals.size),\n",
    "                \"prefix_mean\": float(prefix_vals.mean()),\n",
    "                \"prefix_std\": float(prefix_vals.std()),\n",
    "                \"packing_mean\": float(pack_vals.mean()),\n",
    "                \"packing_std\": float(pack_vals.std()),\n",
    "            }\n",
    "        )\n",
    "    return summary\n",
    "\n",
    "\n",
    "def summarize_overall(rows: List[Dict[str, float]]) -> List[Dict[str, float]]:\n",
    "    grouped: Dict[Tuple[str, str], List[Dict[str, float]]] = {}\n",
    "    for row in rows:\n",
    "        grouped.setdefault((row[\"split\"], row[\"model\"]), []).append(row)\n",
    "\n",
    "    overall: List[Dict[str, float]] = []\n",
    "    for (split, model), items in sorted(grouped.items()):\n",
    "        prefix_vals = np.array([r[\"prefix_score\"] for r in items], dtype=float)\n",
    "        pack_vals = np.array([r[\"packing_score\"] for r in items], dtype=float)\n",
    "        overall.append(\n",
    "            {\n",
    "                \"split\": split,\n",
    "                \"model\": model,\n",
    "                \"samples\": int(prefix_vals.size),\n",
    "                \"prefix_mean\": float(prefix_vals.mean()),\n",
    "                \"prefix_std\": float(prefix_vals.std()),\n",
    "                \"packing_mean\": float(pack_vals.mean()),\n",
    "                \"packing_std\": float(pack_vals.std()),\n",
    "            }\n",
    "        )\n",
    "    return overall\n",
    "\n",
    "\n",
    "def challenge_score_from_results(rows: List[Dict[str, float]], model: str, split: str) -> float:\n",
    "    grouped: Dict[int, List[float]] = {}\n",
    "    for row in rows:\n",
    "        if row.get(\"model\") != model or row.get(\"split\") != split:\n",
    "            continue\n",
    "        grouped.setdefault(int(row[\"n\"]), []).append(float(row[\"packing_score\"]))\n",
    "    total = 0.0\n",
    "    for n in sorted(grouped):\n",
    "        mean_s = float(np.mean(grouped[n]))\n",
    "        total += (mean_s * mean_s) / n\n",
    "    return total\n",
    "\n",
    "\n",
    "def write_csv(path: Path, rows: List[Dict[str, float]]) -> None:\n",
    "    if not rows:\n",
    "        return\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", newline=\"\") as f:\n",
    "        fieldnames: List[str] = []\n",
    "        seen = set()\n",
    "        for row in rows:\n",
    "            for key in row.keys():\n",
    "                if key not in seen:\n",
    "                    seen.add(key)\n",
    "                    fieldnames.append(key)\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "\n",
    "def save_eval_artifacts(\n",
    "    run_dir: Path,\n",
    "    rows: List[Dict[str, float]],\n",
    "    summary: List[Dict[str, float]],\n",
    "    overall: List[Dict[str, float]],\n",
    "    meta: Dict[str, object],\n",
    ") -> None:\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    write_csv(run_dir / \"metrics.csv\", rows)\n",
    "    write_csv(run_dir / \"per_n.csv\", summary)\n",
    "    write_csv(run_dir / \"overall.csv\", overall)\n",
    "    (run_dir / \"meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "    lines = [\n",
    "        \"# L2O evaluation summary\",\n",
    "        \"\",\n",
    "        \"Lower is better. Prefix score matches the leaderboard aggregate.\",\n",
    "        \"\",\n",
    "        \"## Overall (mean across n and seeds)\",\n",
    "    ]\n",
    "    for row in overall:\n",
    "        lines.append(\n",
    "            f\"- [{row['split']}] {row['model']}: prefix={row['prefix_mean']:.4f} +/- {row['prefix_std']:.4f}, \"\n",
    "            f\"packing={row['packing_mean']:.4f} +/- {row['packing_std']:.4f}\"\n",
    "        )\n",
    "    (run_dir / \"summary.md\").write_text(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "def plot_eval_curves(summary: List[Dict[str, float]], out_path: Path | None = None) -> None:\n",
    "    if not summary:\n",
    "        return\n",
    "    grouped: Dict[Tuple[str, str], List[Dict[str, float]]] = {}\n",
    "    for row in summary:\n",
    "        grouped.setdefault((row[\"split\"], row[\"model\"]), []).append(row)\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    for (split, model), items in grouped.items():\n",
    "        items = sorted(items, key=lambda r: r[\"n\"])\n",
    "        ns = [r[\"n\"] for r in items]\n",
    "        means = [r[\"prefix_mean\"] for r in items]\n",
    "        stds = [r[\"prefix_std\"] for r in items]\n",
    "        plt.plot(ns, means, marker=\"o\", label=f\"{model} ({split})\")\n",
    "        plt.fill_between(ns, np.array(means) - np.array(stds), np.array(means) + np.array(stds), alpha=0.2)\n",
    "    plt.xlabel(\"n\")\n",
    "    plt.ylabel(\"prefix score\")\n",
    "    plt.title(\"Prefix score vs n\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if out_path is not None:\n",
    "        plt.savefig(out_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9493d",
   "metadata": {
    "title": "Configuracoes"
   },
   "outputs": [],
   "source": [
    "# === Configuracoes ===\n",
    "TRAIN_N_LIST = [8, 10, 12]\n",
    "VAL_N_LIST = [6, 9, 11]\n",
    "TRAIN_STEPS = 400\n",
    "ROLLOUT_STEPS = 50\n",
    "BATCH = 64\n",
    "REWARD = \"prefix\"  # \"packing\" ou \"prefix\"\n",
    "HIDDEN_SIZE = 32\n",
    "ACTION_SCALE = 0.05\n",
    "FEATURE_MODE = \"raw\"  # raw | bbox_norm\n",
    "TRAIN_INIT_MODE = \"all\"  # grid | random | mix | lattice | all\n",
    "TRAIN_LATTICE_PATTERN = \"hex\"\n",
    "TRAIN_LATTICE_MARGIN = 0.02\n",
    "TRAIN_LATTICE_ROTATE = 0.0\n",
    "TRAIN_CURRICULUM = False\n",
    "TRAIN_CURRICULUM_START_MAX = None\n",
    "TRAIN_CURRICULUM_END_MAX = None\n",
    "TRAIN_CURRICULUM_STEPS = None\n",
    "\n",
    "BASELINE_MODE = \"ema\"  # \"batch\" (baseline por batch) | \"ema\" (media movel)\n",
    "BASELINE_DECAY = 0.9\n",
    "\n",
    "MLP_DEPTH = 2\n",
    "GNN_STEPS = 2\n",
    "GNN_ATTENTION = False\n",
    "\n",
    "TRAIN_EVAL_SEEDS = [0, 1, 2]\n",
    "VAL_EVAL_SEEDS = [3, 4, 5]\n",
    "EVAL_STEPS = 50\n",
    "INIT_MODE = \"grid\"  # grid | random | mix\n",
    "RAND_SCALE = 0.3\n",
    "\n",
    "# ---- L2O sweep (muitos experimentos) ----\n",
    "# Ideia: explorar varias configs (seed/hparams) rapidamente, rankear no \"val\" e\n",
    "# depois re-treinar apenas o melhor MLP/GNN com o budget cheio.\n",
    "RUN_L2O_SWEEP = True\n",
    "L2O_SWEEP_SEEDS = \"1..8\"  # ex.: \"1..30\" para MUITOS seeds\n",
    "L2O_SWEEP_FEATURE_MODES = [\"raw\", \"bbox_norm\", \"rich\"]\n",
    "L2O_SWEEP_HIDDEN_SIZES = [32, 64]\n",
    "L2O_SWEEP_ACTION_SCALES = [0.05, 0.1]\n",
    "L2O_SWEEP_MLP_DEPTHS = [1, 2, 3]\n",
    "L2O_SWEEP_GNN_STEPS = [1, 2, 3]\n",
    "L2O_SWEEP_GNN_ATTENTION = [False, True]\n",
    "L2O_SWEEP_KNN_K = [4, 6]\n",
    "L2O_SWEEP_LR = [1e-3, 3e-4]\n",
    "L2O_SWEEP_OVERLAP_LAMBDA = [0.0, 0.01]\n",
    "\n",
    "L2O_SWEEP_TRAIN_STEPS = 200\n",
    "L2O_SWEEP_ROLLOUT_STEPS = ROLLOUT_STEPS\n",
    "L2O_SWEEP_EVAL_N_LIST = VAL_N_LIST\n",
    "L2O_SWEEP_EVAL_SEEDS = VAL_EVAL_SEEDS\n",
    "L2O_SWEEP_EVAL_STEPS = EVAL_STEPS\n",
    "L2O_SWEEP_MAX_EXPERIMENTS = None  # None = roda tudo (pode explodir)\n",
    "L2O_SWEEP_TOPK_PER_POLICY = 5  # exporta top-K p/ usar no generate_submission (opcional)\n",
    "L2O_SWEEP_RETRAIN_FINAL = True\n",
    "L2O_FINAL_TRAIN_STEPS = TRAIN_STEPS\n",
    "L2O_FINAL_ROLLOUT_STEPS = ROLLOUT_STEPS\n",
    "\n",
    "SA_STEPS = 300\n",
    "SA_TRANS_SIGMA = 0.2\n",
    "SA_ROT_SIGMA = 15.0\n",
    "SA_ROT_PROB = 0.3\n",
    "SA_OBJECTIVE = REWARD\n",
    "\n",
    "RUN_BC_PIPELINE = True\n",
    "BC_POLICY = \"gnn\"\n",
    "BC_KNN_K = 4\n",
    "BC_RUNS_PER_N = 3\n",
    "BC_STEPS = 200\n",
    "BC_TRAIN_STEPS = 200\n",
    "BC_SEED = 0\n",
    "BC_INIT_MODE = \"all\"  # grid | random | mix | lattice | all\n",
    "BC_RAND_SCALE = 0.3\n",
    "BC_LATTICE_PATTERN = \"hex\"\n",
    "BC_LATTICE_MARGIN = 0.02\n",
    "BC_LATTICE_ROTATE = 0.0\n",
    "BC_CURRICULUM = False\n",
    "BC_CURRICULUM_START_MAX = None\n",
    "BC_CURRICULUM_END_MAX = None\n",
    "BC_CURRICULUM_STEPS = None\n",
    "BC_DATASET_PATH = None  # sobrescreva para reutilizar dataset\n",
    "BC_POLICY_PATH = None  # sobrescreva para reutilizar policy\n",
    "\n",
    "RUN_META_TRAIN = True\n",
    "META_INIT_MODEL_PATH = None\n",
    "META_TRAIN_STEPS = 30\n",
    "META_ES_POP = 6\n",
    "META_SA_STEPS = 150\n",
    "\n",
    "RUN_HEATMAP_TRAIN = True\n",
    "HEATMAP_MODEL_PATH = None\n",
    "HEATMAP_TRAIN_STEPS = 30\n",
    "HEATMAP_ES_POP = 6\n",
    "HEATMAP_STEPS = 200\n",
    "\n",
    "RUN_ENSEMBLE = True\n",
    "ENSEMBLE_SCORE = \"prefix\"  # criterio de selecao no ensemble\n",
    "L2O_REFINE_GRID = False\n",
    "L2O_REFINE_SA = False\n",
    "REFINE_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9e16a",
   "metadata": {
    "title": "Treino das politicas (setup)"
   },
   "outputs": [],
   "source": [
    "RUN_DIR = WORK_DIR / \"runs\" / f\"l2o_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "points = np.array(TREE_POINTS, dtype=float)\n",
    "spacing = 2.0 * polygon_radius(points) * 1.2\n",
    "VIS_N = TRAIN_N_LIST[0]\n",
    "init = shift_poses_to_origin(points, grid_initial(VIS_N, spacing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f3b4d",
   "metadata": {
    "title": "Solvers (needed early for sweep/eval)"
   },
   "outputs": [],
   "source": [
    "initial_cache: Dict[Tuple[int, int], np.ndarray] = {}\n",
    "\n",
    "\n",
    "def get_initial(n: int, seed: int) -> np.ndarray:\n",
    "    key = (n, seed)\n",
    "    if key not in initial_cache:\n",
    "        initial_cache[key] = make_initial(points, n, spacing, seed, INIT_MODE, RAND_SCALE)\n",
    "    return initial_cache[key]\n",
    "\n",
    "\n",
    "def solve_grid(n: int, seed: int) -> np.ndarray:\n",
    "    return get_initial(n, seed)\n",
    "\n",
    "\n",
    "def solve_sa(n: int, seed: int) -> np.ndarray:\n",
    "    init_pose = get_initial(n, seed)\n",
    "    init_batch = jnp.array(init_pose)[None, :, :]\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    best_poses, _ = run_sa_batch(\n",
    "        key,\n",
    "        SA_STEPS,\n",
    "        n,\n",
    "        init_batch,\n",
    "        trans_sigma=SA_TRANS_SIGMA,\n",
    "        rot_sigma=SA_ROT_SIGMA,\n",
    "        rot_prob=SA_ROT_PROB,\n",
    "        objective=SA_OBJECTIVE,\n",
    "    )\n",
    "    return np.array(best_poses[0])\n",
    "\n",
    "\n",
    "def solve_l2o(params, cfg: L2OConfig, *, steps: int | None = None) -> Callable[[int, int], np.ndarray]:\n",
    "    def _solve(n: int, seed: int) -> np.ndarray:\n",
    "        init_pose = get_initial(n, seed)\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        nsteps = EVAL_STEPS if steps is None else int(steps)\n",
    "        poses = optimize_with_l2o(key, params, jnp.array(init_pose), nsteps, cfg)\n",
    "        return np.array(poses)\n",
    "\n",
    "    return _solve\n",
    "\n",
    "\n",
    "def solve_l2o_refine(\n",
    "    base_solver: Callable[[int, int], np.ndarray],\n",
    "    params,\n",
    "    cfg: L2OConfig,\n",
    "    *,\n",
    "    steps: int | None = None,\n",
    ") -> Callable[[int, int], np.ndarray]:\n",
    "    def _solve(n: int, seed: int) -> np.ndarray:\n",
    "        base_pose = base_solver(n, seed)\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        nsteps = REFINE_STEPS if steps is None else int(steps)\n",
    "        poses = optimize_with_l2o(key, params, jnp.array(base_pose), nsteps, cfg)\n",
    "        return np.array(poses)\n",
    "\n",
    "    return _solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df4656",
   "metadata": {
    "title": "Treino L2O (sweep -> seleciona os melhores)"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[l2o_sweep] 50/5760 done. best_val=12.498953\n"
     ]
    }
   ],
   "source": [
    "L2O_SWEEP_TOP_MODELS: Dict[str, Path] = {}\n",
    "mlp_meta: Dict[str, object] = {\n",
    "    \"policy\": \"mlp\",\n",
    "    \"hidden\": HIDDEN_SIZE,\n",
    "    \"mlp_depth\": MLP_DEPTH,\n",
    "    \"gnn_steps\": GNN_STEPS,\n",
    "    \"gnn_attention\": GNN_ATTENTION,\n",
    "    \"feature_mode\": FEATURE_MODE,\n",
    "    \"action_scale\": ACTION_SCALE,\n",
    "    \"knn_k\": 4,\n",
    "    \"overlap_penalty\": 50.0,\n",
    "    \"overlap_lambda\": 0.0,\n",
    "}\n",
    "gnn_meta: Dict[str, object] = {\n",
    "    \"policy\": \"gnn\",\n",
    "    \"hidden\": HIDDEN_SIZE,\n",
    "    \"knn_k\": 4,\n",
    "    \"mlp_depth\": MLP_DEPTH,\n",
    "    \"gnn_steps\": GNN_STEPS,\n",
    "    \"gnn_attention\": GNN_ATTENTION,\n",
    "    \"feature_mode\": FEATURE_MODE,\n",
    "    \"action_scale\": ACTION_SCALE,\n",
    "    \"overlap_penalty\": 50.0,\n",
    "    \"overlap_lambda\": 0.0,\n",
    "}\n",
    "\n",
    "if RUN_L2O_SWEEP:\n",
    "    sweep_dir = RUN_DIR / \"l2o_sweep\"\n",
    "    sweep_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _cfg_id(data: Dict[str, object]) -> str:\n",
    "        blob = json.dumps(data, sort_keys=True, default=str).encode(\"utf-8\")\n",
    "        return hashlib.sha1(blob).hexdigest()[:10]\n",
    "\n",
    "    seeds = parse_int_list(L2O_SWEEP_SEEDS)\n",
    "    if not seeds:\n",
    "        seeds = [1]\n",
    "\n",
    "    planned: List[Dict[str, object]] = []\n",
    "\n",
    "    # MLP space\n",
    "    for seed, feature_mode, hidden, action_scale, lr, overlap_lambda, mlp_depth in itertools.product(\n",
    "        seeds,\n",
    "        L2O_SWEEP_FEATURE_MODES,\n",
    "        L2O_SWEEP_HIDDEN_SIZES,\n",
    "        L2O_SWEEP_ACTION_SCALES,\n",
    "        L2O_SWEEP_LR,\n",
    "        L2O_SWEEP_OVERLAP_LAMBDA,\n",
    "        L2O_SWEEP_MLP_DEPTHS,\n",
    "    ):\n",
    "        meta = {\n",
    "            \"policy\": \"mlp\",\n",
    "            \"seed\": int(seed),\n",
    "            \"hidden\": int(hidden),\n",
    "            \"mlp_depth\": int(mlp_depth),\n",
    "            \"gnn_steps\": int(GNN_STEPS),\n",
    "            \"gnn_attention\": bool(GNN_ATTENTION),\n",
    "            \"knn_k\": 4,\n",
    "            \"feature_mode\": str(feature_mode),\n",
    "            \"action_scale\": float(action_scale),\n",
    "            \"lr\": float(lr),\n",
    "            \"baseline_mode\": str(BASELINE_MODE),\n",
    "            \"baseline_decay\": float(BASELINE_DECAY),\n",
    "            \"overlap_penalty\": 50.0,\n",
    "            \"overlap_lambda\": float(overlap_lambda),\n",
    "        }\n",
    "        cid = _cfg_id(meta)\n",
    "        planned.append({\"name\": f\"mlp_{cid}\", \"meta\": meta})\n",
    "\n",
    "    # GNN space\n",
    "    for (\n",
    "        seed,\n",
    "        feature_mode,\n",
    "        hidden,\n",
    "        action_scale,\n",
    "        lr,\n",
    "        overlap_lambda,\n",
    "        gnn_steps,\n",
    "        gnn_attention,\n",
    "        knn_k,\n",
    "    ) in itertools.product(\n",
    "        seeds,\n",
    "        L2O_SWEEP_FEATURE_MODES,\n",
    "        L2O_SWEEP_HIDDEN_SIZES,\n",
    "        L2O_SWEEP_ACTION_SCALES,\n",
    "        L2O_SWEEP_LR,\n",
    "        L2O_SWEEP_OVERLAP_LAMBDA,\n",
    "        L2O_SWEEP_GNN_STEPS,\n",
    "        L2O_SWEEP_GNN_ATTENTION,\n",
    "        L2O_SWEEP_KNN_K,\n",
    "    ):\n",
    "        meta = {\n",
    "            \"policy\": \"gnn\",\n",
    "            \"seed\": int(seed),\n",
    "            \"hidden\": int(hidden),\n",
    "            \"knn_k\": int(knn_k),\n",
    "            \"mlp_depth\": int(MLP_DEPTH),\n",
    "            \"gnn_steps\": int(gnn_steps),\n",
    "            \"gnn_attention\": bool(gnn_attention),\n",
    "            \"feature_mode\": str(feature_mode),\n",
    "            \"action_scale\": float(action_scale),\n",
    "            \"lr\": float(lr),\n",
    "            \"baseline_mode\": str(BASELINE_MODE),\n",
    "            \"baseline_decay\": float(BASELINE_DECAY),\n",
    "            \"overlap_penalty\": 50.0,\n",
    "            \"overlap_lambda\": float(overlap_lambda),\n",
    "        }\n",
    "        cid = _cfg_id(meta)\n",
    "        planned.append({\"name\": f\"gnn_{cid}\", \"meta\": meta})\n",
    "\n",
    "    planned = sorted(planned, key=lambda r: str(r[\"name\"]))\n",
    "    if L2O_SWEEP_MAX_EXPERIMENTS is not None:\n",
    "        planned = planned[: int(L2O_SWEEP_MAX_EXPERIMENTS)]\n",
    "\n",
    "    (sweep_dir / \"sweep_meta.json\").write_text(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"planned_total\": int(len(planned)),\n",
    "                \"seeds\": list(seeds),\n",
    "                \"train_steps\": int(L2O_SWEEP_TRAIN_STEPS),\n",
    "                \"rollout_steps\": int(L2O_SWEEP_ROLLOUT_STEPS),\n",
    "                \"eval_n_list\": list(L2O_SWEEP_EVAL_N_LIST),\n",
    "                \"eval_seeds\": list(L2O_SWEEP_EVAL_SEEDS),\n",
    "                \"eval_steps\": int(L2O_SWEEP_EVAL_STEPS),\n",
    "                \"max_experiments\": L2O_SWEEP_MAX_EXPERIMENTS,\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    sweep_rows: List[Dict[str, object]] = []\n",
    "    for idx, item in enumerate(planned, start=1):\n",
    "        name = str(item[\"name\"])\n",
    "        meta = dict(item[\"meta\"])  # type: ignore[arg-type]\n",
    "\n",
    "        out_path = sweep_dir / f\"{name}.npz\"\n",
    "        if out_path.exists():\n",
    "            params, saved_meta = load_params_npz(out_path)\n",
    "            meta = dict(saved_meta)\n",
    "            loss = []\n",
    "        else:\n",
    "            params, loss = train_l2o_model_safe(\n",
    "                seed=int(meta[\"seed\"]),\n",
    "                n_list=TRAIN_N_LIST,\n",
    "                batch=BATCH,\n",
    "                train_steps=L2O_SWEEP_TRAIN_STEPS,\n",
    "                steps=L2O_SWEEP_ROLLOUT_STEPS,\n",
    "                lr=float(meta.get(\"lr\", 1e-3)),\n",
    "                hidden_size=int(meta.get(\"hidden\", HIDDEN_SIZE)),\n",
    "                policy=str(meta.get(\"policy\", \"mlp\")),\n",
    "                reward=REWARD,\n",
    "                action_scale=float(meta.get(\"action_scale\", ACTION_SCALE)),\n",
    "                knn_k=int(meta.get(\"knn_k\", 4)),\n",
    "                mlp_depth=int(meta.get(\"mlp_depth\", MLP_DEPTH)),\n",
    "                gnn_steps=int(meta.get(\"gnn_steps\", GNN_STEPS)),\n",
    "                gnn_attention=bool(meta.get(\"gnn_attention\", False)),\n",
    "                init_mode=TRAIN_INIT_MODE,\n",
    "                rand_scale=RAND_SCALE,\n",
    "                lattice_pattern=TRAIN_LATTICE_PATTERN,\n",
    "                lattice_margin=TRAIN_LATTICE_MARGIN,\n",
    "                lattice_rotate=TRAIN_LATTICE_ROTATE,\n",
    "                baseline_mode=str(meta.get(\"baseline_mode\", BASELINE_MODE)),\n",
    "                baseline_decay=float(meta.get(\"baseline_decay\", BASELINE_DECAY)),\n",
    "                curriculum=TRAIN_CURRICULUM,\n",
    "                curriculum_start_max=TRAIN_CURRICULUM_START_MAX,\n",
    "                curriculum_end_max=TRAIN_CURRICULUM_END_MAX,\n",
    "                curriculum_steps=TRAIN_CURRICULUM_STEPS,\n",
    "                feature_mode=str(meta.get(\"feature_mode\", FEATURE_MODE)),\n",
    "                overlap_lambda=float(meta.get(\"overlap_lambda\", 0.0)),\n",
    "                verbose_freq=0,\n",
    "            )\n",
    "            save_params_npz(out_path, params, meta=meta)\n",
    "\n",
    "        eval_cfg = l2o_config_from_meta(meta, reward=REWARD, deterministic=True)\n",
    "        eval_rows = evaluate_solver(\n",
    "            name,\n",
    "            solve_l2o(params, eval_cfg, steps=L2O_SWEEP_EVAL_STEPS),\n",
    "            L2O_SWEEP_EVAL_N_LIST,\n",
    "            L2O_SWEEP_EVAL_SEEDS,\n",
    "            points,\n",
    "            split=\"sweep_val\",\n",
    "        )\n",
    "        val_score = float(challenge_score_from_results(eval_rows, name, \"sweep_val\"))\n",
    "\n",
    "        sweep_rows.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"policy\": str(meta.get(\"policy\")),\n",
    "                \"seed\": int(meta.get(\"seed\", -1)),\n",
    "                \"hidden\": int(meta.get(\"hidden\", -1)),\n",
    "                \"feature_mode\": str(meta.get(\"feature_mode\")),\n",
    "                \"action_scale\": float(meta.get(\"action_scale\", float(\"nan\"))),\n",
    "                \"lr\": float(meta.get(\"lr\", float(\"nan\"))),\n",
    "                \"overlap_lambda\": float(meta.get(\"overlap_lambda\", float(\"nan\"))),\n",
    "                \"val_score\": val_score,\n",
    "                \"path\": str(out_path),\n",
    "                \"loss_last\": float(loss[-1]) if loss else float(\"nan\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if idx == 1 or idx % 5 == 0 or idx == len(planned):\n",
    "            print(f\"[l2o_sweep] {idx}/{len(planned)} done. best_val={min(r['val_score'] for r in sweep_rows):.6f}\")\n",
    "\n",
    "    sweep_rows = sorted(sweep_rows, key=lambda r: (float(r.get(\"val_score\") or float(\"inf\")), str(r.get(\"name\"))))\n",
    "    write_csv(sweep_dir / \"rank.csv\", sweep_rows)\n",
    "    (sweep_dir / \"rank.json\").write_text(json.dumps(sweep_rows, indent=2))\n",
    "\n",
    "    def _topk(policy: str) -> List[Dict[str, object]]:\n",
    "        items = [r for r in sweep_rows if str(r.get(\"policy\")) == policy]\n",
    "        return items[: int(max(L2O_SWEEP_TOPK_PER_POLICY, 0))]\n",
    "\n",
    "    top_mlp = _topk(\"mlp\")\n",
    "    top_gnn = _topk(\"gnn\")\n",
    "    for r in top_mlp + top_gnn:\n",
    "        name = str(r[\"name\"])\n",
    "        L2O_SWEEP_TOP_MODELS[name] = Path(str(r[\"path\"]))\n",
    "\n",
    "    best_mlp = top_mlp[0] if top_mlp else None\n",
    "    best_gnn = top_gnn[0] if top_gnn else None\n",
    "    (sweep_dir / \"best.json\").write_text(json.dumps({\"best_mlp\": best_mlp, \"best_gnn\": best_gnn}, indent=2))\n",
    "    (sweep_dir / \"top_models.json\").write_text(json.dumps({k: str(v) for k, v in L2O_SWEEP_TOP_MODELS.items()}, indent=2))\n",
    "\n",
    "    def _fmt_top(items: List[Dict[str, object]], k: int = 5) -> str:\n",
    "        parts = []\n",
    "        for r in items[:k]:\n",
    "            parts.append(f\"{r['name']}:{float(r['val_score']):.6f}\")\n",
    "        return \", \".join(parts) if parts else \"(none)\"\n",
    "\n",
    "    print(f\"[l2o_sweep] top_mlp: {_fmt_top(top_mlp)}\")\n",
    "    print(f\"[l2o_sweep] top_gnn: {_fmt_top(top_gnn)}\")\n",
    "\n",
    "    summary_lines = [\n",
    "        \"# L2O sweep summary\",\n",
    "        \"\",\n",
    "        f\"- experiments: {len(planned)}\",\n",
    "        f\"- eval split: sweep_val (n={list(L2O_SWEEP_EVAL_N_LIST)} seeds={list(L2O_SWEEP_EVAL_SEEDS)} steps={int(L2O_SWEEP_EVAL_STEPS)})\",\n",
    "        \"\",\n",
    "        \"## Top MLP\",\n",
    "    ]\n",
    "    for r in top_mlp[:10]:\n",
    "        summary_lines.append(f\"- {r['name']}: val_score={float(r['val_score']):.6f}  path={r['path']}\")\n",
    "    summary_lines.append(\"\")\n",
    "    summary_lines.append(\"## Top GNN\")\n",
    "    for r in top_gnn[:10]:\n",
    "        summary_lines.append(f\"- {r['name']}: val_score={float(r['val_score']):.6f}  path={r['path']}\")\n",
    "    (sweep_dir / \"summary.md\").write_text(\"\\n\".join(summary_lines) + \"\\n\")\n",
    "\n",
    "    if L2O_SWEEP_RETRAIN_FINAL and best_mlp is not None:\n",
    "        meta = load_params_npz(Path(str(best_mlp[\"path\"])))[1]\n",
    "        mlp_meta = dict(meta)\n",
    "        mlp_params, mlp_loss = train_l2o_model_safe(\n",
    "            seed=int(mlp_meta.get(\"seed\", 1)),\n",
    "            n_list=TRAIN_N_LIST,\n",
    "            batch=BATCH,\n",
    "            train_steps=L2O_FINAL_TRAIN_STEPS,\n",
    "            steps=L2O_FINAL_ROLLOUT_STEPS,\n",
    "            lr=float(mlp_meta.get(\"lr\", 1e-3)),\n",
    "            hidden_size=int(mlp_meta.get(\"hidden\", HIDDEN_SIZE)),\n",
    "            policy=\"mlp\",\n",
    "            reward=REWARD,\n",
    "            action_scale=float(mlp_meta.get(\"action_scale\", ACTION_SCALE)),\n",
    "            mlp_depth=int(mlp_meta.get(\"mlp_depth\", MLP_DEPTH)),\n",
    "            init_mode=TRAIN_INIT_MODE,\n",
    "            rand_scale=RAND_SCALE,\n",
    "            lattice_pattern=TRAIN_LATTICE_PATTERN,\n",
    "            lattice_margin=TRAIN_LATTICE_MARGIN,\n",
    "            lattice_rotate=TRAIN_LATTICE_ROTATE,\n",
    "            baseline_mode=str(mlp_meta.get(\"baseline_mode\", BASELINE_MODE)),\n",
    "            baseline_decay=float(mlp_meta.get(\"baseline_decay\", BASELINE_DECAY)),\n",
    "            curriculum=TRAIN_CURRICULUM,\n",
    "            curriculum_start_max=TRAIN_CURRICULUM_START_MAX,\n",
    "            curriculum_end_max=TRAIN_CURRICULUM_END_MAX,\n",
    "            curriculum_steps=TRAIN_CURRICULUM_STEPS,\n",
    "            feature_mode=str(mlp_meta.get(\"feature_mode\", FEATURE_MODE)),\n",
    "            overlap_lambda=float(mlp_meta.get(\"overlap_lambda\", 0.0)),\n",
    "            verbose_freq=10,\n",
    "        )\n",
    "    else:\n",
    "        mlp_params, mlp_loss = train_l2o_model_safe(\n",
    "            seed=1,\n",
    "            n_list=TRAIN_N_LIST,\n",
    "            batch=BATCH,\n",
    "            train_steps=TRAIN_STEPS,\n",
    "            steps=ROLLOUT_STEPS,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            policy=\"mlp\",\n",
    "            reward=REWARD,\n",
    "            action_scale=ACTION_SCALE,\n",
    "            mlp_depth=MLP_DEPTH,\n",
    "            gnn_steps=GNN_STEPS,\n",
    "            gnn_attention=GNN_ATTENTION,\n",
    "            init_mode=TRAIN_INIT_MODE,\n",
    "            rand_scale=RAND_SCALE,\n",
    "            lattice_pattern=TRAIN_LATTICE_PATTERN,\n",
    "            lattice_margin=TRAIN_LATTICE_MARGIN,\n",
    "            lattice_rotate=TRAIN_LATTICE_ROTATE,\n",
    "            baseline_mode=BASELINE_MODE,\n",
    "            baseline_decay=BASELINE_DECAY,\n",
    "            curriculum=TRAIN_CURRICULUM,\n",
    "            curriculum_start_max=TRAIN_CURRICULUM_START_MAX,\n",
    "            curriculum_end_max=TRAIN_CURRICULUM_END_MAX,\n",
    "            curriculum_steps=TRAIN_CURRICULUM_STEPS,\n",
    "            feature_mode=FEATURE_MODE,\n",
    "            verbose_freq=10,\n",
    "        )\n",
    "\n",
    "    if L2O_SWEEP_RETRAIN_FINAL and best_gnn is not None:\n",
    "        meta = load_params_npz(Path(str(best_gnn[\"path\"])))[1]\n",
    "        gnn_meta = dict(meta)\n",
    "        gnn_params, gnn_loss = train_l2o_model_safe(\n",
    "            seed=int(gnn_meta.get(\"seed\", 2)),\n",
    "            n_list=TRAIN_N_LIST,\n",
    "            batch=BATCH,\n",
    "            train_steps=L2O_FINAL_TRAIN_STEPS,\n",
    "            steps=L2O_FINAL_ROLLOUT_STEPS,\n",
    "            lr=float(gnn_meta.get(\"lr\", 1e-3)),\n",
    "            hidden_size=int(gnn_meta.get(\"hidden\", HIDDEN_SIZE)),\n",
    "            policy=\"gnn\",\n",
    "            reward=REWARD,\n",
    "            action_scale=float(gnn_meta.get(\"action_scale\", ACTION_SCALE)),\n",
    "            knn_k=int(gnn_meta.get(\"knn_k\", 4)),\n",
    "            mlp_depth=int(gnn_meta.get(\"mlp_depth\", MLP_DEPTH)),\n",
    "            gnn_steps=int(gnn_meta.get(\"gnn_steps\", GNN_STEPS)),\n",
    "            gnn_attention=bool(gnn_meta.get(\"gnn_attention\", False)),\n",
    "            init_mode=TRAIN_INIT_MODE,\n",
    "            rand_scale=RAND_SCALE,\n",
    "            lattice_pattern=TRAIN_LATTICE_PATTERN,\n",
    "            lattice_margin=TRAIN_LATTICE_MARGIN,\n",
    "            lattice_rotate=TRAIN_LATTICE_ROTATE,\n",
    "            baseline_mode=str(gnn_meta.get(\"baseline_mode\", BASELINE_MODE)),\n",
    "            baseline_decay=float(gnn_meta.get(\"baseline_decay\", BASELINE_DECAY)),\n",
    "            curriculum=TRAIN_CURRICULUM,\n",
    "            curriculum_start_max=TRAIN_CURRICULUM_START_MAX,\n",
    "            curriculum_end_max=TRAIN_CURRICULUM_END_MAX,\n",
    "            curriculum_steps=TRAIN_CURRICULUM_STEPS,\n",
    "            feature_mode=str(gnn_meta.get(\"feature_mode\", FEATURE_MODE)),\n",
    "            overlap_lambda=float(gnn_meta.get(\"overlap_lambda\", 0.0)),\n",
    "            verbose_freq=10,\n",
    "        )\n",
    "    else:\n",
    "        gnn_params, gnn_loss = train_l2o_model_safe(\n",
    "            seed=2,\n",
    "            n_list=TRAIN_N_LIST,\n",
    "            batch=BATCH,\n",
    "            train_steps=TRAIN_STEPS,\n",
    "            steps=ROLLOUT_STEPS,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            policy=\"gnn\",\n",
    "            reward=REWARD,\n",
    "            action_scale=ACTION_SCALE,\n",
    "            knn_k=4,\n",
    "            mlp_depth=MLP_DEPTH,\n",
    "            gnn_steps=GNN_STEPS,\n",
    "            gnn_attention=GNN_ATTENTION,\n",
    "            init_mode=TRAIN_INIT_MODE,\n",
    "            rand_scale=RAND_SCALE,\n",
    "            lattice_pattern=TRAIN_LATTICE_PATTERN,\n",
    "            lattice_margin=TRAIN_LATTICE_MARGIN,\n",
    "            lattice_rotate=TRAIN_LATTICE_ROTATE,\n",
    "            baseline_mode=BASELINE_MODE,\n",
    "            baseline_decay=BASELINE_DECAY,\n",
    "            curriculum=TRAIN_CURRICULUM,\n",
    "            curriculum_start_max=TRAIN_CURRICULUM_START_MAX,\n",
    "            curriculum_end_max=TRAIN_CURRICULUM_END_MAX,\n",
    "            curriculum_steps=TRAIN_CURRICULUM_STEPS,\n",
    "            feature_mode=FEATURE_MODE,\n",
    "            verbose_freq=10,\n",
    "        )\n",
    "else:\n",
    "    # Baseline (sem sweep)\n",
    "    mlp_params, mlp_loss = train_l2o_model_safe(\n",
    "        seed=1,\n",
    "        n_list=TRAIN_N_LIST,\n",
    "        batch=BATCH,\n",
    "        train_steps=TRAIN_STEPS,\n",
    "        steps=ROLLOUT_STEPS,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        policy=\"mlp\",\n",
    "        reward=REWARD,\n",
    "        action_scale=ACTION_SCALE,\n",
    "        mlp_depth=MLP_DEPTH,\n",
    "        gnn_steps=GNN_STEPS,\n",
    "        gnn_attention=GNN_ATTENTION,\n",
    "        init_mode=TRAIN_INIT_MODE,\n",
    "        rand_scale=RAND_SCALE,\n",
    "        lattice_pattern=TRAIN_LATTICE_PATTERN,\n",
    "        lattice_margin=TRAIN_LATTICE_MARGIN,\n",
    "        lattice_rotate=TRAIN_LATTICE_ROTATE,\n",
    "        baseline_mode=BASELINE_MODE,\n",
    "        baseline_decay=BASELINE_DECAY,\n",
    "        curriculum=TRAIN_CURRICULUM,\n",
    "        curriculum_start_max=TRAIN_CURRICULUM_START_MAX,\n",
    "        curriculum_end_max=TRAIN_CURRICULUM_END_MAX,\n",
    "        curriculum_steps=TRAIN_CURRICULUM_STEPS,\n",
    "        feature_mode=FEATURE_MODE,\n",
    "        verbose_freq=10,\n",
    "    )\n",
    "\n",
    "    gnn_params, gnn_loss = train_l2o_model_safe(\n",
    "        seed=2,\n",
    "        n_list=TRAIN_N_LIST,\n",
    "        batch=BATCH,\n",
    "        train_steps=TRAIN_STEPS,\n",
    "        steps=ROLLOUT_STEPS,\n",
    "        hidden_size=HIDDEN_SIZE,\n",
    "        policy=\"gnn\",\n",
    "        reward=REWARD,\n",
    "        action_scale=ACTION_SCALE,\n",
    "        knn_k=4,\n",
    "        mlp_depth=MLP_DEPTH,\n",
    "        gnn_steps=GNN_STEPS,\n",
    "        gnn_attention=GNN_ATTENTION,\n",
    "        init_mode=TRAIN_INIT_MODE,\n",
    "        rand_scale=RAND_SCALE,\n",
    "        lattice_pattern=TRAIN_LATTICE_PATTERN,\n",
    "        lattice_margin=TRAIN_LATTICE_MARGIN,\n",
    "        lattice_rotate=TRAIN_LATTICE_ROTATE,\n",
    "        baseline_mode=BASELINE_MODE,\n",
    "        baseline_decay=BASELINE_DECAY,\n",
    "        curriculum=TRAIN_CURRICULUM,\n",
    "        curriculum_start_max=TRAIN_CURRICULUM_START_MAX,\n",
    "        curriculum_end_max=TRAIN_CURRICULUM_END_MAX,\n",
    "        curriculum_steps=TRAIN_CURRICULUM_STEPS,\n",
    "        feature_mode=FEATURE_MODE,\n",
    "        verbose_freq=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add05513",
   "metadata": {
    "title": "Plot losses"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(mlp_loss, label=\"MLP\")\n",
    "plt.plot(gnn_loss, label=\"GNN\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"L2O Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(RUN_DIR / \"loss_curve.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db91ec0",
   "metadata": {
    "title": "Visualizacao de packing (MLP)"
   },
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "config = l2o_config_from_meta(mlp_meta, reward=REWARD, deterministic=True)\n",
    "mlp_poses = optimize_with_l2o(key, mlp_params, jnp.array(init), ROLLOUT_STEPS, config)\n",
    "plot_packing(np.array(mlp_poses), \"MLP packing\", RUN_DIR / \"mlp_packing.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ce906",
   "metadata": {
    "title": "Visualizacao de packing (GNN)"
   },
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "config = l2o_config_from_meta(gnn_meta, reward=REWARD, deterministic=True)\n",
    "gnn_poses = optimize_with_l2o(key, gnn_params, jnp.array(init), ROLLOUT_STEPS, config)\n",
    "plot_packing(np.array(gnn_poses), \"GNN packing\", RUN_DIR / \"gnn_packing.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f186918",
   "metadata": {
    "title": "Pipelines opcionais (BC / meta / heatmap)"
   },
   "outputs": [],
   "source": [
    "bc_policy_path: Path | None = Path(BC_POLICY_PATH) if BC_POLICY_PATH else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab214d",
   "metadata": {
    "title": "Pipeline opcional: BC (imitation learning)"
   },
   "outputs": [],
   "source": [
    "if RUN_BC_PIPELINE:\n",
    "    bc_dataset = Path(BC_DATASET_PATH) if BC_DATASET_PATH else RUN_DIR / \"bc_dataset.npz\"\n",
    "    bc_policy_path = Path(BC_POLICY_PATH) if BC_POLICY_PATH else RUN_DIR / \"bc_policy.npz\"\n",
    "    run_cmd(\n",
    "        [\n",
    "            sys.executable,\n",
    "            str(ROOT / \"scripts\" / \"data\" / \"collect_sa_dataset.py\"),\n",
    "            \"--n-list\",\n",
    "            \",\".join(str(n) for n in TRAIN_N_LIST),\n",
    "            \"--runs-per-n\",\n",
    "            str(BC_RUNS_PER_N),\n",
    "            \"--steps\",\n",
    "            str(BC_STEPS),\n",
    "            \"--seed\",\n",
    "            str(BC_SEED),\n",
    "            \"--init\",\n",
    "            BC_INIT_MODE,\n",
    "            \"--rand-scale\",\n",
    "            str(BC_RAND_SCALE),\n",
    "            \"--lattice-pattern\",\n",
    "            BC_LATTICE_PATTERN,\n",
    "            \"--lattice-margin\",\n",
    "            str(BC_LATTICE_MARGIN),\n",
    "            \"--lattice-rotate\",\n",
    "            str(BC_LATTICE_ROTATE),\n",
    "            \"--out\",\n",
    "            str(bc_dataset),\n",
    "        ]\n",
    "    )\n",
    "    train_cmd = [\n",
    "        sys.executable,\n",
    "        str(ROOT / \"scripts\" / \"training\" / \"train_l2o_bc.py\"),\n",
    "        \"--dataset\",\n",
    "        str(bc_dataset),\n",
    "        \"--policy\",\n",
    "        BC_POLICY,\n",
    "        \"--knn-k\",\n",
    "        str(BC_KNN_K),\n",
    "        \"--train-steps\",\n",
    "        str(BC_TRAIN_STEPS),\n",
    "        \"--seed\",\n",
    "        str(BC_SEED + 1),\n",
    "        \"--reward\",\n",
    "        REWARD,\n",
    "        \"--hidden\",\n",
    "        str(HIDDEN_SIZE),\n",
    "        \"--mlp-depth\",\n",
    "        str(MLP_DEPTH),\n",
    "        \"--gnn-steps\",\n",
    "        str(GNN_STEPS),\n",
    "        \"--feature-mode\",\n",
    "        FEATURE_MODE,\n",
    "        \"--out\",\n",
    "        str(bc_policy_path),\n",
    "    ]\n",
    "    if BC_CURRICULUM:\n",
    "        train_cmd.append(\"--curriculum\")\n",
    "    if BC_CURRICULUM_START_MAX is not None:\n",
    "        train_cmd += [\"--curriculum-start-max\", str(int(BC_CURRICULUM_START_MAX))]\n",
    "    if BC_CURRICULUM_END_MAX is not None:\n",
    "        train_cmd += [\"--curriculum-end-max\", str(int(BC_CURRICULUM_END_MAX))]\n",
    "    if BC_CURRICULUM_STEPS is not None:\n",
    "        train_cmd += [\"--curriculum-steps\", str(int(BC_CURRICULUM_STEPS))]\n",
    "    if GNN_ATTENTION:\n",
    "        train_cmd.append(\"--gnn-attention\")\n",
    "    run_cmd(train_cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f90fbc",
   "metadata": {
    "title": "Pipeline opcional: meta-init"
   },
   "outputs": [],
   "source": [
    "meta_init_path: Path | None = Path(META_INIT_MODEL_PATH) if META_INIT_MODEL_PATH else None\n",
    "if RUN_META_TRAIN:\n",
    "    meta_init_path = Path(META_INIT_MODEL_PATH) if META_INIT_MODEL_PATH else RUN_DIR / \"meta_init.npz\"\n",
    "    run_cmd(\n",
    "        [\n",
    "            sys.executable,\n",
    "            str(ROOT / \"scripts\" / \"training\" / \"train_meta_init.py\"),\n",
    "            \"--n-list\",\n",
    "            \",\".join(str(n) for n in TRAIN_N_LIST),\n",
    "            \"--train-steps\",\n",
    "            str(META_TRAIN_STEPS),\n",
    "            \"--es-pop\",\n",
    "            str(META_ES_POP),\n",
    "            \"--sa-steps\",\n",
    "            str(META_SA_STEPS),\n",
    "            \"--out\",\n",
    "            str(meta_init_path),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608b723",
   "metadata": {
    "title": "Pipeline opcional: heatmap"
   },
   "outputs": [],
   "source": [
    "heatmap_path: Path | None = Path(HEATMAP_MODEL_PATH) if HEATMAP_MODEL_PATH else None\n",
    "if RUN_HEATMAP_TRAIN:\n",
    "    heatmap_path = Path(HEATMAP_MODEL_PATH) if HEATMAP_MODEL_PATH else RUN_DIR / \"heatmap_meta.npz\"\n",
    "    run_cmd(\n",
    "        [\n",
    "            sys.executable,\n",
    "            str(ROOT / \"scripts\" / \"training\" / \"train_heatmap_meta.py\"),\n",
    "            \"--n-list\",\n",
    "            \",\".join(str(n) for n in TRAIN_N_LIST),\n",
    "            \"--train-steps\",\n",
    "            str(HEATMAP_TRAIN_STEPS),\n",
    "            \"--es-pop\",\n",
    "            str(HEATMAP_ES_POP),\n",
    "            \"--heatmap-steps\",\n",
    "            str(HEATMAP_STEPS),\n",
    "            \"--policy\",\n",
    "            \"gnn\",\n",
    "            \"--knn-k\",\n",
    "            str(BC_KNN_K),\n",
    "            \"--out\",\n",
    "            str(heatmap_path),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734a8ce",
   "metadata": {
    "title": "Solvers e configuracao de avaliacao"
   },
   "outputs": [],
   "source": [
    "if set(TRAIN_N_LIST) & set(VAL_N_LIST):\n",
    "    raise ValueError(\"TRAIN_N_LIST and VAL_N_LIST must be disjoint to avoid leakage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce6fe1",
   "metadata": {
    "title": "Modelos opcionais carregados de disco"
   },
   "outputs": [],
   "source": [
    "bc_params = None\n",
    "bc_config = None\n",
    "if bc_policy_path is not None and bc_policy_path.exists():\n",
    "    bc_params, bc_meta = load_params_npz(bc_policy_path)\n",
    "    bc_config = l2o_config_from_meta(bc_meta, reward=REWARD, deterministic=True)\n",
    "\n",
    "def solve_meta_init_sa(n: int, seed: int) -> np.ndarray:\n",
    "    if meta_init_path is None or not meta_init_path.exists():\n",
    "        return solve_sa(n, seed)\n",
    "    try:\n",
    "        from santa_packing.meta_init import MetaInitConfig, apply_meta_init, load_meta_params  # noqa: E402\n",
    "    except Exception:\n",
    "        return solve_sa(n, seed)\n",
    "    params, meta = load_meta_params(meta_init_path)\n",
    "    config = MetaInitConfig(\n",
    "        hidden_size=int(meta.get(\"hidden\", 32)) if hasattr(meta.get(\"hidden\", 32), \"__int__\") else 32,\n",
    "        delta_xy=float(meta.get(\"delta_xy\", 0.2)),\n",
    "        delta_theta=float(meta.get(\"delta_theta\", 10.0)),\n",
    "    )\n",
    "    init_pose = get_initial(n, seed)\n",
    "    init_pose = np.array(apply_meta_init(params, jnp.array(init_pose), config))\n",
    "    init_batch = jnp.array(init_pose)[None, :, :]\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    best_poses, _ = run_sa_batch(\n",
    "        key,\n",
    "        SA_STEPS,\n",
    "        n,\n",
    "        init_batch,\n",
    "        trans_sigma=SA_TRANS_SIGMA,\n",
    "        rot_sigma=SA_ROT_SIGMA,\n",
    "        rot_prob=SA_ROT_PROB,\n",
    "        objective=SA_OBJECTIVE,\n",
    "    )\n",
    "    return np.array(best_poses[0])\n",
    "\n",
    "\n",
    "def solve_heatmap(n: int, seed: int) -> np.ndarray:\n",
    "    if heatmap_path is None or not heatmap_path.exists():\n",
    "        return solve_grid(n, seed)\n",
    "    try:\n",
    "        from santa_packing.heatmap_meta import HeatmapConfig, heatmap_search, load_params  # noqa: E402\n",
    "    except Exception:\n",
    "        return solve_grid(n, seed)\n",
    "    params, meta = load_params(heatmap_path)\n",
    "    config = HeatmapConfig(\n",
    "        hidden_size=int(meta.get(\"hidden\", 32)) if hasattr(meta.get(\"hidden\", 32), \"__int__\") else 32,\n",
    "        policy=str(meta.get(\"policy\", \"gnn\")),\n",
    "        knn_k=int(meta.get(\"knn_k\", 4)) if hasattr(meta.get(\"knn_k\", 4), \"__int__\") else 4,\n",
    "        heatmap_lr=float(meta.get(\"heatmap_lr\", 0.1)),\n",
    "        trans_sigma=float(meta.get(\"trans_sigma\", 0.2)),\n",
    "        rot_sigma=float(meta.get(\"rot_sigma\", 10.0)),\n",
    "    )\n",
    "    base = get_initial(n, seed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    poses, _ = heatmap_search(params, base, config, HEATMAP_STEPS, rng)\n",
    "    return np.array(poses)\n",
    "\n",
    "\n",
    "def solve_ensemble(candidates: Dict[str, Callable[[int, int], np.ndarray]]) -> Callable[[int, int], Tuple[np.ndarray, Dict[str, str]]]:\n",
    "    def _solve(n: int, seed: int) -> Tuple[np.ndarray, Dict[str, str]]:\n",
    "        best_score = float(\"inf\")\n",
    "        best_pose: np.ndarray | None = None\n",
    "        best_name = \"none\"\n",
    "        for name, fn in candidates.items():\n",
    "            poses = fn(n, seed)\n",
    "            if ENSEMBLE_SCORE == \"packing\":\n",
    "                score = packing_score(points, poses)\n",
    "            else:\n",
    "                score = prefix_packing_score_np(points, poses)\n",
    "            if score < best_score:\n",
    "                best_score = float(score)\n",
    "                best_pose = poses\n",
    "                best_name = name\n",
    "        if best_pose is None:\n",
    "            best_pose = candidates[next(iter(candidates))](n, seed)\n",
    "        return best_pose, {\"selected\": best_name}\n",
    "\n",
    "    return _solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a095b86",
   "metadata": {
    "title": "Rodar avaliacao (configs)"
   },
   "outputs": [],
   "source": [
    "results: List[Dict[str, float]] = []\n",
    "l2o_mlp_cfg = l2o_config_from_meta(mlp_meta, reward=REWARD, deterministic=True)\n",
    "l2o_gnn_cfg = l2o_config_from_meta(gnn_meta, reward=REWARD, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f5ddc0",
   "metadata": {
    "title": "Avaliacao: baselines"
   },
   "outputs": [],
   "source": [
    "results += evaluate_solver(\"grid\", solve_grid, TRAIN_N_LIST, TRAIN_EVAL_SEEDS, points, split=\"train\")\n",
    "results += evaluate_solver(\"grid\", solve_grid, VAL_N_LIST, VAL_EVAL_SEEDS, points, split=\"val\")\n",
    "results += evaluate_solver(\"sa\", solve_sa, TRAIN_N_LIST, TRAIN_EVAL_SEEDS, points, split=\"train\")\n",
    "results += evaluate_solver(\"sa\", solve_sa, VAL_N_LIST, VAL_EVAL_SEEDS, points, split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5790bac3",
   "metadata": {
    "title": "Avaliacao: L2O"
   },
   "outputs": [],
   "source": [
    "results += evaluate_solver(\n",
    "    \"l2o_mlp\",\n",
    "    solve_l2o(mlp_params, l2o_mlp_cfg),\n",
    "    TRAIN_N_LIST,\n",
    "    TRAIN_EVAL_SEEDS,\n",
    "    points,\n",
    "    split=\"train\",\n",
    ")\n",
    "results += evaluate_solver(\n",
    "    \"l2o_gnn\",\n",
    "    solve_l2o(gnn_params, l2o_gnn_cfg),\n",
    "    TRAIN_N_LIST,\n",
    "    TRAIN_EVAL_SEEDS,\n",
    "    points,\n",
    "    split=\"train\",\n",
    ")\n",
    "results += evaluate_solver(\n",
    "    \"l2o_mlp\",\n",
    "    solve_l2o(mlp_params, l2o_mlp_cfg),\n",
    "    VAL_N_LIST,\n",
    "    VAL_EVAL_SEEDS,\n",
    "    points,\n",
    "    split=\"val\",\n",
    ")\n",
    "results += evaluate_solver(\n",
    "    \"l2o_gnn\",\n",
    "    solve_l2o(gnn_params, l2o_gnn_cfg),\n",
    "    VAL_N_LIST,\n",
    "    VAL_EVAL_SEEDS,\n",
    "    points,\n",
    "    split=\"val\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b45d96",
   "metadata": {
    "title": "Avaliacao: modelos opcionais"
   },
   "outputs": [],
   "source": [
    "if bc_params is not None and bc_config is not None:\n",
    "    results += evaluate_solver(\n",
    "        \"l2o_bc\",\n",
    "        solve_l2o(bc_params, bc_config),\n",
    "        TRAIN_N_LIST,\n",
    "        TRAIN_EVAL_SEEDS,\n",
    "        points,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    results += evaluate_solver(\n",
    "        \"l2o_bc\",\n",
    "        solve_l2o(bc_params, bc_config),\n",
    "        VAL_N_LIST,\n",
    "        VAL_EVAL_SEEDS,\n",
    "        points,\n",
    "        split=\"val\",\n",
    "    )\n",
    "\n",
    "if meta_init_path is not None and meta_init_path.exists():\n",
    "    results += evaluate_solver(\n",
    "        \"sa_meta_init\",\n",
    "        solve_meta_init_sa,\n",
    "        TRAIN_N_LIST,\n",
    "        TRAIN_EVAL_SEEDS,\n",
    "        points,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    results += evaluate_solver(\n",
    "        \"sa_meta_init\",\n",
    "        solve_meta_init_sa,\n",
    "        VAL_N_LIST,\n",
    "        VAL_EVAL_SEEDS,\n",
    "        points,\n",
    "        split=\"val\",\n",
    "    )\n",
    "\n",
    "if heatmap_path is not None and heatmap_path.exists():\n",
    "    results += evaluate_solver(\n",
    "        \"heatmap\",\n",
    "        solve_heatmap,\n",
    "        TRAIN_N_LIST,\n",
    "        TRAIN_EVAL_SEEDS,\n",
    "        points,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    results += evaluate_solver(\n",
    "        \"heatmap\",\n",
    "        solve_heatmap,\n",
    "        VAL_N_LIST,\n",
    "        VAL_EVAL_SEEDS,\n",
    "        points,\n",
    "        split=\"val\",\n",
    "    )\n",
    "\n",
    "if L2O_REFINE_GRID:\n",
    "    results += evaluate_solver(\n",
    "        \"l2o_refine_grid\",\n",
    "        solve_l2o_refine(solve_grid, mlp_params, l2o_mlp_cfg),\n",
    "        TRAIN_N_LIST,\n",
    "        TRAIN_EVAL_SEEDS,\n",
    "        points,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    results += evaluate_solver(\n",
    "        \"l2o_refine_grid\",\n",
    "        solve_l2o_refine(solve_grid, mlp_params, l2o_mlp_cfg),\n",
    "        VAL_N_LIST,\n",
    "        VAL_EVAL_SEEDS,\n",
    "        points,\n",
    "        split=\"val\",\n",
    "    )\n",
    "\n",
    "if L2O_REFINE_SA:\n",
    "    results += evaluate_solver(\n",
    "        \"l2o_refine_sa\",\n",
    "        solve_l2o_refine(solve_sa, gnn_params, l2o_gnn_cfg),\n",
    "        TRAIN_N_LIST,\n",
    "        TRAIN_EVAL_SEEDS,\n",
    "        points,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    results += evaluate_solver(\n",
    "        \"l2o_refine_sa\",\n",
    "        solve_l2o_refine(solve_sa, gnn_params, l2o_gnn_cfg),\n",
    "        VAL_N_LIST,\n",
    "        VAL_EVAL_SEEDS,\n",
    "        points,\n",
    "        split=\"val\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb9801",
   "metadata": {
    "title": "Avaliacao: ensemble"
   },
   "outputs": [],
   "source": [
    "if RUN_ENSEMBLE:\n",
    "    ensemble_candidates = {\n",
    "        \"grid\": solve_grid,\n",
    "        \"sa\": solve_sa,\n",
    "        \"l2o_mlp\": solve_l2o(mlp_params, l2o_mlp_cfg),\n",
    "        \"l2o_gnn\": solve_l2o(gnn_params, l2o_gnn_cfg),\n",
    "    }\n",
    "    if bc_params is not None and bc_config is not None:\n",
    "        ensemble_candidates[\"l2o_bc\"] = solve_l2o(bc_params, bc_config)\n",
    "    if meta_init_path is not None and meta_init_path.exists():\n",
    "        ensemble_candidates[\"sa_meta_init\"] = solve_meta_init_sa\n",
    "    if heatmap_path is not None and heatmap_path.exists():\n",
    "        ensemble_candidates[\"heatmap\"] = solve_heatmap\n",
    "    results += evaluate_solver(\n",
    "        \"ensemble\",\n",
    "        solve_ensemble(ensemble_candidates),\n",
    "        TRAIN_N_LIST,\n",
    "        TRAIN_EVAL_SEEDS,\n",
    "        points,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    results += evaluate_solver(\n",
    "        \"ensemble\",\n",
    "        solve_ensemble(ensemble_candidates),\n",
    "        VAL_N_LIST,\n",
    "        VAL_EVAL_SEEDS,\n",
    "        points,\n",
    "        split=\"val\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0299e31",
   "metadata": {
    "title": "Resumos e artifacts"
   },
   "outputs": [],
   "source": [
    "per_n = summarize_results(results)\n",
    "overall = summarize_overall(results)\n",
    "\n",
    "meta = {\n",
    "    \"reward\": REWARD,\n",
    "    \"train_n_list\": TRAIN_N_LIST,\n",
    "    \"val_n_list\": VAL_N_LIST,\n",
    "    \"train_eval_seeds\": TRAIN_EVAL_SEEDS,\n",
    "    \"val_eval_seeds\": VAL_EVAL_SEEDS,\n",
    "    \"train_steps\": TRAIN_STEPS,\n",
    "    \"rollout_steps\": ROLLOUT_STEPS,\n",
    "    \"batch\": BATCH,\n",
    "    \"eval_steps\": EVAL_STEPS,\n",
    "    \"hidden_size\": HIDDEN_SIZE,\n",
    "    \"mlp_depth\": MLP_DEPTH,\n",
    "    \"gnn_steps\": GNN_STEPS,\n",
    "    \"gnn_attention\": GNN_ATTENTION,\n",
    "    \"action_scale\": ACTION_SCALE,\n",
    "    \"feature_mode\": FEATURE_MODE,\n",
    "    \"baseline_mode\": BASELINE_MODE,\n",
    "    \"baseline_decay\": BASELINE_DECAY,\n",
    "    \"init_mode\": INIT_MODE,\n",
    "    \"rand_scale\": RAND_SCALE,\n",
    "    \"sa_steps\": SA_STEPS,\n",
    "    \"sa_trans_sigma\": SA_TRANS_SIGMA,\n",
    "    \"sa_rot_sigma\": SA_ROT_SIGMA,\n",
    "    \"sa_rot_prob\": SA_ROT_PROB,\n",
    "    \"sa_objective\": SA_OBJECTIVE,\n",
    "    \"run_bc_pipeline\": RUN_BC_PIPELINE,\n",
    "    \"bc_policy\": BC_POLICY,\n",
    "    \"bc_runs_per_n\": BC_RUNS_PER_N,\n",
    "    \"bc_steps\": BC_STEPS,\n",
    "    \"bc_train_steps\": BC_TRAIN_STEPS,\n",
    "    \"bc_policy_path\": str(bc_policy_path) if bc_policy_path else None,\n",
    "    \"run_meta_train\": RUN_META_TRAIN,\n",
    "    \"meta_init_path\": str(meta_init_path) if meta_init_path else None,\n",
    "    \"run_heatmap_train\": RUN_HEATMAP_TRAIN,\n",
    "    \"heatmap_path\": str(heatmap_path) if heatmap_path else None,\n",
    "    \"run_ensemble\": RUN_ENSEMBLE,\n",
    "    \"ensemble_score\": ENSEMBLE_SCORE,\n",
    "    \"l2o_refine_grid\": L2O_REFINE_GRID,\n",
    "    \"l2o_refine_sa\": L2O_REFINE_SA,\n",
    "    \"refine_steps\": REFINE_STEPS,\n",
    "}\n",
    "\n",
    "save_eval_artifacts(RUN_DIR, results, per_n, overall, meta)\n",
    "plot_eval_curves(per_n, RUN_DIR / \"eval_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8cd0c3",
   "metadata": {
    "title": "Score GNN (proposta do desafio)"
   },
   "outputs": [],
   "source": [
    "gnn_train_score = challenge_score_from_results(results, \"l2o_gnn\", \"train\")\n",
    "gnn_val_score = challenge_score_from_results(results, \"l2o_gnn\", \"val\")\n",
    "(RUN_DIR / \"gnn_score.txt\").write_text(\n",
    "    f\"gnn_train_score={gnn_train_score:.6f}\\n\"\n",
    "    f\"gnn_val_score={gnn_val_score:.6f}\\n\"\n",
    ")\n",
    "\n",
    "print(\"GNN score (challenge-style):\")\n",
    "print(f\"  train={gnn_train_score:.6f}\")\n",
    "print(f\"  val={gnn_val_score:.6f}\")\n",
    "print(\"Eval artifacts saved to\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734ff62",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Gerar submission.csv (Kaggle)"
   },
   "outputs": [],
   "source": [
    "SUBMISSION_NMAX = 200\n",
    "SUBMISSION_SEED = 1\n",
    "SUBMISSION_OVERLAP_CHECK = True  # para score final, mantenha True\n",
    "\n",
    "RUN_SUBMISSION_SWEEP = False  # True = gera/score varias receitas + seeds\n",
    "SWEEP_NMAX = 50  # use 200 para score final\n",
    "SWEEP_SEEDS: List[int] | str = \"1..4\"\n",
    "SWEEP_SCORE_OVERLAP_CHECK = False  # durante sweep rapido, pode ser False; no final use True\n",
    "SWEEP_BUILD_ENSEMBLE = True\n",
    "SWEEP_JOBS = max(1, int(os.cpu_count() or 1))\n",
    "SWEEP_REUSE = True\n",
    "SWEEP_KEEP_GOING = True\n",
    "\n",
    "# Quanto maior, mais combinacoes (receitas) entram no sweep.\n",
    "# Use `None` para \"maximo\" (pode demorar bastante).\n",
    "RECIPES_MAX_RECIPES_PER_FAMILY = None\n",
    "RECIPES_MAX_LATTICE_VARIANTS = None\n",
    "\n",
    "# Salvar as politicas treinadas neste notebook (para usar no generate_submission/guided SA)\n",
    "MLP_POLICY_PATH = RUN_DIR / \"l2o_mlp.npz\"\n",
    "GNN_POLICY_PATH = RUN_DIR / \"l2o_gnn.npz\"\n",
    "save_params_npz(\n",
    "    MLP_POLICY_PATH,\n",
    "    mlp_params,\n",
    "    meta={**mlp_meta, \"reward\": REWARD},\n",
    ")\n",
    "save_params_npz(\n",
    "    GNN_POLICY_PATH,\n",
    "    gnn_params,\n",
    "    meta={**gnn_meta, \"reward\": REWARD},\n",
    ")\n",
    "\n",
    "\n",
    "def run_cmd_capture(cmd: List[str]) -> str:\n",
    "    print(\"$\", \" \".join(cmd))\n",
    "    result = subprocess.run(cmd, cwd=ROOT, text=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed with code {result.returncode}:\\n{result.stdout}\")\n",
    "    return result.stdout\n",
    "\n",
    "\n",
    "def score_csv(csv_path: Path, *, nmax: int, check_overlap: bool) -> Dict[str, object]:\n",
    "    cmd = [sys.executable, \"-m\", \"santa_packing.cli.score_submission\", str(csv_path), \"--nmax\", str(nmax)]\n",
    "    if not check_overlap:\n",
    "        cmd.append(\"--no-overlap\")\n",
    "    out = run_cmd_capture(cmd).strip()\n",
    "    return json.loads(out) if out else {}\n",
    "\n",
    "\n",
    "def generate_submission(\n",
    "    out_csv: Path,\n",
    "    *,\n",
    "    seed: int,\n",
    "    nmax: int,\n",
    "    args: Dict[str, object],\n",
    ") -> None:\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"santa_packing.cli.generate_submission\",\n",
    "        \"--out\",\n",
    "        str(out_csv),\n",
    "        \"--seed\",\n",
    "        str(seed),\n",
    "        \"--nmax\",\n",
    "        str(nmax),\n",
    "    ]\n",
    "    for key, value in args.items():\n",
    "        if value is None:\n",
    "            continue\n",
    "        flag = \"--\" + key.replace(\"_\", \"-\")\n",
    "        if isinstance(value, bool):\n",
    "            if value:\n",
    "                cmd.append(flag)\n",
    "            continue\n",
    "        cmd += [flag, str(value)]\n",
    "    run_cmd(cmd)\n",
    "\n",
    "\n",
    "def _best_per_puzzle_ensemble(\n",
    "    out_csv: Path,\n",
    "    candidates: Dict[str, Path],\n",
    "    *,\n",
    "    nmax: int,\n",
    "    check_overlap: bool,\n",
    ") -> Dict[str, object]:\n",
    "    try:\n",
    "        from santa_packing.scoring import load_submission  # noqa: E402\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\"Failed to import scoring.load_submission\") from exc\n",
    "    try:\n",
    "        from santa_packing.postopt_np import has_overlaps  # noqa: E402\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(\"Failed to import postopt_np.has_overlaps\") from exc\n",
    "\n",
    "    points = np.array(TREE_POINTS, dtype=float)\n",
    "    loaded = {name: load_submission(path, nmax=nmax) for name, path in candidates.items()}\n",
    "\n",
    "    selected: Dict[int, str] = {}\n",
    "    best_poses: Dict[int, np.ndarray] = {}\n",
    "    for n in range(1, nmax + 1):\n",
    "        best_s = float(\"inf\")\n",
    "        best_name = None\n",
    "        best_pose = None\n",
    "        for name, puzzles in loaded.items():\n",
    "            poses = puzzles.get(n)\n",
    "            if poses is None or poses.shape[0] != n:\n",
    "                continue\n",
    "            poses = np.array(poses, dtype=float, copy=True)\n",
    "            poses[:, 2] = np.mod(poses[:, 2], 360.0)\n",
    "            poses = shift_poses_to_origin(points, poses)\n",
    "            if check_overlap and has_overlaps(points, poses):\n",
    "                continue\n",
    "            s = float(packing_score(points, poses))\n",
    "            if s < best_s:\n",
    "                best_s = s\n",
    "                best_name = name\n",
    "                best_pose = poses\n",
    "        if best_name is None or best_pose is None:\n",
    "            raise ValueError(f\"No feasible candidates for puzzle {n} (check_overlap={check_overlap})\")\n",
    "        selected[n] = best_name\n",
    "        best_poses[n] = np.array(best_pose, dtype=float)\n",
    "\n",
    "    out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_csv.open(\"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\", \"x\", \"y\", \"deg\"])\n",
    "        for n in range(1, nmax + 1):\n",
    "            poses = np.array(best_poses[n], dtype=float, copy=True)\n",
    "            poses[:, 2] = np.mod(poses[:, 2], 360.0)\n",
    "            poses = shift_poses_to_origin(points, poses)\n",
    "            for i, (x, y, deg) in enumerate(poses):\n",
    "                writer.writerow([f\"{n:03d}_{i}\", f\"s{float(x):.17f}\", f\"s{float(y):.17f}\", f\"s{float(deg):.17f}\"])\n",
    "\n",
    "    return {\"selected_by_puzzle\": {str(k): v for k, v in selected.items()}}\n",
    "\n",
    "\n",
    "# === Modelos disponiveis (paths) ===\n",
    "META_INIT_MODEL = str(meta_init_path) if (meta_init_path is not None and meta_init_path.exists()) else None\n",
    "HEATMAP_MODEL = str(heatmap_path) if (heatmap_path is not None and heatmap_path.exists()) else None\n",
    "\n",
    "# L2O models trained in this run (plus optional BC policy).\n",
    "CANDIDATE_L2O_MODELS: Dict[str, Path] = {\n",
    "    \"reinforce_gnn\": GNN_POLICY_PATH,\n",
    "    \"reinforce_mlp\": MLP_POLICY_PATH,\n",
    "}\n",
    "if bc_policy_path is not None and bc_policy_path.exists():\n",
    "    CANDIDATE_L2O_MODELS[\"bc\"] = bc_policy_path\n",
    "\n",
    "# Opcional: inclui os melhores modelos do sweep (rapidos) para aumentar o portfolio.\n",
    "for name, path in sorted(L2O_SWEEP_TOP_MODELS.items()):\n",
    "    if path.exists():\n",
    "        CANDIDATE_L2O_MODELS[f\"sweep_{name}\"] = path\n",
    "CANDIDATE_GUIDED_MODELS: Dict[str, Path] = dict(CANDIDATE_L2O_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3935c3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Experimentos de submission (1 clula = 1 experimento)\n",
    "\n",
    "Objetivo: cada clula abaixo gera um `submission.csv` (via `python -m santa_packing.cli.generate_submission`)\n",
    "e j imprime o **score do desafio** (mtrica do Kaggle) calculado por `python -m santa_packing.cli.score_submission`.\n",
    "\n",
    "Dica: deixe `REUSE=True` para no re-gerar CSVs j existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce65c0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "SUBMISSION_NMAX = 200\n",
    "SUBMISSION_SEED = 1\n",
    "SUBMISSION_OVERLAP_CHECK = True\n",
    "REUSE = True\n",
    "\n",
    "SUBMISSION_CELL_DIR = RUN_DIR / \"submission_cells\"\n",
    "SUBMISSION_CELL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _sanitize_name(name: str) -> str:\n",
    "    out = []\n",
    "    for ch in str(name).strip():\n",
    "        if ch.isalnum() or ch in {\"-\", \"_\", \".\"}:\n",
    "            out.append(ch)\n",
    "        else:\n",
    "            out.append(\"_\")\n",
    "    cleaned = \"\".join(out).strip(\"._-\")\n",
    "    return cleaned or \"exp\"\n",
    "\n",
    "\n",
    "def _path_exists(value: object) -> bool:\n",
    "    if value is None:\n",
    "        return False\n",
    "    try:\n",
    "        p = Path(str(value))\n",
    "    except Exception:\n",
    "        return False\n",
    "    return p.exists()\n",
    "\n",
    "\n",
    "def _check_required_models(args: Dict[str, object]) -> tuple[bool, list[str]]:\n",
    "    missing: list[str] = []\n",
    "    for k, v in args.items():\n",
    "        if v is None:\n",
    "            continue\n",
    "        if k.endswith(\"_model\") or k == \"guided_model\":\n",
    "            if not _path_exists(v):\n",
    "                missing.append(k)\n",
    "    return (len(missing) == 0), missing\n",
    "\n",
    "\n",
    "def run_submission_experiment(\n",
    "    name: str,\n",
    "    args: Dict[str, object],\n",
    "    *,\n",
    "    seed: int = SUBMISSION_SEED,\n",
    "    nmax: int = SUBMISSION_NMAX,\n",
    "    check_overlap: bool = SUBMISSION_OVERLAP_CHECK,\n",
    "    reuse: bool = REUSE,\n",
    ") -> Dict[str, object] | None:\n",
    "    exp = _sanitize_name(name)\n",
    "    ok, missing = _check_required_models(args)\n",
    "    if not ok:\n",
    "        print(f\"[skip] {exp} (modelos ausentes: {missing})\")\n",
    "        return None\n",
    "\n",
    "    out_csv = SUBMISSION_CELL_DIR / f\"{exp}_seed{int(seed)}_n{int(nmax)}.csv\"\n",
    "    out_score = SUBMISSION_CELL_DIR / f\"{exp}_seed{int(seed)}_n{int(nmax)}.score.json\"\n",
    "    out_meta = SUBMISSION_CELL_DIR / f\"{exp}_seed{int(seed)}_n{int(nmax)}.meta.json\"\n",
    "\n",
    "    if not (reuse and out_csv.exists()):\n",
    "        generate_submission(out_csv, seed=int(seed), nmax=int(nmax), args=args)\n",
    "\n",
    "    score = score_csv(out_csv, nmax=int(nmax), check_overlap=bool(check_overlap))\n",
    "    out_score.write_text(json.dumps(score, indent=2), encoding=\"utf-8\")\n",
    "    out_meta.write_text(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"exp\": exp,\n",
    "                \"seed\": int(seed),\n",
    "                \"nmax\": int(nmax),\n",
    "                \"check_overlap\": bool(check_overlap),\n",
    "                \"csv\": str(out_csv),\n",
    "                \"args\": args,\n",
    "            },\n",
    "            indent=2,\n",
    "            default=str,\n",
    "        ),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    val = score.get(\"score\")\n",
    "    s_max = score.get(\"s_max\")\n",
    "    print(f\"[{exp}] score={val}  s_max={s_max}  csv={out_csv.name}\")\n",
    "    return score\n",
    "\n",
    "\n",
    "# Base \"vazio\": lattice como fallback. Cada experimento sobrescreve o que precisa.\n",
    "BASE_ARGS: Dict[str, object] = {\n",
    "    \"mother_prefix\": False,\n",
    "    \"mother_reorder\": \"radial\",\n",
    "    \"lattice_pattern\": \"hex\",\n",
    "    \"lattice_margin\": 0.02,\n",
    "    \"lattice_rotate\": 0.0,\n",
    "    \"lattice_rotations\": \"0,15,30\",\n",
    "    \"sa_nmax\": 0,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_steps\": 400,\n",
    "    \"sa_trans_sigma\": 0.2,\n",
    "    \"sa_rot_sigma\": 15.0,\n",
    "    \"sa_rot_prob\": 0.3,\n",
    "    \"sa_rot_prob_end\": -1.0,\n",
    "    \"sa_swap_prob\": 0.0,\n",
    "    \"sa_swap_prob_end\": -1.0,\n",
    "    \"sa_cooling\": \"geom\",\n",
    "    \"sa_cooling_power\": 1.0,\n",
    "    \"sa_trans_nexp\": 0.0,\n",
    "    \"sa_rot_nexp\": 0.0,\n",
    "    \"sa_sigma_nref\": 50.0,\n",
    "    \"sa_objective\": \"packing\",\n",
    "    \"sa_proposal\": \"random\",\n",
    "    \"sa_smart_prob\": 1.0,\n",
    "    \"sa_smart_beta\": 8.0,\n",
    "    \"sa_smart_drift\": 1.0,\n",
    "    \"sa_smart_noise\": 0.25,\n",
    "    \"sa_overlap_lambda\": 0.0,\n",
    "    \"sa_allow_collisions\": False,\n",
    "    \"meta_init_model\": None,\n",
    "    \"heatmap_model\": None,\n",
    "    \"heatmap_nmax\": 0,\n",
    "    \"heatmap_steps\": 200,\n",
    "    \"l2o_model\": None,\n",
    "    \"l2o_init\": \"lattice\",\n",
    "    \"l2o_nmax\": 0,\n",
    "    \"l2o_steps\": 250,\n",
    "    \"l2o_trans_sigma\": 0.2,\n",
    "    \"l2o_rot_sigma\": 10.0,\n",
    "    \"l2o_deterministic\": True,\n",
    "    \"refine_nmin\": 0,\n",
    "    \"refine_batch\": 16,\n",
    "    \"refine_steps\": 0,\n",
    "    \"refine_trans_sigma\": 0.2,\n",
    "    \"refine_rot_sigma\": 15.0,\n",
    "    \"refine_rot_prob\": 0.3,\n",
    "    \"refine_rot_prob_end\": -1.0,\n",
    "    \"refine_swap_prob\": 0.0,\n",
    "    \"refine_swap_prob_end\": -1.0,\n",
    "    \"refine_cooling\": \"geom\",\n",
    "    \"refine_cooling_power\": 1.0,\n",
    "    \"refine_trans_nexp\": 0.0,\n",
    "    \"refine_rot_nexp\": 0.0,\n",
    "    \"refine_sigma_nref\": 50.0,\n",
    "    \"refine_objective\": \"packing\",\n",
    "    \"refine_proposal\": \"random\",\n",
    "    \"refine_smart_prob\": 1.0,\n",
    "    \"refine_smart_beta\": 8.0,\n",
    "    \"refine_smart_drift\": 1.0,\n",
    "    \"refine_smart_noise\": 0.25,\n",
    "    \"refine_overlap_lambda\": 0.0,\n",
    "    \"refine_allow_collisions\": False,\n",
    "    \"lns_nmax\": 0,\n",
    "    \"lns_passes\": 0,\n",
    "    \"lns_destroy_k\": 8,\n",
    "    \"lns_destroy_mode\": \"mixed\",\n",
    "    \"lns_tabu_tenure\": 0,\n",
    "    \"lns_candidates\": 64,\n",
    "    \"lns_angle_samples\": 8,\n",
    "    \"lns_pad_scale\": 2.0,\n",
    "    \"lns_group_moves\": 0,\n",
    "    \"lns_group_size\": 3,\n",
    "    \"lns_group_trans_sigma\": 0.05,\n",
    "    \"lns_group_rot_sigma\": 20.0,\n",
    "    \"lns_t_start\": 0.0,\n",
    "    \"lns_t_end\": 0.0,\n",
    "    \"guided_model\": None,\n",
    "    \"guided_prob\": 1.0,\n",
    "    \"guided_pmax\": 0.05,\n",
    "    \"guided_prob_end\": -1.0,\n",
    "    \"guided_pmax_end\": -1.0,\n",
    "    \"block_nmax\": 0,\n",
    "    \"block_size\": 2,\n",
    "    \"block_batch\": 32,\n",
    "    \"block_steps\": 0,\n",
    "    \"block_trans_sigma\": 0.2,\n",
    "    \"block_rot_sigma\": 15.0,\n",
    "    \"block_rot_prob\": 0.25,\n",
    "    \"block_rot_prob_end\": -1.0,\n",
    "    \"block_cooling\": \"geom\",\n",
    "    \"block_cooling_power\": 1.0,\n",
    "    \"block_trans_nexp\": 0.0,\n",
    "    \"block_rot_nexp\": 0.0,\n",
    "    \"block_sigma_nref\": 50.0,\n",
    "    \"block_overlap_lambda\": 0.0,\n",
    "    \"block_allow_collisions\": False,\n",
    "    \"block_objective\": \"packing\",\n",
    "    \"block_init\": \"cluster\",\n",
    "    \"block_template_pattern\": \"hex\",\n",
    "    \"block_template_margin\": 0.02,\n",
    "    \"block_template_rotate\": 0.0,\n",
    "    \"hc_nmax\": 0,\n",
    "    \"hc_passes\": 0,\n",
    "    \"hc_step_xy\": 0.01,\n",
    "    \"hc_step_deg\": 2.0,\n",
    "    \"ga_nmax\": 0,\n",
    "    \"ga_pop\": 24,\n",
    "    \"ga_gens\": 0,\n",
    "    \"ga_elite_frac\": 0.25,\n",
    "    \"ga_crossover_prob\": 0.5,\n",
    "    \"ga_mut_sigma_xy\": 0.01,\n",
    "    \"ga_mut_sigma_deg\": 2.0,\n",
    "    \"ga_directed_prob\": 0.5,\n",
    "    \"ga_directed_step_xy\": 0.02,\n",
    "    \"ga_directed_k\": 8,\n",
    "    \"ga_repair_iters\": 200,\n",
    "    \"ga_hc_passes\": 0,\n",
    "    \"ga_hc_step_xy\": 0.01,\n",
    "    \"ga_hc_step_deg\": 2.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddebb115",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Baselines (lattice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffa0d1",
   "metadata": {
    "title": "(lattice_hex_rots0_15_30)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"sa_nmax\": 0, \"lattice_pattern\": \"hex\", \"lattice_margin\": 0.02, \"lattice_rotations\": \"0,15,30\"}\n",
    "run_submission_experiment(\"lattice_hex_rots0_15_30\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737531a",
   "metadata": {
    "title": "(lattice_hex_rots0_5_10_15_20_25_30)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"sa_nmax\": 0, \"lattice_pattern\": \"hex\", \"lattice_margin\": 0.005, \"lattice_rotations\": \"0,5,10,15,20,25,30\"}\n",
    "run_submission_experiment(\"lattice_hex_rots0_5_10_15_20_25_30\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd8f78",
   "metadata": {
    "title": "(lattice_square_rots0_15_30)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"sa_nmax\": 0, \"lattice_pattern\": \"square\", \"lattice_margin\": 0.02, \"lattice_rotations\": \"0,15,30\"}\n",
    "run_submission_experiment(\"lattice_square_rots0_15_30\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c1886",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "(lattice_hex_const_rot15)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"sa_nmax\": 0, \"lattice_pattern\": \"hex\", \"lattice_margin\": 0.02, \"lattice_rotations\": \"none\", \"lattice_rotate\": 15.0}\n",
    "run_submission_experiment(\"lattice_hex_const_rot15\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be97b82d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## SA (Simulated Annealing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac376ef8",
   "metadata": {
    "title": "(sa_packing_random_geom)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"sa_nmax\": 50,\n",
    "    \"sa_steps\": 800,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_objective\": \"packing\",\n",
    "    \"sa_proposal\": \"random\",\n",
    "    \"sa_cooling\": \"geom\",\n",
    "}\n",
    "run_submission_experiment(\"sa_packing_random_geom\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d88d69",
   "metadata": {
    "title": "(sa_packing_mixed_log)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"sa_nmax\": 50,\n",
    "    \"sa_steps\": 800,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_objective\": \"packing\",\n",
    "    \"sa_proposal\": \"mixed\",\n",
    "    \"sa_cooling\": \"log\",\n",
    "    \"sa_smart_prob\": 0.7,\n",
    "}\n",
    "run_submission_experiment(\"sa_packing_mixed_log\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67709869",
   "metadata": {
    "title": "(sa_packing_bbox_inward_geom)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"sa_nmax\": 50,\n",
    "    \"sa_steps\": 800,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_objective\": \"packing\",\n",
    "    \"sa_proposal\": \"bbox_inward\",\n",
    "    \"sa_cooling\": \"geom\",\n",
    "}\n",
    "run_submission_experiment(\"sa_packing_bbox_inward_geom\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d434754f",
   "metadata": {
    "title": "(sa_prefix_mixed_log_swap)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"sa_nmax\": 50,\n",
    "    \"sa_steps\": 900,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_objective\": \"prefix\",\n",
    "    \"sa_proposal\": \"mixed\",\n",
    "    \"sa_cooling\": \"log\",\n",
    "    \"sa_swap_prob\": 0.05,\n",
    "    \"sa_swap_prob_end\": 0.0,\n",
    "    \"sa_smart_prob\": 0.7,\n",
    "}\n",
    "run_submission_experiment(\"sa_prefix_mixed_log_swap\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c0d43",
   "metadata": {
    "title": "(sa_prefix_smart_log_swap)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"sa_nmax\": 50,\n",
    "    \"sa_steps\": 900,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_objective\": \"prefix\",\n",
    "    \"sa_proposal\": \"smart\",\n",
    "    \"sa_cooling\": \"log\",\n",
    "    \"sa_swap_prob\": 0.05,\n",
    "    \"sa_swap_prob_end\": 0.0,\n",
    "    \"sa_smart_prob\": 1.0,\n",
    "}\n",
    "run_submission_experiment(\"sa_prefix_smart_log_swap\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ed3d0",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "(sa_prefix_mixed_log_overlap001)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"sa_nmax\": 50,\n",
    "    \"sa_steps\": 900,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_objective\": \"prefix\",\n",
    "    \"sa_proposal\": \"mixed\",\n",
    "    \"sa_cooling\": \"log\",\n",
    "    \"sa_swap_prob\": 0.05,\n",
    "    \"sa_swap_prob_end\": 0.0,\n",
    "    \"sa_overlap_lambda\": 0.01,\n",
    "    \"sa_smart_prob\": 0.7,\n",
    "}\n",
    "run_submission_experiment(\"sa_prefix_mixed_log_overlap001\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ebb553",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Refine (SA sobre o lattice/solver base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e174452b",
   "metadata": {
    "title": "(refine_packing_mixed_geom)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"refine_nmin\": 80,\n",
    "    \"refine_steps\": 800,\n",
    "    \"refine_batch\": 24,\n",
    "    \"refine_objective\": \"packing\",\n",
    "    \"refine_proposal\": \"mixed\",\n",
    "    \"refine_cooling\": \"geom\",\n",
    "    \"refine_smart_prob\": 0.7,\n",
    "}\n",
    "run_submission_experiment(\"refine_packing_mixed_geom\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ca7d2",
   "metadata": {
    "title": "(refine_prefix_mixed_log)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"refine_nmin\": 80,\n",
    "    \"refine_steps\": 900,\n",
    "    \"refine_batch\": 24,\n",
    "    \"refine_objective\": \"prefix\",\n",
    "    \"refine_proposal\": \"mixed\",\n",
    "    \"refine_cooling\": \"log\",\n",
    "    \"refine_smart_prob\": 0.7,\n",
    "}\n",
    "run_submission_experiment(\"refine_prefix_mixed_log\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd4b28b",
   "metadata": {
    "title": "(refine_prefix_mixed_log_overlap001)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"refine_nmin\": 80,\n",
    "    \"refine_steps\": 900,\n",
    "    \"refine_batch\": 24,\n",
    "    \"refine_objective\": \"prefix\",\n",
    "    \"refine_proposal\": \"mixed\",\n",
    "    \"refine_cooling\": \"log\",\n",
    "    \"refine_overlap_lambda\": 0.01,\n",
    "    \"refine_smart_prob\": 0.7,\n",
    "}\n",
    "run_submission_experiment(\"refine_prefix_mixed_log_overlap001\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0900c033",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "(refine_prefix_bbox_inward_log)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"refine_nmin\": 80,\n",
    "    \"refine_steps\": 900,\n",
    "    \"refine_batch\": 24,\n",
    "    \"refine_objective\": \"prefix\",\n",
    "    \"refine_proposal\": \"bbox_inward\",\n",
    "    \"refine_cooling\": \"log\",\n",
    "}\n",
    "run_submission_experiment(\"refine_prefix_bbox_inward_log\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a974c93",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Block SA (meta-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5eba87",
   "metadata": {
    "title": "(block_cluster_b2_prefix)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"block_nmax\": 200,\n",
    "    \"block_init\": \"cluster\",\n",
    "    \"block_size\": 2,\n",
    "    \"block_steps\": 350,\n",
    "    \"block_batch\": 32,\n",
    "    \"block_objective\": \"prefix\",\n",
    "}\n",
    "run_submission_experiment(\"block_cluster_b2_prefix\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c567355e",
   "metadata": {
    "title": "(block_cluster_b3_prefix)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"block_nmax\": 200,\n",
    "    \"block_init\": \"cluster\",\n",
    "    \"block_size\": 3,\n",
    "    \"block_steps\": 350,\n",
    "    \"block_batch\": 32,\n",
    "    \"block_objective\": \"prefix\",\n",
    "}\n",
    "run_submission_experiment(\"block_cluster_b3_prefix\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47fba65",
   "metadata": {
    "title": "(block_cluster_b4_prefix)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"block_nmax\": 200,\n",
    "    \"block_init\": \"cluster\",\n",
    "    \"block_size\": 4,\n",
    "    \"block_steps\": 350,\n",
    "    \"block_batch\": 32,\n",
    "    \"block_objective\": \"prefix\",\n",
    "}\n",
    "run_submission_experiment(\"block_cluster_b4_prefix\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff58af5",
   "metadata": {
    "title": "(block_template_hex_b2_prefix)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"block_nmax\": 200,\n",
    "    \"block_init\": \"template\",\n",
    "    \"block_template_pattern\": \"hex\",\n",
    "    \"block_template_margin\": 0.02,\n",
    "    \"block_template_rotate\": 0.0,\n",
    "    \"block_size\": 2,\n",
    "    \"block_steps\": 350,\n",
    "    \"block_batch\": 32,\n",
    "    \"block_objective\": \"prefix\",\n",
    "}\n",
    "run_submission_experiment(\"block_template_hex_b2_prefix\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2fd048",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "(block_template_square_b2_prefix)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"block_nmax\": 200,\n",
    "    \"block_init\": \"template\",\n",
    "    \"block_template_pattern\": \"square\",\n",
    "    \"block_template_margin\": 0.02,\n",
    "    \"block_template_rotate\": 0.0,\n",
    "    \"block_size\": 2,\n",
    "    \"block_steps\": 350,\n",
    "    \"block_batch\": 32,\n",
    "    \"block_objective\": \"prefix\",\n",
    "}\n",
    "run_submission_experiment(\"block_template_square_b2_prefix\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef133ea",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## LNS / ALNS (post-opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc93d3d",
   "metadata": {
    "title": "(lns_mixed)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"lns_nmax\": 200, \"lns_passes\": 10, \"lns_destroy_mode\": \"mixed\", \"lns_destroy_k\": 8, \"lns_group_moves\": 4}\n",
    "run_submission_experiment(\"lns_mixed\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733b52d3",
   "metadata": {
    "title": "(lns_mixed_tabu5)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"lns_nmax\": 200, \"lns_passes\": 10, \"lns_destroy_mode\": \"mixed\", \"lns_tabu_tenure\": 5, \"lns_destroy_k\": 8, \"lns_group_moves\": 4}\n",
    "run_submission_experiment(\"lns_mixed_tabu5\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f5834",
   "metadata": {
    "title": "(lns_boundary_tabu5)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"lns_nmax\": 200, \"lns_passes\": 10, \"lns_destroy_mode\": \"boundary\", \"lns_tabu_tenure\": 5, \"lns_destroy_k\": 10, \"lns_group_moves\": 4}\n",
    "run_submission_experiment(\"lns_boundary_tabu5\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00533c03",
   "metadata": {
    "title": "(lns_cluster_tabu5)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"lns_nmax\": 200, \"lns_passes\": 10, \"lns_destroy_mode\": \"cluster\", \"lns_tabu_tenure\": 5, \"lns_destroy_k\": 10, \"lns_group_moves\": 4}\n",
    "run_submission_experiment(\"lns_cluster_tabu5\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aec66c",
   "metadata": {
    "title": "(lns_random)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"lns_nmax\": 200, \"lns_passes\": 10, \"lns_destroy_mode\": \"random\", \"lns_destroy_k\": 8, \"lns_group_moves\": 4}\n",
    "run_submission_experiment(\"lns_random\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701e151",
   "metadata": {
    "title": "(lns_alns)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"lns_nmax\": 200, \"lns_passes\": 10, \"lns_destroy_mode\": \"alns\", \"lns_destroy_k\": 8, \"lns_group_moves\": 4}\n",
    "run_submission_experiment(\"lns_alns\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec6945",
   "metadata": {
    "title": "(lns_alns_tabu5)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"lns_nmax\": 200, \"lns_passes\": 10, \"lns_destroy_mode\": \"alns\", \"lns_tabu_tenure\": 5, \"lns_destroy_k\": 8, \"lns_group_moves\": 4}\n",
    "run_submission_experiment(\"lns_alns_tabu5\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76cd4d",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "(lns_alns_sa_accept_tabu10)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"lns_nmax\": 200,\n",
    "    \"lns_passes\": 20,\n",
    "    \"lns_destroy_mode\": \"alns\",\n",
    "    \"lns_tabu_tenure\": 10,\n",
    "    \"lns_destroy_k\": 10,\n",
    "    \"lns_candidates\": 96,\n",
    "    \"lns_angle_samples\": 12,\n",
    "    \"lns_group_moves\": 6,\n",
    "    \"lns_group_size\": 4,\n",
    "    \"lns_t_start\": 0.2,\n",
    "    \"lns_t_end\": 0.02,\n",
    "}\n",
    "run_submission_experiment(\"lns_alns_sa_accept_tabu10\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3a611a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## GA / Hill-climb (n pequeno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba12dfe",
   "metadata": {
    "title": "(hc20_lattice)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"hc_nmax\": 20, \"hc_passes\": 2, \"hc_step_xy\": 0.01, \"hc_step_deg\": 2.0}\n",
    "run_submission_experiment(\"hc20_lattice\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8348c00",
   "metadata": {
    "title": "(ga20_lattice)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"ga_nmax\": 20, \"ga_gens\": 20, \"ga_pop\": 24, \"ga_mut_sigma_xy\": 0.01, \"ga_mut_sigma_deg\": 2.0}\n",
    "run_submission_experiment(\"ga20_lattice\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f784a",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "(sa20_ga20)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"sa_nmax\": 20,\n",
    "    \"sa_steps\": 600,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_objective\": \"packing\",\n",
    "    \"sa_proposal\": \"mixed\",\n",
    "    \"sa_cooling\": \"geom\",\n",
    "    \"ga_nmax\": 20,\n",
    "    \"ga_gens\": 20,\n",
    "    \"ga_pop\": 24,\n",
    "}\n",
    "run_submission_experiment(\"sa20_ga20\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f59ca6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Pipelines combinados (SA/block + refine + (A)LNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c59d6",
   "metadata": {
    "title": "(sa50_ref80_prefix)"
   },
   "outputs": [],
   "source": [
    "args = {**BASE_ARGS, \"sa_nmax\": 50, \"sa_steps\": 800, \"sa_batch\": 64, \"sa_objective\": \"prefix\", \"sa_proposal\": \"mixed\", \"sa_cooling\": \"log\", \"sa_swap_prob\": 0.05, \"sa_swap_prob_end\": 0.0, \"refine_nmin\": 80, \"refine_steps\": 900, \"refine_batch\": 24, \"refine_objective\": \"prefix\", \"refine_proposal\": \"mixed\", \"refine_cooling\": \"log\"}\n",
    "run_submission_experiment(\"sa50_ref80_prefix\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5da29",
   "metadata": {
    "title": "(sa50_ref80_prefix_lns_alns_tabu5)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"sa_nmax\": 50,\n",
    "    \"sa_steps\": 800,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_objective\": \"prefix\",\n",
    "    \"sa_proposal\": \"mixed\",\n",
    "    \"sa_cooling\": \"log\",\n",
    "    \"sa_swap_prob\": 0.05,\n",
    "    \"sa_swap_prob_end\": 0.0,\n",
    "    \"refine_nmin\": 80,\n",
    "    \"refine_steps\": 900,\n",
    "    \"refine_batch\": 24,\n",
    "    \"refine_objective\": \"prefix\",\n",
    "    \"refine_proposal\": \"mixed\",\n",
    "    \"refine_cooling\": \"log\",\n",
    "    \"lns_nmax\": 200,\n",
    "    \"lns_passes\": 10,\n",
    "    \"lns_destroy_mode\": \"alns\",\n",
    "    \"lns_tabu_tenure\": 5,\n",
    "    \"lns_group_moves\": 4,\n",
    "}\n",
    "run_submission_experiment(\"sa50_ref80_prefix_lns_alns_tabu5\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07fdb9b",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "(block_b2_ref200_prefix_lns_alns_tabu5)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"block_nmax\": 200,\n",
    "    \"block_init\": \"cluster\",\n",
    "    \"block_size\": 2,\n",
    "    \"block_steps\": 350,\n",
    "    \"block_batch\": 32,\n",
    "    \"block_objective\": \"prefix\",\n",
    "    \"lns_nmax\": 200,\n",
    "    \"lns_passes\": 10,\n",
    "    \"lns_destroy_mode\": \"alns\",\n",
    "    \"lns_tabu_tenure\": 5,\n",
    "    \"lns_group_moves\": 4,\n",
    "    \"refine_nmin\": 200,\n",
    "    \"refine_steps\": 1200,\n",
    "    \"refine_batch\": 64,\n",
    "    \"refine_objective\": \"prefix\",\n",
    "    \"refine_proposal\": \"mixed\",\n",
    "    \"refine_cooling\": \"log\",\n",
    "}\n",
    "run_submission_experiment(\"block_b2_ref200_prefix_lns_alns_tabu5\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77329041",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Model-based (meta-init / heatmap / L2O / guided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48575289",
   "metadata": {
    "title": "(meta_init_sa_prefix)  # requer meta_init_path existir"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"meta_init_model\": str(meta_init_path) if (meta_init_path is not None and meta_init_path.exists()) else None,\n",
    "    \"sa_nmax\": 50,\n",
    "    \"sa_steps\": 900,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_objective\": \"prefix\",\n",
    "    \"sa_proposal\": \"mixed\",\n",
    "    \"sa_cooling\": \"log\",\n",
    "    \"sa_swap_prob\": 0.05,\n",
    "    \"sa_swap_prob_end\": 0.0,\n",
    "}\n",
    "run_submission_experiment(\"meta_init_sa_prefix\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3eaa14",
   "metadata": {
    "title": "(heatmap20_only)  # requer heatmap_path existir"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"heatmap_model\": str(heatmap_path) if (heatmap_path is not None and heatmap_path.exists()) else None,\n",
    "    \"heatmap_nmax\": 20,\n",
    "    \"heatmap_steps\": 250,\n",
    "}\n",
    "run_submission_experiment(\"heatmap20_only\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282ca55",
   "metadata": {
    "title": "(heatmap20_sa50_ref80_prefix)  # requer heatmap_path existir"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"heatmap_model\": str(heatmap_path) if (heatmap_path is not None and heatmap_path.exists()) else None,\n",
    "    \"heatmap_nmax\": 20,\n",
    "    \"heatmap_steps\": 250,\n",
    "    \"sa_nmax\": 50,\n",
    "    \"sa_steps\": 800,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_objective\": \"prefix\",\n",
    "    \"sa_proposal\": \"mixed\",\n",
    "    \"sa_cooling\": \"log\",\n",
    "    \"sa_swap_prob\": 0.05,\n",
    "    \"sa_swap_prob_end\": 0.0,\n",
    "    \"refine_nmin\": 80,\n",
    "    \"refine_steps\": 900,\n",
    "    \"refine_batch\": 24,\n",
    "    \"refine_objective\": \"prefix\",\n",
    "    \"refine_proposal\": \"mixed\",\n",
    "    \"refine_cooling\": \"log\",\n",
    "}\n",
    "run_submission_experiment(\"heatmap20_sa50_ref80_prefix\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff1ac7",
   "metadata": {
    "title": "(l2o20_gnn)  # requer policy treinada existir (GNN_POLICY_PATH)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"l2o_model\": str(GNN_POLICY_PATH) if GNN_POLICY_PATH.exists() else None,\n",
    "    \"l2o_init\": \"lattice\",\n",
    "    \"l2o_nmax\": 20,\n",
    "    \"l2o_steps\": 300,\n",
    "    \"l2o_deterministic\": True,\n",
    "}\n",
    "run_submission_experiment(\"l2o20_gnn\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc467f",
   "metadata": {
    "title": "(guided_refine_gnn_lns_alns_tabu5)  # requer policy existir"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"guided_model\": str(GNN_POLICY_PATH) if GNN_POLICY_PATH.exists() else None,\n",
    "    \"guided_prob\": 1.0,\n",
    "    \"guided_pmax\": 0.05,\n",
    "    \"sa_nmax\": 50,\n",
    "    \"sa_steps\": 800,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_objective\": \"prefix\",\n",
    "    \"sa_proposal\": \"mixed\",\n",
    "    \"sa_cooling\": \"log\",\n",
    "    \"sa_swap_prob\": 0.05,\n",
    "    \"sa_swap_prob_end\": 0.0,\n",
    "    \"refine_nmin\": 80,\n",
    "    \"refine_steps\": 900,\n",
    "    \"refine_batch\": 24,\n",
    "    \"refine_objective\": \"prefix\",\n",
    "    \"refine_proposal\": \"mixed\",\n",
    "    \"refine_cooling\": \"log\",\n",
    "    \"lns_nmax\": 200,\n",
    "    \"lns_passes\": 10,\n",
    "    \"lns_destroy_mode\": \"alns\",\n",
    "    \"lns_tabu_tenure\": 5,\n",
    "    \"lns_group_moves\": 4,\n",
    "}\n",
    "run_submission_experiment(\"guided_refine_gnn_lns_alns_tabu5\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa72c5",
   "metadata": {
    "title": "(l2o20_mlp)  # requer policy treinada existir (MLP_POLICY_PATH)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"l2o_model\": str(MLP_POLICY_PATH) if MLP_POLICY_PATH.exists() else None,\n",
    "    \"l2o_init\": \"lattice\",\n",
    "    \"l2o_nmax\": 20,\n",
    "    \"l2o_steps\": 300,\n",
    "    \"l2o_deterministic\": True,\n",
    "}\n",
    "run_submission_experiment(\"l2o20_mlp\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e785a",
   "metadata": {
    "title": "(guided_refine_mlp_lns_alns_tabu5)  # requer policy existir"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"guided_model\": str(MLP_POLICY_PATH) if MLP_POLICY_PATH.exists() else None,\n",
    "    \"guided_prob\": 1.0,\n",
    "    \"guided_pmax\": 0.05,\n",
    "    \"sa_nmax\": 50,\n",
    "    \"sa_steps\": 800,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_objective\": \"prefix\",\n",
    "    \"sa_proposal\": \"mixed\",\n",
    "    \"sa_cooling\": \"log\",\n",
    "    \"sa_swap_prob\": 0.05,\n",
    "    \"sa_swap_prob_end\": 0.0,\n",
    "    \"refine_nmin\": 80,\n",
    "    \"refine_steps\": 900,\n",
    "    \"refine_batch\": 24,\n",
    "    \"refine_objective\": \"prefix\",\n",
    "    \"refine_proposal\": \"mixed\",\n",
    "    \"refine_cooling\": \"log\",\n",
    "    \"lns_nmax\": 200,\n",
    "    \"lns_passes\": 10,\n",
    "    \"lns_destroy_mode\": \"alns\",\n",
    "    \"lns_tabu_tenure\": 5,\n",
    "    \"lns_group_moves\": 4,\n",
    "}\n",
    "run_submission_experiment(\"guided_refine_mlp_lns_alns_tabu5\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7134f1",
   "metadata": {
    "title": "(l2o20_bc)  # requer bc_policy_path existir"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"l2o_model\": str(bc_policy_path) if (bc_policy_path is not None and bc_policy_path.exists()) else None,\n",
    "    \"l2o_init\": \"lattice\",\n",
    "    \"l2o_nmax\": 20,\n",
    "    \"l2o_steps\": 300,\n",
    "    \"l2o_deterministic\": True,\n",
    "}\n",
    "run_submission_experiment(\"l2o20_bc\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e488cc",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "(guided_refine_bc_lns_alns_tabu5)  # requer bc_policy_path existir"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"guided_model\": str(bc_policy_path) if (bc_policy_path is not None and bc_policy_path.exists()) else None,\n",
    "    \"guided_prob\": 1.0,\n",
    "    \"guided_pmax\": 0.05,\n",
    "    \"sa_nmax\": 50,\n",
    "    \"sa_steps\": 800,\n",
    "    \"sa_batch\": 64,\n",
    "    \"sa_objective\": \"prefix\",\n",
    "    \"sa_proposal\": \"mixed\",\n",
    "    \"sa_cooling\": \"log\",\n",
    "    \"sa_swap_prob\": 0.05,\n",
    "    \"sa_swap_prob_end\": 0.0,\n",
    "    \"refine_nmin\": 80,\n",
    "    \"refine_steps\": 900,\n",
    "    \"refine_batch\": 24,\n",
    "    \"refine_objective\": \"prefix\",\n",
    "    \"refine_proposal\": \"mixed\",\n",
    "    \"refine_cooling\": \"log\",\n",
    "    \"lns_nmax\": 200,\n",
    "    \"lns_passes\": 10,\n",
    "    \"lns_destroy_mode\": \"alns\",\n",
    "    \"lns_tabu_tenure\": 5,\n",
    "    \"lns_group_moves\": 4,\n",
    "}\n",
    "run_submission_experiment(\"guided_refine_bc_lns_alns_tabu5\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b96bc1e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Mother-prefix (resolve N=200 uma vez e emite prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61b8db",
   "metadata": {
    "title": "(mother_refine2000_prefix)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"mother_prefix\": True,\n",
    "    \"mother_reorder\": \"radial\",\n",
    "    \"sa_nmax\": 0,\n",
    "    \"refine_nmin\": 200,\n",
    "    \"refine_steps\": 2000,\n",
    "    \"refine_batch\": 64,\n",
    "    \"refine_objective\": \"prefix\",\n",
    "    \"refine_proposal\": \"mixed\",\n",
    "    \"refine_cooling\": \"log\",\n",
    "    \"refine_trans_sigma\": 0.35,\n",
    "    \"refine_rot_sigma\": 25.0,\n",
    "    \"refine_rot_prob\": 0.4,\n",
    "    \"refine_rot_prob_end\": 0.1,\n",
    "}\n",
    "run_submission_experiment(\"mother_refine2000_prefix\", args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e5bd08",
   "metadata": {
    "title": "(mother_block_alns_tabu_refine2000_prefix)"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    **BASE_ARGS,\n",
    "    \"mother_prefix\": True,\n",
    "    \"mother_reorder\": \"radial\",\n",
    "    \"block_nmax\": 200,\n",
    "    \"block_init\": \"cluster\",\n",
    "    \"block_size\": 2,\n",
    "    \"block_steps\": 500,\n",
    "    \"block_batch\": 32,\n",
    "    \"block_objective\": \"prefix\",\n",
    "    \"lns_nmax\": 200,\n",
    "    \"lns_passes\": 10,\n",
    "    \"lns_destroy_mode\": \"alns\",\n",
    "    \"lns_tabu_tenure\": 5,\n",
    "    \"lns_group_moves\": 4,\n",
    "    \"refine_nmin\": 200,\n",
    "    \"refine_steps\": 2000,\n",
    "    \"refine_batch\": 64,\n",
    "    \"refine_objective\": \"prefix\",\n",
    "    \"refine_proposal\": \"mixed\",\n",
    "    \"refine_cooling\": \"log\",\n",
    "    \"refine_trans_sigma\": 0.35,\n",
    "    \"refine_rot_sigma\": 25.0,\n",
    "    \"refine_rot_prob\": 0.4,\n",
    "    \"refine_rot_prob_end\": 0.1,\n",
    "}\n",
    "run_submission_experiment(\"mother_block_alns_tabu_refine2000_prefix\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd6abf1",
   "metadata": {},
   "source": [
    "## Sweep automtico de receitas (opcional)\n",
    "\n",
    "Gera uma pool grande de receitas (combinaes de flags do `python -m santa_packing.cli.generate_submission`),\n",
    "filtra as que dependem de modelos inexistentes e permite:\n",
    "- `RUN_SUBMISSION_SWEEP=True`: sweep 2-stage (rpido + final) + (opcional) ensemble por puzzle\n",
    "- `RUN_SUBMISSION_SWEEP=False`: gera 1 `submission.csv` com uma receita forte e calcula o score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf09741",
   "metadata": {
    "title": "Receitas automticas (pool + filtro)"
   },
   "outputs": [],
   "source": [
    "def _stable_hash_dict(data: Dict[str, object]) -> str:\n",
    "    blob = json.dumps(data, sort_keys=True, default=str).encode(\"utf-8\")\n",
    "    return hashlib.sha1(blob).hexdigest()[:10]\n",
    "\n",
    "\n",
    "def _build_recipe_pool() -> tuple[Dict[str, Dict[str, object]], Dict[str, Dict[str, object]], Dict[str, object]]:\n",
    "    \"\"\"Gera uma pool de receitas cobrindo todas as features do generate_submission.py.\n",
    "\n",
    "    Mantem as receitas como um dict de flags (somente chaves suportadas pelo script).\n",
    "    Metadados (familia/modelo) ficam em um dict separado para auditoria.\n",
    "    \"\"\"\n",
    "\n",
    "    # Base: desliga tudo e deixa lattice como fallback garantido.\n",
    "    base: Dict[str, object] = {\n",
    "        \"sa_nmax\": 0,\n",
    "        \"sa_batch\": 64,\n",
    "        \"sa_steps\": 400,\n",
    "        \"sa_trans_sigma\": 0.2,\n",
    "        \"sa_rot_sigma\": 15.0,\n",
    "        \"sa_rot_prob\": 0.3,\n",
    "        \"sa_rot_prob_end\": -1.0,\n",
    "        \"sa_swap_prob\": 0.0,\n",
    "        \"sa_swap_prob_end\": -1.0,\n",
    "        \"sa_cooling\": \"geom\",\n",
    "        \"sa_cooling_power\": 1.0,\n",
    "        \"sa_trans_nexp\": 0.0,\n",
    "        \"sa_rot_nexp\": 0.0,\n",
    "        \"sa_sigma_nref\": 50.0,\n",
    "        \"sa_objective\": \"packing\",\n",
    "        \"sa_proposal\": \"random\",\n",
    "        \"sa_smart_prob\": 1.0,\n",
    "        \"sa_smart_beta\": 8.0,\n",
    "        \"sa_smart_drift\": 1.0,\n",
    "        \"sa_smart_noise\": 0.25,\n",
    "        \"sa_overlap_lambda\": 0.0,\n",
    "        \"sa_allow_collisions\": False,\n",
    "        \"meta_init_model\": None,\n",
    "        \"heatmap_model\": None,\n",
    "        \"heatmap_nmax\": 0,\n",
    "        \"heatmap_steps\": 200,\n",
    "        \"l2o_model\": None,\n",
    "        \"l2o_init\": \"grid\",\n",
    "        \"l2o_nmax\": 0,\n",
    "        \"l2o_steps\": 200,\n",
    "        \"l2o_trans_sigma\": 0.2,\n",
    "        \"l2o_rot_sigma\": 10.0,\n",
    "        \"l2o_deterministic\": True,\n",
    "        \"lattice_pattern\": \"hex\",\n",
    "        \"lattice_margin\": 0.02,\n",
    "        \"lattice_rotate\": 0.0,\n",
    "        \"lattice_rotations\": \"0,15,30\",\n",
    "        \"mother_prefix\": False,\n",
    "        \"mother_reorder\": \"radial\",\n",
    "        \"refine_nmin\": 0,\n",
    "        \"refine_batch\": 16,\n",
    "        \"refine_steps\": 0,\n",
    "        \"refine_trans_sigma\": 0.2,\n",
    "        \"refine_rot_sigma\": 15.0,\n",
    "        \"refine_rot_prob\": 0.3,\n",
    "        \"refine_rot_prob_end\": -1.0,\n",
    "        \"refine_swap_prob\": 0.0,\n",
    "        \"refine_swap_prob_end\": -1.0,\n",
    "        \"refine_cooling\": \"geom\",\n",
    "        \"refine_cooling_power\": 1.0,\n",
    "        \"refine_trans_nexp\": 0.0,\n",
    "        \"refine_rot_nexp\": 0.0,\n",
    "        \"refine_sigma_nref\": 50.0,\n",
    "        \"refine_objective\": \"packing\",\n",
    "        \"refine_proposal\": \"random\",\n",
    "        \"refine_smart_prob\": 1.0,\n",
    "        \"refine_smart_beta\": 8.0,\n",
    "        \"refine_smart_drift\": 1.0,\n",
    "        \"refine_smart_noise\": 0.25,\n",
    "        \"refine_overlap_lambda\": 0.0,\n",
    "        \"refine_allow_collisions\": False,\n",
    "        \"lns_nmax\": 0,\n",
    "        \"lns_passes\": 0,\n",
    "        \"lns_destroy_k\": 8,\n",
    "        \"lns_destroy_mode\": \"mixed\",\n",
    "        \"lns_tabu_tenure\": 0,\n",
    "        \"lns_candidates\": 64,\n",
    "        \"lns_angle_samples\": 8,\n",
    "        \"lns_pad_scale\": 2.0,\n",
    "        \"lns_group_moves\": 0,\n",
    "        \"lns_group_size\": 3,\n",
    "        \"lns_group_trans_sigma\": 0.05,\n",
    "        \"lns_group_rot_sigma\": 20.0,\n",
    "        \"lns_t_start\": 0.0,\n",
    "        \"lns_t_end\": 0.0,\n",
    "        \"guided_model\": None,\n",
    "        \"guided_prob\": 1.0,\n",
    "        \"guided_pmax\": 0.05,\n",
    "        \"guided_prob_end\": -1.0,\n",
    "        \"guided_pmax_end\": -1.0,\n",
    "        \"block_nmax\": 0,\n",
    "        \"block_size\": 2,\n",
    "        \"block_batch\": 32,\n",
    "        \"block_steps\": 0,\n",
    "        \"block_trans_sigma\": 0.2,\n",
    "        \"block_rot_sigma\": 15.0,\n",
    "        \"block_rot_prob\": 0.25,\n",
    "        \"block_rot_prob_end\": -1.0,\n",
    "        \"block_cooling\": \"geom\",\n",
    "        \"block_cooling_power\": 1.0,\n",
    "        \"block_trans_nexp\": 0.0,\n",
    "        \"block_rot_nexp\": 0.0,\n",
    "        \"block_sigma_nref\": 50.0,\n",
    "        \"block_overlap_lambda\": 0.0,\n",
    "        \"block_allow_collisions\": False,\n",
    "        \"block_objective\": \"packing\",\n",
    "        \"block_init\": \"cluster\",\n",
    "        \"block_template_pattern\": \"hex\",\n",
    "        \"block_template_margin\": 0.02,\n",
    "        \"block_template_rotate\": 0.0,\n",
    "        \"hc_nmax\": 0,\n",
    "        \"hc_passes\": 2,\n",
    "        \"hc_step_xy\": 0.01,\n",
    "        \"hc_step_deg\": 2.0,\n",
    "        \"ga_nmax\": 0,\n",
    "        \"ga_pop\": 24,\n",
    "        \"ga_gens\": 20,\n",
    "        \"ga_elite_frac\": 0.25,\n",
    "        \"ga_crossover_prob\": 0.5,\n",
    "        \"ga_mut_sigma_xy\": 0.01,\n",
    "        \"ga_mut_sigma_deg\": 2.0,\n",
    "        \"ga_directed_prob\": 0.5,\n",
    "        \"ga_directed_step_xy\": 0.02,\n",
    "        \"ga_directed_k\": 8,\n",
    "        \"ga_repair_iters\": 200,\n",
    "        \"ga_hc_passes\": 0,\n",
    "        \"ga_hc_step_xy\": 0.01,\n",
    "        \"ga_hc_step_deg\": 2.0,\n",
    "    }\n",
    "\n",
    "    # ===== Experiment grids (ajuste aqui) =====\n",
    "    # Lattice sweep (base/fallback de tudo).\n",
    "    lattice_variants_all: List[Dict[str, object]] = []\n",
    "    lattice_margins = [0.0, 0.005, 0.01, 0.015, 0.02]\n",
    "    lattice_rot_sets = [\n",
    "        \"0,15,30\",\n",
    "        \"0,10,20,30\",\n",
    "        \"0,5,10,15,20,25,30\",\n",
    "        \"0,9,18,27,36\",\n",
    "    ]\n",
    "    lattice_const_rots = [0.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 35.0, 40.0, 45.0]\n",
    "\n",
    "    # Multi-rotation: picks best lattice per n.\n",
    "    for pattern, margin, rots in itertools.product([\"hex\", \"square\"], lattice_margins, lattice_rot_sets):\n",
    "        lattice_variants_all.append(\n",
    "            {\n",
    "                \"lattice_pattern\": pattern,\n",
    "                \"lattice_margin\": float(margin),\n",
    "                \"lattice_rotations\": str(rots),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Constant rotation: disable rotate-set and use `lattice_rotate`.\n",
    "    for pattern, margin, rot in itertools.product([\"hex\", \"square\"], lattice_margins, lattice_const_rots):\n",
    "        lattice_variants_all.append(\n",
    "            {\n",
    "                \"lattice_pattern\": pattern,\n",
    "                \"lattice_margin\": float(margin),\n",
    "                \"lattice_rotations\": \"none\",\n",
    "                \"lattice_rotate\": float(rot),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # SA (n pequeno): custo principal do solver -> vale experimentar.\n",
    "    sa_level_presets: Dict[str, Dict[str, object]] = {\n",
    "        \"sa30\": {\"sa_nmax\": 30, \"sa_batch\": 64, \"sa_steps\": 400, \"sa_trans_sigma\": 0.2, \"sa_rot_sigma\": 15.0, \"sa_rot_prob\": 0.3},\n",
    "        \"sa50\": {\"sa_nmax\": 50, \"sa_batch\": 64, \"sa_steps\": 500, \"sa_trans_sigma\": 0.2, \"sa_rot_sigma\": 15.0, \"sa_rot_prob\": 0.3},\n",
    "        \"sa80\": {\"sa_nmax\": 80, \"sa_batch\": 96, \"sa_steps\": 600, \"sa_trans_sigma\": 0.2, \"sa_rot_sigma\": 18.0, \"sa_rot_prob\": 0.35},\n",
    "    }\n",
    "    sa_objectives = [\"packing\", \"prefix\"]\n",
    "    sa_proposals = [\"random\", \"mixed\", \"bbox_inward\", \"bbox\", \"inward\", \"smart\"]\n",
    "    sa_coolings = [\"geom\", \"linear\", \"log\"]\n",
    "    sa_overlap_lambdas = [0.0, 0.01]\n",
    "    sa_presets: Dict[str, Dict[str, object]] = {}\n",
    "    for lvl_name, lvl_cfg in sa_level_presets.items():\n",
    "        for objective, proposal, cooling, overlap_lambda in itertools.product(\n",
    "            sa_objectives,\n",
    "            sa_proposals,\n",
    "            sa_coolings,\n",
    "            sa_overlap_lambdas,\n",
    "        ):\n",
    "            cfg: Dict[str, object] = dict(lvl_cfg)\n",
    "            cfg.update(\n",
    "                {\n",
    "                    \"sa_objective\": str(objective),\n",
    "                    \"sa_proposal\": str(proposal),\n",
    "                    \"sa_cooling\": str(cooling),\n",
    "                    \"sa_overlap_lambda\": float(overlap_lambda),\n",
    "                    # Swap  til principalmente quando objetivo=prefix (reordena prefixos sem mexer na geometria).\n",
    "                    \"sa_swap_prob\": 0.05 if objective == \"prefix\" else 0.0,\n",
    "                    \"sa_swap_prob_end\": 0.0 if objective == \"prefix\" else -1.0,\n",
    "                }\n",
    "            )\n",
    "            if proposal != \"random\":\n",
    "                cfg.update(\n",
    "                    {\n",
    "                        \"sa_smart_prob\": 0.7,\n",
    "                        \"sa_smart_beta\": 8.0,\n",
    "                        \"sa_smart_drift\": 1.0,\n",
    "                        \"sa_smart_noise\": 0.25,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            key = f\"{lvl_name}_{objective}_{proposal}_{cooling}_ol{overlap_lambda:g}\"\n",
    "            sa_presets[key] = cfg\n",
    "\n",
    "    # Refine (n alto): warm-start lattice/l2o e refina via SA.\n",
    "    refine_level_presets: Dict[str, Dict[str, object]] = {\n",
    "        \"ref80_200\": {\"refine_nmin\": 80, \"refine_batch\": 16, \"refine_steps\": 200, \"refine_trans_sigma\": 0.2, \"refine_rot_sigma\": 15.0, \"refine_rot_prob\": 0.3},\n",
    "        \"ref80_400\": {\"refine_nmin\": 80, \"refine_batch\": 24, \"refine_steps\": 400, \"refine_trans_sigma\": 0.2, \"refine_rot_sigma\": 15.0, \"refine_rot_prob\": 0.3},\n",
    "        \"ref120_300\": {\"refine_nmin\": 120, \"refine_batch\": 24, \"refine_steps\": 300, \"refine_trans_sigma\": 0.2, \"refine_rot_sigma\": 15.0, \"refine_rot_prob\": 0.3},\n",
    "        # heavy: usado principalmente com mother_prefix (n=200)\n",
    "        \"ref200_2000\": {\"refine_nmin\": 200, \"refine_batch\": 64, \"refine_steps\": 2000, \"refine_trans_sigma\": 0.35, \"refine_rot_sigma\": 25.0, \"refine_rot_prob\": 0.4, \"refine_rot_prob_end\": 0.1},\n",
    "    }\n",
    "    refine_objectives = [\"packing\", \"prefix\"]\n",
    "    refine_proposals = [\"random\", \"mixed\", \"bbox_inward\", \"bbox\", \"inward\", \"smart\"]\n",
    "    refine_coolings = [\"geom\", \"linear\", \"log\"]\n",
    "    refine_overlap_lambdas = [0.0, 0.01]\n",
    "    refine_presets: Dict[str, Dict[str, object]] = {}\n",
    "    for lvl_name, lvl_cfg in refine_level_presets.items():\n",
    "        for objective, proposal, cooling, overlap_lambda in itertools.product(\n",
    "            refine_objectives,\n",
    "            refine_proposals,\n",
    "            refine_coolings,\n",
    "            refine_overlap_lambdas,\n",
    "        ):\n",
    "            cfg: Dict[str, object] = dict(lvl_cfg)\n",
    "            cfg.update(\n",
    "                {\n",
    "                    \"refine_objective\": str(objective),\n",
    "                    \"refine_proposal\": str(proposal),\n",
    "                    \"refine_cooling\": str(cooling),\n",
    "                    \"refine_overlap_lambda\": float(overlap_lambda),\n",
    "                    # schedules teis para \"esfriar\" no fim\n",
    "                    \"refine_rot_prob_end\": float(lvl_cfg.get(\"refine_rot_prob_end\", -1.0)),\n",
    "                }\n",
    "            )\n",
    "            if proposal != \"random\":\n",
    "                cfg.update(\n",
    "                    {\n",
    "                        \"refine_smart_prob\": 0.7,\n",
    "                        \"refine_smart_beta\": 8.0,\n",
    "                        \"refine_smart_drift\": 1.0,\n",
    "                        \"refine_smart_noise\": 0.25,\n",
    "                    }\n",
    "                )\n",
    "            key = f\"{lvl_name}_{objective}_{proposal}_{cooling}_ol{overlap_lambda:g}\"\n",
    "            refine_presets[key] = cfg\n",
    "\n",
    "    # Meta-model blocks (2..4): cluster trees into rigid groups and optimize blocks before per-tree refinement.\n",
    "    block_sizes = [2, 3, 4]\n",
    "    block_objectives = [\"packing\", \"prefix\"]\n",
    "    block_steps_list = [200, 350, 500]\n",
    "    block_presets: Dict[str, Dict[str, object]] = {}\n",
    "\n",
    "    # Cluster-init blocks (cheap).\n",
    "    for size, objective, steps in itertools.product(block_sizes, block_objectives, block_steps_list):\n",
    "        cfg: Dict[str, object] = {\n",
    "            \"block_nmax\": 200,\n",
    "            \"block_size\": int(size),\n",
    "            \"block_batch\": 32,\n",
    "            \"block_steps\": int(steps),\n",
    "            \"block_trans_sigma\": 0.2,\n",
    "            \"block_rot_sigma\": 20.0,\n",
    "            \"block_rot_prob\": 0.25,\n",
    "            \"block_objective\": str(objective),\n",
    "            \"block_init\": \"cluster\",\n",
    "        }\n",
    "        key = f\"blk200_cluster_b{size}_{objective}_s{steps}\"\n",
    "        block_presets[key] = cfg\n",
    "\n",
    "    # Template-init blocks (more structured).\n",
    "    template_patterns = [\"hex\", \"square\"]\n",
    "    template_rotates = [0.0, 15.0, 30.0]\n",
    "    template_margins = [0.0, 0.02]\n",
    "    for size, objective, steps, tpat, trot, tmar in itertools.product(\n",
    "        block_sizes,\n",
    "        block_objectives,\n",
    "        block_steps_list,\n",
    "        template_patterns,\n",
    "        template_rotates,\n",
    "        template_margins,\n",
    "    ):\n",
    "        cfg = {\n",
    "            \"block_nmax\": 200,\n",
    "            \"block_size\": int(size),\n",
    "            \"block_batch\": 32,\n",
    "            \"block_steps\": int(steps),\n",
    "            \"block_trans_sigma\": 0.2,\n",
    "            \"block_rot_sigma\": 20.0,\n",
    "            \"block_rot_prob\": 0.25,\n",
    "            \"block_objective\": str(objective),\n",
    "            \"block_init\": \"template\",\n",
    "            \"block_template_pattern\": str(tpat),\n",
    "            \"block_template_margin\": float(tmar),\n",
    "            \"block_template_rotate\": float(trot),\n",
    "        }\n",
    "        key = f\"blk200_template_{tpat}_r{trot:g}_m{tmar:g}_b{size}_{objective}_s{steps}\"\n",
    "        block_presets[key] = cfg\n",
    "\n",
    "    # Large Neighborhood Search / ALNS-style post-opt (n pequeno/medio).\n",
    "    lns_presets: Dict[str, Dict[str, object]] = {\n",
    "        \"lns10\": {\n",
    "            \"lns_nmax\": 200,\n",
    "            \"lns_passes\": 10,\n",
    "            \"lns_destroy_k\": 8,\n",
    "            \"lns_destroy_mode\": \"mixed\",\n",
    "            \"lns_tabu_tenure\": 0,\n",
    "            \"lns_candidates\": 64,\n",
    "            \"lns_angle_samples\": 8,\n",
    "            \"lns_pad_scale\": 2.0,\n",
    "            \"lns_group_moves\": 4,\n",
    "            \"lns_group_size\": 3,\n",
    "            \"lns_group_trans_sigma\": 0.05,\n",
    "            \"lns_group_rot_sigma\": 20.0,\n",
    "            \"lns_t_start\": 0.0,\n",
    "            \"lns_t_end\": 0.0,\n",
    "        },\n",
    "        \"lns10_mixed_t5\": {\n",
    "            \"lns_nmax\": 200,\n",
    "            \"lns_passes\": 10,\n",
    "            \"lns_destroy_k\": 8,\n",
    "            \"lns_destroy_mode\": \"mixed\",\n",
    "            \"lns_tabu_tenure\": 5,\n",
    "            \"lns_candidates\": 64,\n",
    "            \"lns_angle_samples\": 8,\n",
    "            \"lns_pad_scale\": 2.0,\n",
    "            \"lns_group_moves\": 4,\n",
    "            \"lns_group_size\": 3,\n",
    "            \"lns_group_trans_sigma\": 0.05,\n",
    "            \"lns_group_rot_sigma\": 20.0,\n",
    "            \"lns_t_start\": 0.0,\n",
    "            \"lns_t_end\": 0.0,\n",
    "        },\n",
    "        \"lns10_sa\": {\n",
    "            \"lns_nmax\": 200,\n",
    "            \"lns_passes\": 10,\n",
    "            \"lns_destroy_k\": 8,\n",
    "            \"lns_destroy_mode\": \"mixed\",\n",
    "            \"lns_tabu_tenure\": 0,\n",
    "            \"lns_candidates\": 64,\n",
    "            \"lns_angle_samples\": 8,\n",
    "            \"lns_pad_scale\": 2.0,\n",
    "            \"lns_group_moves\": 4,\n",
    "            \"lns_group_size\": 3,\n",
    "            \"lns_group_trans_sigma\": 0.05,\n",
    "            \"lns_group_rot_sigma\": 20.0,\n",
    "            \"lns_t_start\": 0.2,\n",
    "            \"lns_t_end\": 0.02,\n",
    "        },\n",
    "        \"lns10_boundary_t5\": {\n",
    "            \"lns_nmax\": 200,\n",
    "            \"lns_passes\": 10,\n",
    "            \"lns_destroy_k\": 10,\n",
    "            \"lns_destroy_mode\": \"boundary\",\n",
    "            \"lns_tabu_tenure\": 5,\n",
    "            \"lns_candidates\": 96,\n",
    "            \"lns_angle_samples\": 10,\n",
    "            \"lns_pad_scale\": 2.0,\n",
    "            \"lns_group_moves\": 4,\n",
    "            \"lns_group_size\": 3,\n",
    "            \"lns_group_trans_sigma\": 0.05,\n",
    "            \"lns_group_rot_sigma\": 20.0,\n",
    "            \"lns_t_start\": 0.0,\n",
    "            \"lns_t_end\": 0.0,\n",
    "        },\n",
    "        \"lns10_cluster_t5\": {\n",
    "            \"lns_nmax\": 200,\n",
    "            \"lns_passes\": 10,\n",
    "            \"lns_destroy_k\": 10,\n",
    "            \"lns_destroy_mode\": \"cluster\",\n",
    "            \"lns_tabu_tenure\": 5,\n",
    "            \"lns_candidates\": 96,\n",
    "            \"lns_angle_samples\": 10,\n",
    "            \"lns_pad_scale\": 2.0,\n",
    "            \"lns_group_moves\": 4,\n",
    "            \"lns_group_size\": 3,\n",
    "            \"lns_group_trans_sigma\": 0.05,\n",
    "            \"lns_group_rot_sigma\": 20.0,\n",
    "            \"lns_t_start\": 0.0,\n",
    "            \"lns_t_end\": 0.0,\n",
    "        },\n",
    "        \"lns10_random\": {\n",
    "            \"lns_nmax\": 200,\n",
    "            \"lns_passes\": 10,\n",
    "            \"lns_destroy_k\": 8,\n",
    "            \"lns_destroy_mode\": \"random\",\n",
    "            \"lns_tabu_tenure\": 0,\n",
    "            \"lns_candidates\": 64,\n",
    "            \"lns_angle_samples\": 8,\n",
    "            \"lns_pad_scale\": 2.0,\n",
    "            \"lns_group_moves\": 4,\n",
    "            \"lns_group_size\": 3,\n",
    "            \"lns_group_trans_sigma\": 0.05,\n",
    "            \"lns_group_rot_sigma\": 20.0,\n",
    "            \"lns_t_start\": 0.0,\n",
    "            \"lns_t_end\": 0.0,\n",
    "        },\n",
    "        # New: adaptive LNS (ALNS) + tabu\n",
    "        \"lns10_alns\": {\n",
    "            \"lns_nmax\": 200,\n",
    "            \"lns_passes\": 10,\n",
    "            \"lns_destroy_k\": 8,\n",
    "            \"lns_destroy_mode\": \"alns\",\n",
    "            \"lns_tabu_tenure\": 0,\n",
    "            \"lns_candidates\": 64,\n",
    "            \"lns_angle_samples\": 8,\n",
    "            \"lns_pad_scale\": 2.0,\n",
    "            \"lns_group_moves\": 4,\n",
    "            \"lns_group_size\": 3,\n",
    "            \"lns_group_trans_sigma\": 0.05,\n",
    "            \"lns_group_rot_sigma\": 20.0,\n",
    "            \"lns_t_start\": 0.0,\n",
    "            \"lns_t_end\": 0.0,\n",
    "        },\n",
    "        \"lns10_alns_t5\": {\n",
    "            \"lns_nmax\": 200,\n",
    "            \"lns_passes\": 10,\n",
    "            \"lns_destroy_k\": 8,\n",
    "            \"lns_destroy_mode\": \"alns\",\n",
    "            \"lns_tabu_tenure\": 5,\n",
    "            \"lns_candidates\": 64,\n",
    "            \"lns_angle_samples\": 8,\n",
    "            \"lns_pad_scale\": 2.0,\n",
    "            \"lns_group_moves\": 4,\n",
    "            \"lns_group_size\": 3,\n",
    "            \"lns_group_trans_sigma\": 0.05,\n",
    "            \"lns_group_rot_sigma\": 20.0,\n",
    "            \"lns_t_start\": 0.0,\n",
    "            \"lns_t_end\": 0.0,\n",
    "        },\n",
    "        \"lns20_alns_sa_t10\": {\n",
    "            \"lns_nmax\": 200,\n",
    "            \"lns_passes\": 20,\n",
    "            \"lns_destroy_k\": 10,\n",
    "            \"lns_destroy_mode\": \"alns\",\n",
    "            \"lns_tabu_tenure\": 10,\n",
    "            \"lns_candidates\": 96,\n",
    "            \"lns_angle_samples\": 12,\n",
    "            \"lns_pad_scale\": 2.0,\n",
    "            \"lns_group_moves\": 6,\n",
    "            \"lns_group_size\": 4,\n",
    "            \"lns_group_trans_sigma\": 0.05,\n",
    "            \"lns_group_rot_sigma\": 20.0,\n",
    "            \"lns_t_start\": 0.2,\n",
    "            \"lns_t_end\": 0.02,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Guided SA knobs (policy como proposal quando confiante).\n",
    "    guided_presets: Dict[str, Dict[str, object]] = {\n",
    "        \"guided_p005\": {\"guided_prob\": 1.0, \"guided_pmax\": 0.05},\n",
    "        \"guided_p02\": {\"guided_prob\": 1.0, \"guided_pmax\": 0.02},\n",
    "        \"guided_mix\": {\"guided_prob\": 0.5, \"guided_pmax\": 0.05},\n",
    "        \"guided_sched\": {\"guided_prob\": 1.0, \"guided_pmax\": 0.05, \"guided_prob_end\": 0.5, \"guided_pmax_end\": 0.08},\n",
    "        \"guided_sched2\": {\"guided_prob\": 0.8, \"guided_pmax\": 0.03, \"guided_prob_end\": 0.4, \"guided_pmax_end\": 0.06},\n",
    "    }\n",
    "\n",
    "    # L2O knobs (n pequeno): policy pode substituir SA inicial.\n",
    "    l2o_presets: Dict[str, Dict[str, object]] = {\n",
    "        \"l2o10_det\": {\"l2o_init\": \"lattice\", \"l2o_nmax\": 10, \"l2o_steps\": 200, \"l2o_trans_sigma\": 0.2, \"l2o_rot_sigma\": 10.0, \"l2o_deterministic\": True},\n",
    "        \"l2o10_stoch\": {\"l2o_init\": \"lattice\", \"l2o_nmax\": 10, \"l2o_steps\": 250, \"l2o_trans_sigma\": 0.2, \"l2o_rot_sigma\": 10.0, \"l2o_deterministic\": False},\n",
    "        \"l2o20_det\": {\"l2o_init\": \"lattice\", \"l2o_nmax\": 20, \"l2o_steps\": 250, \"l2o_trans_sigma\": 0.2, \"l2o_rot_sigma\": 10.0, \"l2o_deterministic\": True},\n",
    "        \"l2o30_det\": {\"l2o_init\": \"lattice\", \"l2o_nmax\": 30, \"l2o_steps\": 300, \"l2o_trans_sigma\": 0.2, \"l2o_rot_sigma\": 10.0, \"l2o_deterministic\": True},\n",
    "        \"l2o20_grid\": {\"l2o_init\": \"grid\", \"l2o_nmax\": 20, \"l2o_steps\": 250, \"l2o_trans_sigma\": 0.2, \"l2o_rot_sigma\": 10.0, \"l2o_deterministic\": True},\n",
    "    }\n",
    "\n",
    "    # Heatmap knobs (n muito pequeno): meta-optimizer alternativo ao SA/L2O.\n",
    "    heatmap_presets: Dict[str, Dict[str, object]] = {\n",
    "        \"heat10\": {\"heatmap_nmax\": 10, \"heatmap_steps\": 200},\n",
    "        \"heat20\": {\"heatmap_nmax\": 20, \"heatmap_steps\": 250},\n",
    "        \"heat30\": {\"heatmap_nmax\": 30, \"heatmap_steps\": 350},\n",
    "    }\n",
    "\n",
    "    # Post-opt knobs (n pequeno): hill-climb e GA (opcional).\n",
    "    hc_presets: Dict[str, Dict[str, object]] = {\n",
    "        \"hc20\": {\"hc_nmax\": 20, \"hc_passes\": 2, \"hc_step_xy\": 0.01, \"hc_step_deg\": 2.0},\n",
    "        \"hc50\": {\"hc_nmax\": 50, \"hc_passes\": 2, \"hc_step_xy\": 0.01, \"hc_step_deg\": 2.0},\n",
    "    }\n",
    "    ga_presets: Dict[str, Dict[str, object]] = {\n",
    "        \"ga20\": {\"ga_nmax\": 20, \"ga_pop\": 24, \"ga_gens\": 20, \"ga_elite_frac\": 0.25, \"ga_crossover_prob\": 0.5, \"ga_mut_sigma_xy\": 0.01, \"ga_mut_sigma_deg\": 2.0, \"ga_repair_iters\": 200},\n",
    "        \"ga50\": {\"ga_nmax\": 50, \"ga_pop\": 32, \"ga_gens\": 25, \"ga_elite_frac\": 0.25, \"ga_crossover_prob\": 0.5, \"ga_mut_sigma_xy\": 0.01, \"ga_mut_sigma_deg\": 2.0, \"ga_repair_iters\": 250},\n",
    "    }\n",
    "\n",
    "    # Mother-prefix presets: resolve N=nmax once e emite prefixes.\n",
    "    mother_presets: Dict[str, Dict[str, object]] = {\n",
    "        \"mother_ref2000_prefix\": {\n",
    "            \"mother_prefix\": True,\n",
    "            \"mother_reorder\": \"radial\",\n",
    "            \"sa_nmax\": 0,\n",
    "            \"refine_nmin\": 200,\n",
    "            \"refine_steps\": 2000,\n",
    "            \"refine_batch\": 64,\n",
    "            \"refine_objective\": \"prefix\",\n",
    "            \"refine_proposal\": \"mixed\",\n",
    "            \"refine_cooling\": \"log\",\n",
    "            \"refine_trans_sigma\": 0.35,\n",
    "            \"refine_rot_sigma\": 25.0,\n",
    "            \"refine_rot_prob\": 0.4,\n",
    "            \"refine_rot_prob_end\": 0.1,\n",
    "        },\n",
    "        \"mother_sa5000_prefix\": {\n",
    "            \"mother_prefix\": True,\n",
    "            \"mother_reorder\": \"radial\",\n",
    "            \"sa_nmax\": 200,\n",
    "            \"sa_batch\": 128,\n",
    "            \"sa_steps\": 5000,\n",
    "            \"sa_trans_sigma\": 0.25,\n",
    "            \"sa_rot_sigma\": 25.0,\n",
    "            \"sa_rot_prob\": 0.4,\n",
    "            \"sa_rot_prob_end\": 0.1,\n",
    "            \"sa_objective\": \"prefix\",\n",
    "            \"sa_proposal\": \"mixed\",\n",
    "            \"sa_cooling\": \"log\",\n",
    "            \"sa_swap_prob\": 0.05,\n",
    "            \"sa_swap_prob_end\": 0.0,\n",
    "            \"sa_overlap_lambda\": 0.01,\n",
    "            \"refine_nmin\": 200,\n",
    "            \"refine_steps\": 1000,\n",
    "            \"refine_batch\": 64,\n",
    "            \"refine_objective\": \"prefix\",\n",
    "            \"refine_proposal\": \"mixed\",\n",
    "            \"refine_cooling\": \"log\",\n",
    "            \"refine_trans_sigma\": 0.35,\n",
    "            \"refine_rot_sigma\": 25.0,\n",
    "            \"refine_rot_prob\": 0.4,\n",
    "            \"refine_rot_prob_end\": 0.1,\n",
    "        },\n",
    "        \"mother_blk_ref_prefix\": {\n",
    "            \"mother_prefix\": True,\n",
    "            \"mother_reorder\": \"radial\",\n",
    "            \"sa_nmax\": 0,\n",
    "            \"block_nmax\": 200,\n",
    "            \"block_size\": 2,\n",
    "            \"block_steps\": 350,\n",
    "            \"block_batch\": 32,\n",
    "            \"block_objective\": \"prefix\",\n",
    "            \"block_init\": \"cluster\",\n",
    "            \"lns_nmax\": 200,\n",
    "            \"lns_passes\": 10,\n",
    "            \"lns_destroy_k\": 8,\n",
    "            \"lns_destroy_mode\": \"alns\",\n",
    "            \"lns_tabu_tenure\": 5,\n",
    "            \"lns_candidates\": 64,\n",
    "            \"lns_angle_samples\": 8,\n",
    "            \"lns_pad_scale\": 2.0,\n",
    "            \"lns_group_moves\": 4,\n",
    "            \"lns_group_size\": 3,\n",
    "            \"lns_group_trans_sigma\": 0.05,\n",
    "            \"lns_group_rot_sigma\": 20.0,\n",
    "            \"refine_nmin\": 200,\n",
    "            \"refine_steps\": 2000,\n",
    "            \"refine_batch\": 64,\n",
    "            \"refine_objective\": \"prefix\",\n",
    "            \"refine_proposal\": \"mixed\",\n",
    "            \"refine_cooling\": \"log\",\n",
    "            \"refine_trans_sigma\": 0.35,\n",
    "            \"refine_rot_sigma\": 25.0,\n",
    "            \"refine_rot_prob\": 0.4,\n",
    "            \"refine_rot_prob_end\": 0.1,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Limites para nao explodir combinacoes (aumente/disable para explorar mais).\n",
    "    # `None` = sem limite (maximo de receitas).\n",
    "    MAX_RECIPES_PER_FAMILY = RECIPES_MAX_RECIPES_PER_FAMILY\n",
    "    MAX_LATTICE_VARIANTS = RECIPES_MAX_LATTICE_VARIANTS\n",
    "\n",
    "    def limit_variants(variants: List[Dict[str, object]], max_n: int | None) -> List[Dict[str, object]]:\n",
    "        if max_n is None or len(variants) <= max_n:\n",
    "            return variants\n",
    "        scored = [(json.dumps(v, sort_keys=True), v) for v in variants]\n",
    "        scored.sort(key=lambda x: hashlib.sha1(x[0].encode(\"utf-8\")).hexdigest())\n",
    "        return [v for _s, v in scored[:max_n]]\n",
    "\n",
    "    lattice_variants = limit_variants(lattice_variants_all, MAX_LATTICE_VARIANTS)\n",
    "\n",
    "    recipes: Dict[str, Dict[str, object]] = {}\n",
    "    meta: Dict[str, Dict[str, object]] = {}\n",
    "\n",
    "    def add_recipe(family: str, recipe: Dict[str, object], *, meta_extra: Dict[str, object] | None = None) -> None:\n",
    "        full = dict(base)\n",
    "        full.update(recipe)\n",
    "        rid = _stable_hash_dict(full)\n",
    "        name = f\"{family}_{rid}\"\n",
    "        if name in recipes:\n",
    "            return\n",
    "        recipes[name] = full\n",
    "        meta[name] = {\"family\": family}\n",
    "        if meta_extra:\n",
    "            meta[name].update(meta_extra)\n",
    "\n",
    "    # ---- Baselines ----\n",
    "    for lat in lattice_variants:\n",
    "        add_recipe(\"lattice\", lat, meta_extra={\"lattice\": lat})\n",
    "\n",
    "    # ---- SA / SA+refine ----\n",
    "    for lat in lattice_variants:\n",
    "        for sa_name, sa_cfg in sa_presets.items():\n",
    "            add_recipe(\"sa\", {**lat, **sa_cfg}, meta_extra={\"lattice\": lat, \"sa\": sa_name})\n",
    "            for ref_name, ref_cfg in refine_presets.items():\n",
    "                add_recipe(\"sa_refine\", {**lat, **sa_cfg, **ref_cfg}, meta_extra={\"lattice\": lat, \"sa\": sa_name, \"refine\": ref_name})\n",
    "                if META_INIT_MODEL is not None:\n",
    "                    add_recipe(\n",
    "                        \"sa_refine_meta\",\n",
    "                        {**lat, **sa_cfg, **ref_cfg, \"meta_init_model\": META_INIT_MODEL},\n",
    "                        meta_extra={\"lattice\": lat, \"sa\": sa_name, \"refine\": ref_name, \"meta_init\": True},\n",
    "                    )\n",
    "\n",
    "    # ---- Block meta-model variants ----\n",
    "    for lat in lattice_variants:\n",
    "        for blk_name, blk_cfg in block_presets.items():\n",
    "            for ref_name, ref_cfg in refine_presets.items():\n",
    "                add_recipe(\"block_refine\", {**lat, **blk_cfg, **ref_cfg}, meta_extra={\"lattice\": lat, \"block\": blk_name, \"refine\": ref_name})\n",
    "                if META_INIT_MODEL is not None:\n",
    "                    add_recipe(\n",
    "                        \"block_refine_meta\",\n",
    "                        {**lat, **blk_cfg, **ref_cfg, \"meta_init_model\": META_INIT_MODEL},\n",
    "                        meta_extra={\"lattice\": lat, \"block\": blk_name, \"refine\": ref_name, \"meta_init\": True},\n",
    "                    )\n",
    "            for sa_name, sa_cfg in sa_presets.items():\n",
    "                for ref_name, ref_cfg in refine_presets.items():\n",
    "                    add_recipe(\n",
    "                        \"block_sa_refine\",\n",
    "                        {**lat, **blk_cfg, **sa_cfg, **ref_cfg},\n",
    "                        meta_extra={\"lattice\": lat, \"block\": blk_name, \"sa\": sa_name, \"refine\": ref_name},\n",
    "                    )\n",
    "                    if META_INIT_MODEL is not None:\n",
    "                        add_recipe(\n",
    "                            \"block_sa_refine_meta\",\n",
    "                            {**lat, **blk_cfg, **sa_cfg, **ref_cfg, \"meta_init_model\": META_INIT_MODEL},\n",
    "                            meta_extra={\"lattice\": lat, \"block\": blk_name, \"sa\": sa_name, \"refine\": ref_name, \"meta_init\": True},\n",
    "                        )\n",
    "\n",
    "    # ---- LNS / ALNS post-opt variants ----\n",
    "    for lat in lattice_variants:\n",
    "        for lns_name, lns_cfg in lns_presets.items():\n",
    "            add_recipe(\"lns\", {**lat, **lns_cfg}, meta_extra={\"lattice\": lat, \"lns\": lns_name})\n",
    "            for ref_name, ref_cfg in refine_presets.items():\n",
    "                add_recipe(\"refine_lns\", {**lat, **ref_cfg, **lns_cfg}, meta_extra={\"lattice\": lat, \"refine\": ref_name, \"lns\": lns_name})\n",
    "            for sa_name, sa_cfg in sa_presets.items():\n",
    "                for ref_name, ref_cfg in refine_presets.items():\n",
    "                    add_recipe(\n",
    "                        \"sa_refine_lns\",\n",
    "                        {**lat, **sa_cfg, **ref_cfg, **lns_cfg},\n",
    "                        meta_extra={\"lattice\": lat, \"sa\": sa_name, \"refine\": ref_name, \"lns\": lns_name},\n",
    "                    )\n",
    "            for blk_name, blk_cfg in block_presets.items():\n",
    "                for ref_name, ref_cfg in refine_presets.items():\n",
    "                    add_recipe(\n",
    "                        \"block_refine_lns\",\n",
    "                        {**lat, **blk_cfg, **ref_cfg, **lns_cfg},\n",
    "                        meta_extra={\"lattice\": lat, \"block\": blk_name, \"refine\": ref_name, \"lns\": lns_name},\n",
    "                    )\n",
    "\n",
    "    # ---- Guided SA (em cima do melhor preset base) ----\n",
    "    guided_lattice = lattice_variants[:2] if lattice_variants else [{\"lattice_pattern\": \"hex\", \"lattice_margin\": 0.02, \"lattice_rotate\": 0.0, \"lattice_rotations\": \"0,15,30\"}]\n",
    "    guided_sa = sa_presets.get(\"sa50_prefix_mixed_log_ol0\", next(iter(sa_presets.values())))\n",
    "    guided_ref = refine_presets.get(\"ref80_200_prefix_mixed_log_ol0\", next(iter(refine_presets.values())))\n",
    "    for model_name, model_path in CANDIDATE_GUIDED_MODELS.items():\n",
    "        for lat in guided_lattice:\n",
    "            for g_name, g_cfg in guided_presets.items():\n",
    "                add_recipe(\n",
    "                    \"guided_refine\",\n",
    "                    {**lat, **guided_sa, **guided_ref, **g_cfg, \"guided_model\": str(model_path)},\n",
    "                    meta_extra={\"guided_model\": model_name, \"guided\": g_name, \"lattice\": lat},\n",
    "                )\n",
    "                if META_INIT_MODEL is not None:\n",
    "                    add_recipe(\n",
    "                        \"guided_refine_meta\",\n",
    "                        {**lat, **guided_sa, **guided_ref, **g_cfg, \"guided_model\": str(model_path), \"meta_init_model\": META_INIT_MODEL},\n",
    "                        meta_extra={\"guided_model\": model_name, \"guided\": g_name, \"lattice\": lat, \"meta_init\": True},\n",
    "                    )\n",
    "                for lns_name, lns_cfg in lns_presets.items():\n",
    "                    add_recipe(\n",
    "                        \"guided_refine_lns\",\n",
    "                        {**lat, **guided_sa, **guided_ref, **g_cfg, \"guided_model\": str(model_path), **lns_cfg},\n",
    "                        meta_extra={\"guided_model\": model_name, \"guided\": g_name, \"lns\": lns_name, \"lattice\": lat},\n",
    "                    )\n",
    "                    if META_INIT_MODEL is not None:\n",
    "                        add_recipe(\n",
    "                            \"guided_refine_lns_meta\",\n",
    "                            {**lat, **guided_sa, **guided_ref, **g_cfg, \"guided_model\": str(model_path), **lns_cfg, \"meta_init_model\": META_INIT_MODEL},\n",
    "                            meta_extra={\"guided_model\": model_name, \"guided\": g_name, \"lns\": lns_name, \"lattice\": lat, \"meta_init\": True},\n",
    "                        )\n",
    "\n",
    "                for blk_name, blk_cfg in block_presets.items():\n",
    "                    add_recipe(\n",
    "                        \"guided_block_refine\",\n",
    "                        {**lat, **blk_cfg, **guided_sa, **guided_ref, **g_cfg, \"guided_model\": str(model_path)},\n",
    "                        meta_extra={\"guided_model\": model_name, \"guided\": g_name, \"block\": blk_name, \"lattice\": lat},\n",
    "                    )\n",
    "                    if META_INIT_MODEL is not None:\n",
    "                        add_recipe(\n",
    "                            \"guided_block_refine_meta\",\n",
    "                            {**lat, **blk_cfg, **guided_sa, **guided_ref, **g_cfg, \"guided_model\": str(model_path), \"meta_init_model\": META_INIT_MODEL},\n",
    "                            meta_extra={\"guided_model\": model_name, \"guided\": g_name, \"block\": blk_name, \"lattice\": lat, \"meta_init\": True},\n",
    "                        )\n",
    "                    for lns_name, lns_cfg in lns_presets.items():\n",
    "                        add_recipe(\n",
    "                            \"guided_block_refine_lns\",\n",
    "                            {**lat, **blk_cfg, **guided_sa, **guided_ref, **g_cfg, \"guided_model\": str(model_path), **lns_cfg},\n",
    "                            meta_extra={\"guided_model\": model_name, \"guided\": g_name, \"block\": blk_name, \"lns\": lns_name, \"lattice\": lat},\n",
    "                        )\n",
    "                        if META_INIT_MODEL is not None:\n",
    "                            add_recipe(\n",
    "                                \"guided_block_refine_lns_meta\",\n",
    "                                {**lat, **blk_cfg, **guided_sa, **guided_ref, **g_cfg, \"guided_model\": str(model_path), **lns_cfg, \"meta_init_model\": META_INIT_MODEL},\n",
    "                                meta_extra={\"guided_model\": model_name, \"guided\": g_name, \"block\": blk_name, \"lns\": lns_name, \"lattice\": lat, \"meta_init\": True},\n",
    "                            )\n",
    "\n",
    "    # ---- L2O (n pequeno) + SA/refine ----\n",
    "    l2o_lattice = lattice_variants[:2] if lattice_variants else [{\"lattice_pattern\": \"hex\", \"lattice_margin\": 0.02, \"lattice_rotate\": 0.0}]\n",
    "    l2o_sa = sa_presets.get(\"sa50_prefix_mixed_log_ol0\", next(iter(sa_presets.values())))\n",
    "    l2o_ref = refine_presets.get(\"ref80_200_prefix_mixed_log_ol0\", next(iter(refine_presets.values())))\n",
    "    for model_name, model_path in CANDIDATE_L2O_MODELS.items():\n",
    "        for lat in l2o_lattice:\n",
    "            for l2o_name, l2o_cfg in l2o_presets.items():\n",
    "                add_recipe(\n",
    "                    \"l2o_refine\",\n",
    "                    {**lat, **l2o_sa, **l2o_ref, **l2o_cfg, \"l2o_model\": str(model_path)},\n",
    "                    meta_extra={\"l2o_model\": model_name, \"l2o\": l2o_name, \"lattice\": lat},\n",
    "                )\n",
    "                if META_INIT_MODEL is not None:\n",
    "                    add_recipe(\n",
    "                        \"l2o_refine_meta\",\n",
    "                        {**lat, **l2o_sa, **l2o_ref, **l2o_cfg, \"l2o_model\": str(model_path), \"meta_init_model\": META_INIT_MODEL},\n",
    "                        meta_extra={\"l2o_model\": model_name, \"l2o\": l2o_name, \"lattice\": lat, \"meta_init\": True},\n",
    "                    )\n",
    "                for lns_name, lns_cfg in lns_presets.items():\n",
    "                    add_recipe(\n",
    "                        \"l2o_refine_lns\",\n",
    "                        {**lat, **l2o_sa, **l2o_ref, **l2o_cfg, \"l2o_model\": str(model_path), **lns_cfg},\n",
    "                        meta_extra={\"l2o_model\": model_name, \"l2o\": l2o_name, \"lns\": lns_name, \"lattice\": lat},\n",
    "                    )\n",
    "                    if META_INIT_MODEL is not None:\n",
    "                        add_recipe(\n",
    "                            \"l2o_refine_lns_meta\",\n",
    "                            {**lat, **l2o_sa, **l2o_ref, **l2o_cfg, \"l2o_model\": str(model_path), **lns_cfg, \"meta_init_model\": META_INIT_MODEL},\n",
    "                            meta_extra={\"l2o_model\": model_name, \"l2o\": l2o_name, \"lns\": lns_name, \"lattice\": lat, \"meta_init\": True},\n",
    "                        )\n",
    "\n",
    "    # ---- Heatmap variants ----\n",
    "    if HEATMAP_MODEL is not None:\n",
    "        heat_lattice = lattice_variants[:2] if lattice_variants else [{\"lattice_pattern\": \"hex\", \"lattice_margin\": 0.02, \"lattice_rotate\": 0.0}]\n",
    "        for lat in heat_lattice:\n",
    "            for h_name, h_cfg in heatmap_presets.items():\n",
    "                add_recipe(\n",
    "                    \"heatmap\",\n",
    "                    {**lat, **h_cfg, \"heatmap_model\": HEATMAP_MODEL},\n",
    "                    meta_extra={\"heatmap\": h_name, \"lattice\": lat},\n",
    "                )\n",
    "                # heatmap + SA/refine\n",
    "                add_recipe(\n",
    "                    \"heatmap_sa_refine\",\n",
    "                    {**lat, **h_cfg, \"heatmap_model\": HEATMAP_MODEL, **guided_sa, **guided_ref},\n",
    "                    meta_extra={\"heatmap\": h_name, \"lattice\": lat, \"sa\": \"sa50_prefix_mixed_log_ol0\", \"refine\": \"ref80_200_prefix_mixed_log_ol0\"},\n",
    "                )\n",
    "                # heatmap + l2o (com o melhor modelo disponivel) + refine\n",
    "                best_l2o_name = next(iter(CANDIDATE_L2O_MODELS))\n",
    "                best_l2o_path = CANDIDATE_L2O_MODELS[best_l2o_name]\n",
    "                best_l2o_cfg = l2o_presets.get(\"l2o10_det\", next(iter(l2o_presets.values())))\n",
    "                add_recipe(\n",
    "                    \"heatmap_l2o_refine\",\n",
    "                    {**lat, **h_cfg, \"heatmap_model\": HEATMAP_MODEL, **best_l2o_cfg, \"l2o_model\": str(best_l2o_path), **guided_sa, **guided_ref},\n",
    "                    meta_extra={\"heatmap\": h_name, \"l2o_model\": best_l2o_name, \"l2o\": \"l2o10_det\", \"lattice\": lat},\n",
    "                )\n",
    "\n",
    "    # ---- Post-opt (hill-climb / GA) variants ----\n",
    "    post_lattice = lattice_variants[:2] if lattice_variants else [{\"lattice_pattern\": \"hex\", \"lattice_margin\": 0.02, \"lattice_rotate\": 0.0, \"lattice_rotations\": \"0,15,30\"}]\n",
    "    for lat in post_lattice:\n",
    "        for hc_name, hc_cfg in hc_presets.items():\n",
    "            add_recipe(\"hc\", {**lat, **hc_cfg}, meta_extra={\"lattice\": lat, \"hc\": hc_name})\n",
    "            add_recipe(\n",
    "                \"sa_refine_hc\",\n",
    "                {**lat, **guided_sa, **guided_ref, **hc_cfg},\n",
    "                meta_extra={\"lattice\": lat, \"sa\": \"sa50_prefix_mixed_log_ol0\", \"refine\": \"ref80_200_prefix_mixed_log_ol0\", \"hc\": hc_name},\n",
    "            )\n",
    "            for lns_name, lns_cfg in lns_presets.items():\n",
    "                add_recipe(\n",
    "                    \"sa_refine_lns_hc\",\n",
    "                    {**lat, **guided_sa, **guided_ref, **lns_cfg, **hc_cfg},\n",
    "                    meta_extra={\"lattice\": lat, \"sa\": \"sa50_prefix_mixed_log_ol0\", \"refine\": \"ref80_200_prefix_mixed_log_ol0\", \"lns\": lns_name, \"hc\": hc_name},\n",
    "                )\n",
    "\n",
    "        for ga_name, ga_cfg in ga_presets.items():\n",
    "            add_recipe(\"ga\", {**lat, **ga_cfg}, meta_extra={\"lattice\": lat, \"ga\": ga_name})\n",
    "            add_recipe(\n",
    "                \"sa_refine_ga\",\n",
    "                {**lat, **guided_sa, **guided_ref, **ga_cfg},\n",
    "                meta_extra={\"lattice\": lat, \"sa\": \"sa50_prefix_mixed_log_ol0\", \"refine\": \"ref80_200_prefix_mixed_log_ol0\", \"ga\": ga_name},\n",
    "            )\n",
    "            for lns_name, lns_cfg in lns_presets.items():\n",
    "                add_recipe(\n",
    "                    \"sa_refine_lns_ga\",\n",
    "                    {**lat, **guided_sa, **guided_ref, **lns_cfg, **ga_cfg},\n",
    "                    meta_extra={\"lattice\": lat, \"sa\": \"sa50_prefix_mixed_log_ol0\", \"refine\": \"ref80_200_prefix_mixed_log_ol0\", \"lns\": lns_name, \"ga\": ga_name},\n",
    "                )\n",
    "\n",
    "    # ---- Mother-prefix variants (solve N once, emit prefixes) ----\n",
    "    mother_lattice = lattice_variants[:2] if lattice_variants else [{\"lattice_pattern\": \"hex\", \"lattice_margin\": 0.02, \"lattice_rotate\": 0.0, \"lattice_rotations\": \"0,15,30\"}]\n",
    "    for lat in mother_lattice:\n",
    "        for m_name, m_cfg in mother_presets.items():\n",
    "            add_recipe(\"mother\", {**lat, **m_cfg}, meta_extra={\"lattice\": lat, \"mother\": m_name})\n",
    "            # mother + guided refine (usa policy no refine/SA quando habilitado)\n",
    "            for model_name, model_path in CANDIDATE_GUIDED_MODELS.items():\n",
    "                for g_name, g_cfg in guided_presets.items():\n",
    "                    add_recipe(\n",
    "                        \"mother_guided\",\n",
    "                        {**lat, **m_cfg, **g_cfg, \"guided_model\": str(model_path)},\n",
    "                        meta_extra={\"lattice\": lat, \"mother\": m_name, \"guided_model\": model_name, \"guided\": g_name},\n",
    "                    )\n",
    "\n",
    "    # ---- Cap recipes per family ----\n",
    "    if MAX_RECIPES_PER_FAMILY is not None:\n",
    "        by_family: Dict[str, List[str]] = {}\n",
    "        for name in recipes:\n",
    "            fam = str(meta.get(name, {}).get(\"family\", \"misc\"))\n",
    "            by_family.setdefault(fam, []).append(name)\n",
    "        keep: List[str] = []\n",
    "        for fam, names in by_family.items():\n",
    "            keep.extend(sorted(names)[:MAX_RECIPES_PER_FAMILY])\n",
    "        recipes = {k: recipes[k] for k in keep}\n",
    "        meta = {k: meta[k] for k in keep}\n",
    "\n",
    "    settings: Dict[str, object] = {\n",
    "        \"max_recipes_per_family\": MAX_RECIPES_PER_FAMILY,\n",
    "        \"max_lattice_variants\": MAX_LATTICE_VARIANTS,\n",
    "        \"lattice_variants_total\": len(lattice_variants_all),\n",
    "        \"lattice_variants_used\": len(lattice_variants),\n",
    "        \"sa_presets\": sorted(sa_presets.keys()),\n",
    "        \"refine_presets\": sorted(refine_presets.keys()),\n",
    "        \"block_presets\": sorted(block_presets.keys()),\n",
    "        \"lns_presets\": sorted(lns_presets.keys()),\n",
    "        \"guided_presets\": sorted(guided_presets.keys()),\n",
    "        \"l2o_presets\": sorted(l2o_presets.keys()),\n",
    "        \"heatmap_presets\": sorted(heatmap_presets.keys()),\n",
    "        \"hc_presets\": sorted(hc_presets.keys()),\n",
    "        \"ga_presets\": sorted(ga_presets.keys()),\n",
    "        \"mother_presets\": sorted(mother_presets.keys()),\n",
    "    }\n",
    "\n",
    "    return recipes, meta, settings\n",
    "\n",
    "\n",
    "# === Receitas (flags do python -m santa_packing.cli.generate_submission) ===\n",
    "RECIPES, RECIPES_META, RECIPES_SETTINGS = _build_recipe_pool()\n",
    "RECIPES_SETTINGS.update(\n",
    "    {\n",
    "        \"meta_init_model\": META_INIT_MODEL,\n",
    "        \"heatmap_model\": HEATMAP_MODEL,\n",
    "        \"candidate_l2o_models\": {k: str(v) for k, v in CANDIDATE_L2O_MODELS.items()},\n",
    "        \"candidate_guided_models\": {k: str(v) for k, v in CANDIDATE_GUIDED_MODELS.items()},\n",
    "    }\n",
    ")\n",
    "(RUN_DIR / \"recipes.json\").write_text(json.dumps(RECIPES, indent=2, sort_keys=True, default=str))\n",
    "(RUN_DIR / \"recipes_meta.json\").write_text(json.dumps(RECIPES_META, indent=2, sort_keys=True, default=str))\n",
    "(RUN_DIR / \"recipes_settings.json\").write_text(json.dumps(RECIPES_SETTINGS, indent=2, sort_keys=True, default=str))\n",
    "\n",
    "# Remove receitas que dependem de modelos inexistentes\n",
    "ACTIVE_RECIPES: Dict[str, Dict[str, object]] = {}\n",
    "ACTIVE_META: Dict[str, Dict[str, object]] = {}\n",
    "for name, recipe in RECIPES.items():\n",
    "    needs = [\n",
    "        (\"l2o_model\", recipe.get(\"l2o_model\")),\n",
    "        (\"guided_model\", recipe.get(\"guided_model\")),\n",
    "        (\"meta_init_model\", recipe.get(\"meta_init_model\")),\n",
    "        (\"heatmap_model\", recipe.get(\"heatmap_model\")),\n",
    "    ]\n",
    "    missing = [k for k, v in needs if v is not None and not Path(str(v)).exists()]\n",
    "    if missing:\n",
    "        print(f\"[skip] receita '{name}' (modelos ausentes: {missing})\")\n",
    "        continue\n",
    "    ACTIVE_RECIPES[name] = recipe\n",
    "    if name in RECIPES_META:\n",
    "        ACTIVE_META[name] = RECIPES_META[name]\n",
    "(RUN_DIR / \"active_recipes.json\").write_text(json.dumps(ACTIVE_RECIPES, indent=2, sort_keys=True, default=str))\n",
    "(RUN_DIR / \"active_recipes_meta.json\").write_text(json.dumps(ACTIVE_META, indent=2, sort_keys=True, default=str))\n",
    "\n",
    "SUB_DIR = RUN_DIR / \"submissions\"\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Melhor receita \"single\" encontrada no sweep (para reutilizar no max_seed_sweep).\n",
    "BEST_SEED_SWEEP_RECIPE_NAME: str | None = None\n",
    "BEST_SEED_SWEEP_RECIPE: Dict[str, object] | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861234f9",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Experimento: sweep de submissions (2-stage)"
   },
   "outputs": [],
   "source": [
    "if RUN_SUBMISSION_SWEEP:\n",
    "    # Sweep em 2 estagios:\n",
    "    # 1) ranking rapido em n pequeno (p/ cortar combinacoes)\n",
    "    # 2) rerun somente top-K em nmax=200 (com overlap_check) + opcional ensemble por puzzle\n",
    "    SWEEP_TOPK = 20  # quantos candidatos do estagio 1 vao para o estagio 2\n",
    "    TWO_STAGE_SWEEP = True\n",
    "\n",
    "    sweep_seeds = SWEEP_SEEDS if isinstance(SWEEP_SEEDS, list) else parse_int_list(str(SWEEP_SEEDS))\n",
    "    if not sweep_seeds:\n",
    "        raise ValueError(\"SWEEP_SEEDS is empty\")\n",
    "\n",
    "    jobs = int(SWEEP_JOBS)\n",
    "    if jobs <= 0:\n",
    "        jobs = max(1, int(os.cpu_count() or 1))\n",
    "\n",
    "    (RUN_DIR / \"submission_sweep_meta.json\").write_text(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"two_stage\": TWO_STAGE_SWEEP,\n",
    "                \"jobs\": jobs,\n",
    "                \"reuse\": bool(SWEEP_REUSE),\n",
    "                \"keep_going\": bool(SWEEP_KEEP_GOING),\n",
    "                \"stage1\": {\n",
    "                    \"nmax\": int(SWEEP_NMAX),\n",
    "                    \"seeds\": [int(s) for s in sweep_seeds],\n",
    "                    \"overlap_check\": bool(SWEEP_SCORE_OVERLAP_CHECK),\n",
    "                },\n",
    "                \"stage2\": {\n",
    "                    \"nmax\": int(SUBMISSION_NMAX),\n",
    "                    \"overlap_check\": bool(SUBMISSION_OVERLAP_CHECK),\n",
    "                    \"topk\": int(SWEEP_TOPK),\n",
    "                },\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    def _run_submission_candidate(\n",
    "        stage: int,\n",
    "        recipe_name: str,\n",
    "        recipe: Dict[str, object],\n",
    "        seed: int,\n",
    "        *,\n",
    "        nmax: int,\n",
    "        check_overlap: bool,\n",
    "    ) -> tuple[Dict[str, object], str, Path]:\n",
    "        tag = f\"{recipe_name}_seed{seed}\"\n",
    "        out_csv = SUB_DIR / f\"stage{stage}_{tag}.csv\"\n",
    "        if not (SWEEP_REUSE and out_csv.exists()):\n",
    "            generate_submission(out_csv, seed=seed, nmax=nmax, args=recipe)\n",
    "        score = score_csv(out_csv, nmax=nmax, check_overlap=check_overlap)\n",
    "        row: Dict[str, object] = {\n",
    "            \"tag\": tag,\n",
    "            \"stage\": int(stage),\n",
    "            \"recipe\": recipe_name,\n",
    "            \"seed\": int(seed),\n",
    "            \"nmax\": int(nmax),\n",
    "            \"score\": score.get(\"score\"),\n",
    "            \"s_max\": score.get(\"s_max\"),\n",
    "            \"overlap_check\": score.get(\"overlap_check\"),\n",
    "        }\n",
    "        return row, tag, out_csv\n",
    "\n",
    "    stage1_rows: List[Dict[str, object]] = []\n",
    "    stage1_paths: Dict[str, Path] = {}\n",
    "    planned_stage1: List[tuple[str, Dict[str, object], int]] = []\n",
    "    for recipe_name, recipe in ACTIVE_RECIPES.items():\n",
    "        for seed in sweep_seeds:\n",
    "            planned_stage1.append((recipe_name, recipe, int(seed)))\n",
    "\n",
    "    if jobs == 1:\n",
    "        for recipe_name, recipe, seed in planned_stage1:\n",
    "            try:\n",
    "                row, tag, out_csv = _run_submission_candidate(\n",
    "                    1,\n",
    "                    recipe_name,\n",
    "                    recipe,\n",
    "                    seed,\n",
    "                    nmax=SWEEP_NMAX,\n",
    "                    check_overlap=SWEEP_SCORE_OVERLAP_CHECK,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[sweep stage1] failed ({recipe_name}, seed={seed}): {e}\")\n",
    "                if not SWEEP_KEEP_GOING:\n",
    "                    raise\n",
    "                continue\n",
    "            stage1_rows.append(row)\n",
    "            stage1_paths[tag] = out_csv\n",
    "    else:\n",
    "        with ThreadPoolExecutor(max_workers=jobs) as ex:\n",
    "            fut_to_item = {\n",
    "                ex.submit(\n",
    "                    _run_submission_candidate,\n",
    "                    1,\n",
    "                    recipe_name,\n",
    "                    recipe,\n",
    "                    seed,\n",
    "                    nmax=SWEEP_NMAX,\n",
    "                    check_overlap=SWEEP_SCORE_OVERLAP_CHECK,\n",
    "                ): (recipe_name, seed)\n",
    "                for recipe_name, recipe, seed in planned_stage1\n",
    "            }\n",
    "            for fut in as_completed(fut_to_item):\n",
    "                recipe_name, seed = fut_to_item[fut]\n",
    "                try:\n",
    "                    row, tag, out_csv = fut.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"[sweep stage1] failed ({recipe_name}, seed={seed}): {e}\")\n",
    "                    if not SWEEP_KEEP_GOING:\n",
    "                        raise\n",
    "                    continue\n",
    "                stage1_rows.append(row)\n",
    "                stage1_paths[tag] = out_csv\n",
    "\n",
    "    stage1_rows = sorted(stage1_rows, key=lambda r: (float(r.get(\"score\") or float(\"inf\")), str(r[\"tag\"])))\n",
    "    write_csv(RUN_DIR / \"submission_sweep_stage1.csv\", stage1_rows)\n",
    "\n",
    "    if not TWO_STAGE_SWEEP:\n",
    "        # Comportamento antigo (1 estagio): promove o melhor do sweep rapido.\n",
    "        best = stage1_rows[0] if stage1_rows else None\n",
    "        if best is not None:\n",
    "            best_path = stage1_paths[str(best[\"tag\"])]\n",
    "            shutil.copyfile(best_path, RUN_DIR / \"submission_best.csv\")\n",
    "            (RUN_DIR / \"submission_best.txt\").write_text(json.dumps(best, indent=2))\n",
    "            BEST_SEED_SWEEP_RECIPE_NAME = str(best.get(\"recipe\"))\n",
    "            BEST_SEED_SWEEP_RECIPE = ACTIVE_RECIPES.get(BEST_SEED_SWEEP_RECIPE_NAME) if BEST_SEED_SWEEP_RECIPE_NAME else None\n",
    "            print(\"Best (sweep stage1):\", best)\n",
    "            print(\"Saved:\", RUN_DIR / \"submission_best.csv\")\n",
    "\n",
    "        if SWEEP_BUILD_ENSEMBLE and stage1_paths:\n",
    "            ens_csv = RUN_DIR / \"submission_ensemble.csv\"\n",
    "            ens_meta = _best_per_puzzle_ensemble(ens_csv, stage1_paths, nmax=SWEEP_NMAX, check_overlap=SWEEP_SCORE_OVERLAP_CHECK)\n",
    "            (RUN_DIR / \"submission_ensemble_meta.json\").write_text(json.dumps(ens_meta, indent=2))\n",
    "            ens_score = score_csv(ens_csv, nmax=SWEEP_NMAX, check_overlap=SWEEP_SCORE_OVERLAP_CHECK)\n",
    "            (RUN_DIR / \"submission_ensemble_score.json\").write_text(json.dumps(ens_score, indent=2))\n",
    "            print(\"Ensemble score (stage1):\", ens_score.get(\"score\"))\n",
    "            print(\"Saved:\", ens_csv)\n",
    "    else:\n",
    "        selected = stage1_rows[: int(SWEEP_TOPK)] if stage1_rows else []\n",
    "        (RUN_DIR / \"submission_sweep_selected.json\").write_text(json.dumps(selected, indent=2, default=str))\n",
    "        print(f\"Stage1: {len(stage1_rows)} candidates; promoting top {len(selected)} to stage2\")\n",
    "\n",
    "        stage2_rows: List[Dict[str, object]] = []\n",
    "        stage2_paths: Dict[str, Path] = {}\n",
    "        planned_stage2: List[tuple[Dict[str, object], str, Dict[str, object], int]] = []\n",
    "        for row in selected:\n",
    "            recipe_name = str(row[\"recipe\"])\n",
    "            seed = int(row[\"seed\"])\n",
    "            recipe = ACTIVE_RECIPES[recipe_name]\n",
    "            planned_stage2.append((row, recipe_name, recipe, seed))\n",
    "\n",
    "        if jobs == 1:\n",
    "            for row, recipe_name, recipe, seed in planned_stage2:\n",
    "                try:\n",
    "                    out_row, tag, out_csv = _run_submission_candidate(\n",
    "                        2,\n",
    "                        recipe_name,\n",
    "                        recipe,\n",
    "                        seed,\n",
    "                        nmax=SUBMISSION_NMAX,\n",
    "                        check_overlap=SUBMISSION_OVERLAP_CHECK,\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"[sweep stage2] failed ({recipe_name}, seed={seed}): {e}\")\n",
    "                    if not SWEEP_KEEP_GOING:\n",
    "                        raise\n",
    "                    continue\n",
    "                out_row[\"stage1_score\"] = row.get(\"score\")\n",
    "                stage2_rows.append(out_row)\n",
    "                stage2_paths[tag] = out_csv\n",
    "        else:\n",
    "            with ThreadPoolExecutor(max_workers=jobs) as ex:\n",
    "                fut_to_item = {\n",
    "                    ex.submit(\n",
    "                        _run_submission_candidate,\n",
    "                        2,\n",
    "                        recipe_name,\n",
    "                        recipe,\n",
    "                        seed,\n",
    "                        nmax=SUBMISSION_NMAX,\n",
    "                        check_overlap=SUBMISSION_OVERLAP_CHECK,\n",
    "                    ): (row, recipe_name, seed)\n",
    "                    for row, recipe_name, recipe, seed in planned_stage2\n",
    "                }\n",
    "                for fut in as_completed(fut_to_item):\n",
    "                    row, recipe_name, seed = fut_to_item[fut]\n",
    "                    try:\n",
    "                        out_row, tag, out_csv = fut.result()\n",
    "                    except Exception as e:\n",
    "                        print(f\"[sweep stage2] failed ({recipe_name}, seed={seed}): {e}\")\n",
    "                        if not SWEEP_KEEP_GOING:\n",
    "                            raise\n",
    "                        continue\n",
    "                    out_row[\"stage1_score\"] = row.get(\"score\")\n",
    "                    stage2_rows.append(out_row)\n",
    "                    stage2_paths[tag] = out_csv\n",
    "\n",
    "        stage2_rows = sorted(stage2_rows, key=lambda r: (float(r.get(\"score\") or float(\"inf\")), str(r[\"tag\"])))\n",
    "        write_csv(RUN_DIR / \"submission_sweep_stage2.csv\", stage2_rows)\n",
    "\n",
    "        best2 = stage2_rows[0] if stage2_rows else None\n",
    "        best_score = float(best2.get(\"score\")) if best2 is not None and best2.get(\"score\") is not None else float(\"inf\")\n",
    "        best_csv: Path | None = None\n",
    "        best_meta: Dict[str, object] | None = None\n",
    "        if best2 is not None:\n",
    "            best_csv = stage2_paths[str(best2[\"tag\"])]\n",
    "            best_meta = dict(best2)\n",
    "            shutil.copyfile(best_csv, RUN_DIR / \"submission_best.csv\")\n",
    "            (RUN_DIR / \"submission_best.txt\").write_text(json.dumps(best2, indent=2))\n",
    "            BEST_SEED_SWEEP_RECIPE_NAME = str(best2.get(\"recipe\"))\n",
    "            BEST_SEED_SWEEP_RECIPE = ACTIVE_RECIPES.get(BEST_SEED_SWEEP_RECIPE_NAME) if BEST_SEED_SWEEP_RECIPE_NAME else None\n",
    "            print(\"Best (stage2):\", best2)\n",
    "            print(\"Saved:\", RUN_DIR / \"submission_best.csv\")\n",
    "\n",
    "        if SWEEP_BUILD_ENSEMBLE and stage2_paths:\n",
    "            ens_csv = RUN_DIR / \"submission_ensemble.csv\"\n",
    "            ens_meta = _best_per_puzzle_ensemble(ens_csv, stage2_paths, nmax=SUBMISSION_NMAX, check_overlap=SUBMISSION_OVERLAP_CHECK)\n",
    "            (RUN_DIR / \"submission_ensemble_meta.json\").write_text(json.dumps(ens_meta, indent=2))\n",
    "            ens_score = score_csv(ens_csv, nmax=SUBMISSION_NMAX, check_overlap=SUBMISSION_OVERLAP_CHECK)\n",
    "            (RUN_DIR / \"submission_ensemble_score.json\").write_text(json.dumps(ens_score, indent=2))\n",
    "            print(\"Ensemble score (stage2):\", ens_score.get(\"score\"))\n",
    "            print(\"Saved:\", ens_csv)\n",
    "            try:\n",
    "                ens_total = float(ens_score.get(\"score\")) if ens_score.get(\"score\") is not None else float(\"inf\")\n",
    "            except Exception:\n",
    "                ens_total = float(\"inf\")\n",
    "            if ens_total < best_score:\n",
    "                shutil.copyfile(ens_csv, RUN_DIR / \"submission_best.csv\")\n",
    "                (RUN_DIR / \"submission_best.txt\").write_text(\n",
    "                    json.dumps(\n",
    "                        {\n",
    "                            \"tag\": \"ensemble\",\n",
    "                            \"stage\": 2,\n",
    "                            \"score\": ens_score.get(\"score\"),\n",
    "                            \"s_max\": ens_score.get(\"s_max\"),\n",
    "                            \"overlap_check\": ens_score.get(\"overlap_check\"),\n",
    "                            \"selected_by_puzzle\": ens_meta.get(\"selected_by_puzzle\"),\n",
    "                            \"candidates\": list(stage2_paths.keys()),\n",
    "                        },\n",
    "                        indent=2,\n",
    "                    )\n",
    "                )\n",
    "                print(\"Best updated to ensemble.\")\n",
    "else:\n",
    "    print(\"[submission_sweep] RUN_SUBMISSION_SWEEP=False; pule este cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9bce08",
   "metadata": {
    "title": "Experimento: gerar submission.csv (single)"
   },
   "outputs": [],
   "source": [
    "if not RUN_SUBMISSION_SWEEP:\n",
    "    # Rodada unica (use nmax=200 + overlap_check=True para score final)\n",
    "    def _pick_single_recipe() -> str:\n",
    "        preferred_families = [\n",
    "            \"guided_block_refine_lns_meta\",\n",
    "            \"guided_block_refine_lns\",\n",
    "            \"guided_refine_lns_meta\",\n",
    "            \"guided_refine_lns\",\n",
    "            \"l2o_refine_lns_meta\",\n",
    "            \"l2o_refine_lns\",\n",
    "            \"sa_refine_lns\",\n",
    "            \"refine_lns\",\n",
    "            \"block_refine_lns\",\n",
    "            \"lns\",\n",
    "            \"guided_block_refine_meta\",\n",
    "            \"guided_block_refine\",\n",
    "            \"block_sa_refine_meta\",\n",
    "            \"block_sa_refine\",\n",
    "            \"block_refine_meta\",\n",
    "            \"block_refine\",\n",
    "            \"guided_refine_meta\",\n",
    "            \"guided_refine\",\n",
    "            \"l2o_refine_meta\",\n",
    "            \"l2o_refine\",\n",
    "            \"sa_refine_meta\",\n",
    "            \"sa_refine\",\n",
    "            \"sa\",\n",
    "            \"lattice\",\n",
    "        ]\n",
    "        for fam in preferred_families:\n",
    "            cands = [k for k, m in ACTIVE_META.items() if str(m.get(\"family\")) == fam]\n",
    "            if cands:\n",
    "                return sorted(cands)[0]\n",
    "        return sorted(ACTIVE_RECIPES)[0]\n",
    "\n",
    "    SINGLE_RECIPE = _pick_single_recipe()\n",
    "    BEST_SEED_SWEEP_RECIPE_NAME = str(SINGLE_RECIPE)\n",
    "    BEST_SEED_SWEEP_RECIPE = ACTIVE_RECIPES.get(BEST_SEED_SWEEP_RECIPE_NAME)\n",
    "    out_csv = RUN_DIR / \"submission.csv\"\n",
    "    generate_submission(out_csv, seed=SUBMISSION_SEED, nmax=SUBMISSION_NMAX, args=ACTIVE_RECIPES[SINGLE_RECIPE])\n",
    "    score = score_csv(out_csv, nmax=SUBMISSION_NMAX, check_overlap=SUBMISSION_OVERLAP_CHECK)\n",
    "    (RUN_DIR / \"submission_score.json\").write_text(json.dumps(score, indent=2))\n",
    "    print(\"Submission saved to\", out_csv)\n",
    "    print(\"Score:\", score.get(\"score\"))\n",
    "else:\n",
    "    print(\"[single_submission] RUN_SUBMISSION_SWEEP=True; pule este cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c3fe11",
   "metadata": {
    "title": "Maximo de experimentos: multi-start + ensemble por n (via python -m santa_packing.cli.sweep_ensemble)"
   },
   "outputs": [],
   "source": [
    "# Objetivo: rodar MUITAS seeds com uma receita forte (mother-prefix + refine no N=200) e\n",
    "# deixar o ensemble escolher o melhor s_n por puzzle.\n",
    "RUN_MAX_SEED_SWEEP = False\n",
    "MAX_SEED_SWEEP_SEEDS = \"1..200\"  # aumente se quiser (ex.: 1..500)\n",
    "MAX_SEED_SWEEP_JOBS = max(1, int(os.cpu_count() or 1))  # candidatos em paralelo (ajuste conforme CPU/RAM)\n",
    "MAX_SEED_SWEEP_TAG = \"max_seed_sweep\"\n",
    "MAX_SEED_SWEEP_OUT = RUN_DIR / \"submission_max_ensemble.csv\"\n",
    "\n",
    "# Receita forte default (ajuste livremente). Se o sweep de receitas rodar, usamos a melhor receita \"single\".\n",
    "DEFAULT_MAX_SEED_SWEEP_RECIPE: Dict[str, object] = {\n",
    "    \"mother_prefix\": True,\n",
    "    \"sa_nmax\": 0,\n",
    "    \"lattice_pattern\": \"hex\",\n",
    "    \"lattice_margin\": 0.005,\n",
    "    \"lattice_rotations\": \"0,5,10,15,20,25,30\",\n",
    "    \"block_nmax\": 200,\n",
    "    \"block_size\": 2,\n",
    "    \"block_steps\": 200,\n",
    "    \"block_batch\": 32,\n",
    "    \"block_objective\": \"prefix\",\n",
    "    \"block_init\": \"cluster\",\n",
    "    \"lns_nmax\": 200,\n",
    "    \"lns_passes\": 10,\n",
    "    \"lns_destroy_k\": 8,\n",
    "    \"lns_destroy_mode\": \"alns\",\n",
    "    \"lns_tabu_tenure\": 5,\n",
    "    \"lns_candidates\": 64,\n",
    "    \"lns_angle_samples\": 8,\n",
    "    \"lns_pad_scale\": 2.0,\n",
    "    \"lns_group_moves\": 4,\n",
    "    \"lns_group_size\": 3,\n",
    "    \"lns_group_trans_sigma\": 0.05,\n",
    "    \"lns_group_rot_sigma\": 20.0,\n",
    "    \"lns_t_start\": 0.0,\n",
    "    \"lns_t_end\": 0.0,\n",
    "    \"refine_nmin\": 200,\n",
    "    \"refine_steps\": 5000,\n",
    "    \"refine_batch\": 128,\n",
    "    \"refine_objective\": \"prefix\",\n",
    "    \"refine_proposal\": \"mixed\",\n",
    "    \"refine_cooling\": \"log\",\n",
    "    \"refine_trans_sigma\": 0.35,\n",
    "    \"refine_rot_sigma\": 25.0,\n",
    "    \"refine_rot_prob\": 0.4,\n",
    "    \"refine_rot_prob_end\": 0.1,\n",
    "}\n",
    "\n",
    "if RUN_MAX_SEED_SWEEP:\n",
    "    if MAX_SEED_SWEEP_OUT.exists():\n",
    "        print(\"[max_seed_sweep] skipping: already exists:\", MAX_SEED_SWEEP_OUT)\n",
    "    else:\n",
    "        max_seed_recipe_name = BEST_SEED_SWEEP_RECIPE_NAME or \"default_refine\"\n",
    "        max_seed_recipe = BEST_SEED_SWEEP_RECIPE or DEFAULT_MAX_SEED_SWEEP_RECIPE\n",
    "        print(f\"[max_seed_sweep] base_recipe={max_seed_recipe_name}  seeds={MAX_SEED_SWEEP_SEEDS}  jobs={MAX_SEED_SWEEP_JOBS}\")\n",
    "\n",
    "        recipes_json = RUN_DIR / \"max_seed_sweep_recipes.json\"\n",
    "        args_list: List[str] = []\n",
    "        for key, value in max_seed_recipe.items():\n",
    "            if value is None:\n",
    "                continue\n",
    "            flag = \"--\" + key.replace(\"_\", \"-\")\n",
    "            if isinstance(value, bool):\n",
    "                if value:\n",
    "                    args_list.append(flag)\n",
    "            else:\n",
    "                args_list += [flag, str(value)]\n",
    "        recipes_json.write_text(json.dumps([{\"name\": str(max_seed_recipe_name), \"args\": args_list}], indent=2))\n",
    "\n",
        "        run_cmd(\n",
        "            [\n",
        "                sys.executable,\n",
        "                \"-m\",\n",
        "                \"santa_packing.cli.sweep_ensemble\",\n",
        "                \"--repo\",\n",
        "                str(ROOT),\n",
        "                \"--runs-dir\",\n",
        "                str(RUN_DIR),\n",
    "                \"--tag\",\n",
    "                MAX_SEED_SWEEP_TAG,\n",
    "                \"--nmax\",\n",
    "                str(SUBMISSION_NMAX),\n",
    "                \"--seeds\",\n",
    "                str(MAX_SEED_SWEEP_SEEDS),\n",
    "                \"--jobs\",\n",
    "                str(MAX_SEED_SWEEP_JOBS),\n",
    "                \"--recipes-json\",\n",
    "                str(recipes_json),\n",
    "                \"--overlap-check\",\n",
    "                \"selected\",\n",
    "                \"--keep-going\",\n",
    "                \"--out\",\n",
    "                str(MAX_SEED_SWEEP_OUT),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        max_score = score_csv(MAX_SEED_SWEEP_OUT, nmax=SUBMISSION_NMAX, check_overlap=True)\n",
    "        (RUN_DIR / \"submission_max_ensemble_score.json\").write_text(json.dumps(max_score, indent=2))\n",
    "        print(\"[max_seed_sweep] score:\", max_score.get(\"score\"))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
